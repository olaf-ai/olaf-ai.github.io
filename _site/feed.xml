<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <title>Olaf.AI</title>
    <link>http://localhost:4000/</link>
    <description>Onward, Living A Future</description>
    <language>en-us</language>
    <pubDate>Mon, 04 Mar 2024 21:00:45 +0700</pubDate>
    <lastBuildDate>Mon, 04 Mar 2024 21:00:45 +0700</lastBuildDate>
    <generator>Jekyll v3.9.3</generator>
    <image>
      <title>Olaf.AI</title>
      <link>http://localhost:4000/</link>
      <link>http://localhost:4000/images/avatar.jpg</link>
      <width>70</width>
      <height>70</height>
    </image>
    
      
      <item>
        <title>Duc Phan</title>
        <description>üî≠ AI/ CV Research &amp; Engineer

</description>
        <pubDate>Thu, 06 Jun 1996 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/about-us/ducpx</link>
        <guid isPermaLink="true">http://localhost:4000/about-us/ducpx</guid>
        
        
      </item>
      
      <item>
        <title>Phuc Phan</title>
        <description>üî≠ AI/ NLP Research &amp; Engineer

</description>
        <pubDate>Thu, 24 Jul 1997 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/about-us/phucpx</link>
        <guid isPermaLink="true">http://localhost:4000/about-us/phucpx</guid>
        
        
      </item>
      
    
      
    
      
      <item>
        <title>Attention Mechanism</title>
        <description>A potential issue with the Seq2Seq approach is that a neural network
needs to be able to compress all the necessary information of a source
sentence into a fixed-length vector (context vector). This may make it
difficult for the neural network to cope with long sentences, especially
those that are longer than the sentences in the training corpus. This
paper: ‚ÄúOn the Properties of Neural Machine Translation:
Encoder‚ÄìDecoder Approaches‚Äù
showed that indeed the performance of a basic encoder‚Äìdecoder
deteriorates rapidly as the length of an input sentence increases.

</description>
        <pubDate>Mon, 01 Sep 2014 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/stories/Attention</link>
        <guid isPermaLink="true">http://localhost:4000/stories/Attention</guid>
        
        
      </item>
      
      <item>
        <title>Align &amp; Translate with Transformers</title>
        <description>In this part, we are going to take a deep look into this paper: Jointly
Learning to Align and Translate with Transformer
Models which was published by
Apple Inc. in 2019. The official code for this paper can be found in the
official Fairseq GitHub repository:
fairseq/joint_alignment_translation.

</description>
        <pubDate>Wed, 04 Sep 2019 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/stories/Align_and_Translate</link>
        <guid isPermaLink="true">http://localhost:4000/stories/Align_and_Translate</guid>
        
        
      </item>
      
    
  </channel>
</rss>
