[
  
  
    
    
      
      
      {
        "collection": "About Us",
        "title"     : "Duc Phan",
        "url"       : "/about-us/ducpx",
        "date"      : "06/06/1996",
        "content": "üî≠ AI/ CV Research &amp;amp; Engineer\n\n\n  \n    \n      üì´\n      Github\n      LinkedIn\n      X (Twitter)\n      Facebook\n    \n  \n\n"
      },
    
      
      
      {
        "collection": "About Us",
        "title"     : "Phuc Phan",
        "url"       : "/about-us/phucpx",
        "date"      : "24/07/1997",
        "content": "üî≠ AI/ NLP Research &amp;amp; Engineer\n\n\n  \n    \n      üì´\n      Github\n      LinkedIn\n      X (Twitter)\n      Facebook\n    \n  \n\n\n"
      },
    
      
      
      {
        "collection": "About Us",
        "title"     : "Ha Trang",
        "url"       : "/about-us/trangvh",
        "date"      : "01/01/2001",
        "content": "üî≠ Marketing &amp;amp; Sales Specialist\n\n\n  \n    \n      üì´\n      LinkedIn\n      X (Twitter)\n      Facebook\n    \n  \n\n"
      },
    
      
      
      {
        "collection": "About Us",
        "title"     : "Anh Nguyen",
        "url"       : "/about-us/anhnt",
        "date"      : "17/02/2001",
        "content": "üî≠ BDM/ HRM\n\n\n  \n    \n      üì´\n      LinkedIn\n      X (Twitter)\n      Facebook\n    \n  \n\n"
      },
    
  
  
    
    
  
  
    
    
      
      
      {
        "collection": "Stories",
        "title"     : "Attention Mechanism",
        "url"       : "/stories/Attention",
        "date"      : "01/09/2014",
        "content": "A potential issue with the Seq2Seq approach is that a neural network\nneeds to be able to compress all the necessary information of a source\nsentence into a fixed-length vector (context vector). This may make it\ndifficult for the neural network to cope with long sentences, especially\nthose that are longer than the sentences in the training corpus. This\npaper: ‚ÄúOn the Properties of Neural Machine Translation:\nEncoder‚ÄìDecoder Approaches‚Äù\nshowed that indeed the performance of a basic encoder‚Äìdecoder\ndeteriorates rapidly as the length of an input sentence increases.\n\n\n    \n\n\nWhat we can see from the previous graph is that it works quite well for\nshort sentences, so we might achieve a relatively high BLEU score; but\nfor very long sentences, maybe longer than 30 or 40 words, the\nperformance comes down.\n\nIn order to address this issue, this paper ‚ÄúNeural Machine Translation\nby Jointly Learning to Align and\nTranslate‚Äù introduced an extension\nto the encoder‚Äìdecoder model called ‚ÄúAttention mechanism‚Äù in 2014. The\nmost important distinguishing feature of this approach from the basic\nencoder‚Äìdecoder is that it does not attempt to encode a whole input\nsentence into a single fixed-length vector. Instead, it encodes the\ninput sentence into a sequence of vectors and chooses a subset of these\nvectors adaptively while decoding the translation. This frees a neural\ntranslation model from having to squash all the information of a source\nsentence, regardless of its length, into a fixed-length vector. We show\nthis allows a model to cope better with long sentences.\n\nHere, we&#39;ll see the Attention Model which translates maybe a bit more\nlike humans. The way a human translator would translate a sentence is\nnot to first read the whole sentence and then memorize it and then\nregurgitate an English sentence from scratch. Instead, a human\ntranslator reads the first part of the given sentence, maybe generate\npart of the translation. Look at the second part, generate a few more,\nand so on. We kind of work part by part through the sentence, because\nit&#39;s just really difficult to memorize the whole long sentence like\nthat.\n\nIn the following parts, we will talk about the different variants of\nattention mechanisms that have been used in the field so far.\n\nGlobal Soft Attention\n\nAs we said earlier, the attention mechanism was first introduced by\nDzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio in 2014\nand published in this paper: ‚ÄúNeural Machine Translation by Jointly\nLearning to Align and Translate‚Äù\nunder the name ‚ÄúNeural Machine Translation By Jointly Learning To Align\nAnd Translate‚Äù which later defined as a global, soft attention. Here, we\nare going to explain the attention mechanism in more details as\nexplained in this paper: ‚ÄúEffective Approaches to Attention-based\nNeural Machine Translation‚Äù looks\npretty similar to the one mentioned earlier.\n\nTo explain how the attention mechanism works, let‚Äôs consider that we\nhave the following:\n\n\n  \n    The source sequence which is of length $n$ given by $x = \\left[ x_{1},‚Ä¶x_{n} \\right]$.\n  \n  \n    The target sequence which is of length $m$ given by $y = \\left[ y_{1},‚Ä¶y_{m} \\right]$.\n  \n  \n    Encoder hidden states $s = \\left[ s_{1},‚Ä¶s_{n} \\right]$.\n  \n  \n    Decoder hidden states $y = \\left[ h_{1},‚Ä¶h_{m} \\right]$.\n  \n\n\nNow, the attention mechanism works as shown in the following figure:\n\n\n    \n\n\nWhose steps goes like the following:\n\n\n  First, we calculate the alignment score by using the  attention score function \ndefined as $\\text{score}$ between the $j^{th}$\nencoder hidden state $s_{j}$ and the $i^{th}$ decoder hidden state $h_{i}$.\n\n\n\\[a_{i,j} = \\text{softmax}\\left( \\text{score}\\left( h_{i},\\ s_{j} \\right) \\right)\\]\n\nIn the next part, we are going to see the different variants of the\n$\\text{score}$ function.\n\n\n  Next, the context vector $c_{i}$ at position $i$ can be calculated as the\nweighted average of previous encoder hidden states and alignment vector $a_{i,j}$.\n\n\n\\[c_{i} = \\sum_{j = 0}^{n}{a_{i,\\ j}\\text{.}s_{j}}\\]\n\n\n  The source side context vector $c_{i}$ and the hidden state $h_{i}$ are\nconcatenated $\\left\\lbrack c_{i}\\ ;\\ h_{i} \\right\\rbrack$ and the non-linear\n$\\tanh$ activation function is applied to give the attention hidden vector \n $\\widetilde{h}_i$  where $W_{c}$ weights are learned in the training process:\n\n\n\\[{\\widetilde{h}}_{i} = \\tanh\\left( W_{c}.\\left\\lbrack c_{i}\\ ;\\ h_{i} \\right\\rbrack \\right)\\]\n\n\n  The attention hidden vector ${\\widetilde{h}}_{i}$ is passed through a\n$\\text{softmax}$ function to generate the probability distribution given by the\nfollowing formula where $W_{s}$ weights are learned in the training process:\n\n\n\\[P\\left( y_{i} \\middle| y_{&amp;lt; i},\\ x \\right) = \\text{softmax}\\left( W_{s}.{\\widetilde{h}}_{i} \\right)\\]\n\nScore Functions\n\nIn this part, we are going to enumerate the different score functions\n$\\text{score}\\left( h_{i},\\ s_{j} \\right)$ used in different papers\nwhich gives different flavors of the attention mechanism where:\n\n\n  \n    $s_{j}$: is the $j^{th}$ hidden state in the encoder architecture.\n  \n  \n    $h_{i}$: is the $i^{th}$ hidden state in the decoder architecture.\n  \n  \n    $n$: is the number of tokens in the encoder.\n  \n\n\n\n\n    \n        \n            Name\n            Function\n            Parameters\n            Reference\n        \n    \n    \n        Concat (additive)\n        $$v_{a}^T.tanh\\left( W_{a}\\left\\lbrack s_{j};h_{i} \\right\\rbrack \\right)$$\n        $$W_{a}$$\n        Luong et al.\n    \n    \n        Linear (additive)\n        $$v_{a}^{T}.\\tanh\\left( W_{a}s_{j} + U_{a}h_{i} \\right)$$\n        $$W_{a},\\ U_{a}$$\n        Bahdanau et al.\n    \n    \n        Bilinear (multiplicative)\n        $$h_{i}^{T}.W_{a}.s_{j}$$\n        $$W_{a}$$\n        Luong et al.\n    \n    \n        Dot (multiplicative)\n        $$h_{i}^{T}.s_{j}$$\n        -\n        Luong et al.\n    \n    \n        Scaled dot (multiplicative)\n        $$\\frac{1}{\\sqrt{n}}\\left( h_{i}^{T}.s_{j} \\right)$$\n        -\n        Vaswani et al.\n    \n    \n        Location-based\n        $$\\text{softmax}\\left( W_{a}.h_{i}^{T} \\right)$$\n        $$W_{a}$$\n        Luong et al.\n    \n\n\n\nNotes:\n\n\n  \n    The multiplicative and additive score functions generally give similar results.\n  \n  \n    The multiplicative score functions are faster in both computation and\nspace-efficiency since it is using efficient matrix multiplication techniques.\n  \n  \n    The additive score function performs much better when the input dimension is large.\n  \n\n\nSoft Vs. Hard Attention\n\nThe only difference between them is that hard attention picks one of the\nencoder hidden states $s = \\left[ s_{1},\\ ‚Ä¶\\ s_{n} \\right]$ rather\nthan a weighted average over all the inputs as in the soft attention\ndoes. The hard attention is given by:\n\n\\[c_{i} = \\underset{a_{i,j}}{\\arg\\max}\\left\\{ s_{1},s_{2},...s_{n} \\right\\}\\]\n\nWhile the soft attention is given by:\n\n\\[a_{i,j} = \\text{softmax}\\left( \\text{score}\\left( h_{i},\\ s_{j} \\right) \\right)\\]\n\n\\[c_{i} = \\sum_{j = 0}^{n}{a_{i,\\ j}\\text{.}s_{j}}\\]\n\nGlobal Vs. Local Attention\n\nThe global attention, discussed before, is called ‚Äúglobal‚Äù because each\ndecoder hidden state takes $h_{i}$ into consideration ‚Äúall‚Äù of the\nencoder hidden states $s = \\left[ s_{1},‚Ä¶s_{n} \\right]$ while\ncomputing the context vector $c_{i}$. This can be both computationally\nexpensive and many times impractical when the source length $n$ is\nlarge.\n\nLuong et al. in their paper: ‚ÄúEffective Approaches to Attention-based\nNeural Machine Translation‚Äù\nintroduced the local attention which considers a small window of size\n$D$ of the encoder hidden states\n$s = \\left[ s_{p_{i} - D},‚Ä¶s_{p_{i} + D} \\right]$ when computing the\ncontext vector $c_{i}$ instead of all the hidden states.\n\n\n    \n\n\nWhose steps goes like the following:\n\n\n  First, the model predicts an aligned position $p_{i}$ for\neach target word in the decoder at time $i$ using the following formula:\n\n\n\\[p_{i} = n.\\text{sigmoid}\\left( v_{p}^{T}.\\tanh\\left( W_{p}.h_{i} \\right) \\right)\\]\n\nWhere $W_{p}$ and $v_{p}$ are the model parameters to be learned to\npredict the position and $n$ is the length of the source sequence and.\nAnd $p_{i}$ is a number within $\\lbrack 0,n\\rbrack$.\n\n\n  Next, the alignment score is going to be calculated for each position $s$\nin the predefined window as before. However, to favor alignment points near\n$p_{i}$, we place a Gaussian distribution centered around $p_{i}$ and with\nstandard deviation $\\sigma = \\frac{D}{2}$. Now, our alignment weights are\ndefined as:\n\n\n\\[a_{i,j} = \\text{softmax}\\left( \\text{score}\\left( h_{i},\\ s_{j} \\right) \\right)\\exp\\left( - \\frac{\\left( s - p_{i} \\right)^{2}}{2\\sigma^{2}} \\right)\\]\n\n\n  Then, the context vector $c_{i}$ is then derived as a weighted average over\nthe set of source hidden states within the window\n$\\left\\lbrack p_{i} - D,p_{i} + D \\right\\rbrack$ where $D$ is the window size.\n\n\n\\[c_{i} = \\sum_{j = p_{i} - D}^{p_{i} + D}{a_{i,\\ j}\\text{.}s_{j}}\\]\n\nKey-Value Attention\n\nKey-value attention is first created by Daniluk et\nal. in 2017 which is another\nvariant of attention mechanism which splits the encoder hidden layers\ninto key-value pairs where the keys are used for attention distribution\nand the values for context representation.\n\n\n    \n\n\nNow, if the attention mechanism is self-attention, then we will\ncreate another vector called the ‚Äúquery vector‚Äù. And if the attention\nmechanism is not self-attention, then the query vector will be created\nat the decoder network.\n\nThe Key-value attention mechanism can be calculated by following these\nsteps:\n\n\n  Create three vectors from word-embedding vectors. So for each word,\nwe create a Query vector, a Key vector, and a Value\nvector. These vectors are created by multiplying the embedding by\nthree matrices that can be learned.\n\n\n\n    \n\n\nWe can do that using broadcasting which could make things faster like so:\n\n\n    \n\n\n\n  Then, we apply the attention mechanism which can be summarized in\nthis equation:\n\n\n\n    \n\n\nSo, the calculations will look like so:\n\n\n    \n\n\nMulti-head Attention\n\nA multi-headed attention mechanism only means that we need to perform\nthe same attention calculation we outlined above just multiple times\nwith different weight matrices. By doing that, we end up with multiple\ndifferent Z matrices. So, we need to concatenate them together to get\none big matrix as shown below:\n\n\n  concatenate them together to get one big matrix as shown below:\n\n\n\n    \n\n\n\n  Finally, we need to multiply this big matrix with a weight matrix\nthat was trained jointly with the model to get a matrix that\ncaptures information from all attention heads.\n\n\n\n    \n\n\nHierarchical Attention\n\nThis type of architecture was proposed by Yang et al. in his paper:\nHierarchical Attention Networks for Document\nClassification\npublished in 2016. So, it‚Äôs an attention mechanism on the sentence\nlevel.\n\n[YOU CAN READ MORE IN THE REFERENCE BOOK STARTING FROM PAGE 418 .]\n"
      },
    
      
      
      {
        "collection": "Stories",
        "title"     : "Align &amp; Translate with Transformers",
        "url"       : "/stories/Align_and_Translate",
        "date"      : "04/09/2019",
        "content": "In this part, we are going to take a deep look into this paper: Jointly\nLearning to Align and Translate with Transformer\nModels which was published by\nApple Inc. in 2019. The official code for this paper can be found in the\nofficial Fairseq GitHub repository:\nfairseq/joint_alignment_translation.\n\nSince translation and alignment tasks are very closely related, the\npublishers of this paper presented an approach to train a Transformer\nmodel to produce both accurate translations and alignments. The\nalignments are extracted from the attention probabilities learned during\ntraining. This approach produced competitive results compared to other\nstatistical alignment tools such as GIZA++ without sacrificing\ntranslation accuracy.\n\nAttention Matrix\n\n\n    \n\nIn this part, we are going to recap how the encoder-decoder multi-head\nattention sub-layer works and how the attention matrix is represented.\nLet $d_{\\text{emb}},\\ d_{k},\\ d_{v},\\ H$ denote the embedding dimension,\ndimensions of the key and value projections and number of heads,\nrespectively.\n\nThe multi-head attention between a source sentence of $M$ tokens and a\ntarget sentence of $N$ tokens is calculated via the following equation:\n\n\\[\\text{MultiHead}\\left( Q,\\ K,\\ V \\right) = Concat\\left( A_{1},\\ ...\\ A_{H} \\right)W^{O}\\]\n\n\\[A_{i} = Attention\\left( QW_{i}^{Q},\\ KW_{i}^{K},\\ VW_{i}^{V} \\right) = \\text{softmax}\\left( \\frac{QW_{i}^{Q}\\left( KW_{i}^{K} \\right)^{T}}{\\sqrt{d_{k}}} \\right)VW_{i}^{V}\\]\n\nWhere:\n\n\n  \n    $A_{i} \\in \\mathbb{R}^{M \\times N}$ is the Attention matrix of the\n$i^{th}$ head out of $H$ heads.\n  \n  \n    $W_{i}^{Q} \\in \\mathbb{R}^{d_{\\text{emb}} \\times d_{k}},\\ W_{i}^{K} \\in \\mathbb{R}^{d_{\\text{emb}} \\times d_{k}},\\ W_{i}^{V} \\in \\mathbb{R}^{d_{\\text{emb}} \\times d_{v}},\\ W^{O} \\in \\mathbb{R}^{Nd_{v} \\times d_{\\text{emb}}}$\nare projections matrices of the $i^{th}$ head. All of them are learned\nparameters.\n\n    Each alignment matrix $A_{i}$ gives a probability distribution over\nsource/target pair. This distribution is then converted to a\ndiscrete alignment by aligning each target word to the corresponding\nsource word with the highest attention probability.\n  \n\n\nLayer Average Baseline\n\nAs we can see from the previous sections, there are multiple heads and\nmultiple layers in the transformer architecture. Which alignment should\nbe considered the correct alignment of the input pair?\n\nIn the paper, they found out that the best results come from\naveraging all attention heads of the second-to-last layer out\nof the six decoder layers they used. They refer to this as ‚Äúthe layer\naverage baseline‚Äù.\n\nMulti-task Learning\n\nMulti-task learning is a subfield of machine learning in which multiple\nlearning tasks are solved at the same time, while exploiting\ncommonalities and differences across tasks. In this paper, the\npublishers exploited the correlation between Translation and Alignment\ntasks to create a multi-objective loss function $\\mathcal{L}$:\n\n\\[\\mathcal{L} = \\mathcal{L}_{t} + \\lambda\\mathcal{L}_{a}\\left( A \\right)\\]\n\nWhere:\n\n\n  \n    $\\lambda$ is a hyper-parameters to weigh the importance of these two\nloss functions.\n  \n  \n    $\\mathcal{L}_{t}$ is the translation loss function given a source sentence $s = \\left[ s_{1}\\text{‚Ä¶}s_{M} \\right]$ and a target sentence $t = \\left[ t_{1}\\text{‚Ä¶}t_{N} \\right]$:\n  \n\n\n\\[\\mathcal{L}_{t} = - \\frac{1}{N}\\sum_{i = 1}^{N}{\\log\\left( p\\left( t_{i} \\middle| s,\\ t_{&amp;lt; i} \\right) \\right)}\\]\n\n\n  $\\mathcal{L}_{a}$ is the alignment loss:\n\n\n\\[\\mathcal{L}_{a}\\left( A \\right) = - \\frac{1}{N}\\sum_{i = 1}^{N}{\\sum_{j = 1}^{M}G_{i,j}^{p}\\text{.}\\log\\left( A_{i,j} \\right)}\\]\n\nThe alignment loss is calculated in a supervised manner using the\nlayer-average baseline as the true alignment. First, they converted the\ntrue alignments into a probability distribution. Let $G_{N \\times M}$\ndenote a 0-1 matrix such that $G_{i,j} = 1$ if the $j^{th}$ source token is\naligned to the $i^{th}$ target token. Then, they normalized the rows of $G$\nto get matrix $G^{p}$.\n\nCalculating the translation loss $\\mathcal{L}_{t}$ needs a masked target\nwhile calculating the alignment loss $\\mathcal{L}_{a}$ needs the target\nwithout masking. That‚Äôs why the loss function is implemented by\nexecuting two forward passes of the decoder model: One with the masking\nof the future target tokens for computing the translation loss\n$\\mathcal{L}_{t}$ and the other one with no masking for computing the\nalignment loss $\\mathcal{L}_{a}$.\n\nResults\n\nTo achieve state-of-the-art results, they change to the big transformer\nconfiguration with an embedding size of 1024 and 16 attention heads, 6\nlayers in the encoder and decoder. The total number of parameters is\n213M. They trained the transformer with a batch size of 7168 tokens on\n64 Volta GPUs for 30k updates and apply a learning rate of $1e^{-3}$,\n$\\beta_{1} = 0.9$, $\\beta_{2} = 0.98$. The dropout probability is set to\n$0.3$.\n\nAs we can see from this table, the multi-task learning with full context\n(knowing of all target words) and with alignment generated from GIZA++\ntoolkit achieved the least AER while maintaining the translation BLEU\nscore on two different language-pairs.\n\n\n    \n\n"
      }
    
  
]
