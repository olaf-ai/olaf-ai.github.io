[
  
  
    
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "RNN: Recurrent Neural Networks",
        "url"       : "/language-modeling/RNN",
        "date"      : "19/09/1985",
        "content": "The neural n-gram language model we&#39;ve seen earlier was trained using\nthe a window-sized subset of the previous tokens. And this falls short\nwith long sentences as where the contextual dependencies are longer than\nthe window size. Now, we need a model that is able to capture\ndependencies outside the window. In other words, we need a system that\nhas some kind of memory to save these long dependencies.\n\nHere, we are going to talk about RNN or Recurrent Neural Network.\nRecurrent Neural Networks (RNN) are very effective for Natural Language\nProcessing and other sequence tasks because they can read inputs (such\nas words) one at a time, and remember some information/context through\nthe hidden layer activations that get passed from one time-step to the\nnext. This allows a uni-directional RNN to take information from the\npast to process later inputs. A bi-direction RNN can take context from\nboth the past and the future.\n\nBut why we are going to use Recurrent Neural Network (RNN) and not the\nvanilla feed-forward type of neural networks?\n\n\n  \n    First, the vanilla neural network has fixed input and output. But in\napplications like the ones above, it is required to have a flexible\nneural network architecture with different inputs and outputs. For\nexample, a sentiment analysis application should have a flexible\nneural network that can deal with different sentence lengths.\n  \n  \n    The standard neural network losses an important criterion which is\nsharing information between different layers. Unlike the RNN that\ncan connect to any neuron in any layer. This criterion is called\n“Cyclical Connections”.\n  \n\n\nHistory Background\n\nHere, we are going to talk about how the RNN has evolved in the past few\ndecades. The first wave of artificial neural networks started in the\nmid-1980s. After that wave, it became clear that feed-forward networks\nbecame are limited since they are unable to capture temporal\ndependencies, which are dependencies that change over time. Biological\nneural networks have recurring connections, so appling recurrence to\nartificial intelligence made natural sense.\n\nThe first time to add memory to neural networks were the TDNN or “Time\nDelay Neural Network” in 1989. Then after one year in 1990, Jeffrey\nLocke Elman created Elman’s Network or (Simple RNN). Then, Michael Irwin\nJordan produces a network that is similar to Elman’s Network and called\nit “Jordan’s Network” … original!!\n\nAll these networks suffer from something called “Vanishing Gradients”\nwhich means that they can’t capture information with span more than 8 or\n10 steps back. In the mid-1990s, Long Short-Term Memory, or LSTM for\nshort, were invented to address this very problem. The key idea in LSTM\nwas the idea that some signals can be kept fixed by using gates, and\nreintroduced or not. After than GRU or Gated Recurrent Unit was invented\nto optimize LSTM in 2014 by Kyunghyun Cho.\n\nTypes of RNN\n\nThere are different types of RNN:\n\n\n  One-to-many RNN: This neural network is used when we have just\none input and multiple outputs like Music Generation\nApplication which has just one input like the genre of the music,\nand the output is a sequence of music notes.\n\n\n\n    \n\n\n\n  Many-to-one RNN: This neural network is used when we have\nmany inputs and just one output like the sentiment\nanalysis applications which have a sequence of sentences, and the\noutput is a rate of one to five stars.\n\n\n\n    \n\n\n\n  \n    Many-to-many RNN: This neural network is used when we have\nmany inputs and many outputs. And we have two types in\nthese neural network:\n\n    \n      \n        When the input size is the same as the output size like\nin Word Embedding problem.\n      \n      \n        When the input size is different than the output size\nlike in Machine Translation Applications which takes a sentence\nof a certain length and returns another sentence in another\nlanguage which probably has different length.\n      \n    \n  \n\n\n\n    \n\n\n\n  Notes:\n\n  \n    At one-to-many RNNs, we take the output and insert it back as input.\n  This operation is called “Sampling”.\n  \n\n  \n    \n\n\n  \n    At machine translation RNN models, we divide the RNN into two parts,\n  the first is the “encoder” part which takes the original sentence.\n  The second part is the “decoder” part which returns the translated\n  sentence. This architecture is called the “Autoencoding\n  architecture”.\n  \n\n\nRNN Cells\n\n\n    \n\n\nAs you can see, a recurrent neural network can be seen as the repetition\nof a single cell (RNN cell). The following figure describes the\noperations for a single time-step of an RNN cell. The basic RNN cell\ntakes as input $x^{\\left\\langle t \\right\\rangle}$ (current input) and\n$a^{\\left\\langle t - 1 \\right\\rangle}$ (previous hidden state containing\ninformation from the past), and outputs\n$a^{\\left\\langle t \\right\\rangle}$ which is given to the next RNN cell\nand used to predict $y^{\\left\\langle t \\right\\rangle}$.\n\n\n    \n\n\nThe RNN forward propagation consists of several operations:\n\n\n  \n    Initialize $a$ vector that will store all the hidden states computed\nby the RNN. Also, initialize the &quot;next&quot; hidden state as $a_{0}$\n(initial hidden state).\n  \n  \n    Start looping over each time step, your incremental index is $t$:\n\n    \n      \n        Calculate the cell operations.\n      \n      \n        Store the &quot;next&quot; hidden state in $a$ ($t^{th}$ position).\n      \n      \n        Store the prediction in $y$.\n      \n    \n  \n\n\nPros &amp;amp; Cons\n\nRNNs have several advantages:\n\n\n  \n    They can process input sequences of any length.\n  \n  \n    They have some kind of memory as computation for step $t$ can (in\ntheory) use information from many steps back.\n  \n  \n    The same weights are used for all different inputs which means that\nthe number of learn-able parameters are reduced and that number\ndoesn’t scale with the size of the data unlike the traditional\nlanguage models.\n  \n\n\nBut they also have some disadvantages:\n\n\n  Computation is slow - because it is sequential, it cannot be parallelized.\n\n\nIn practice, it is difficult to access information from many steps back\ndue to problems like vanishing and exploding gradients.\n\nLSTM Cell\n\nAs we have mentioned before that a standard recurrent neural network\nwill work well for short sentences, but it suffers from vanishing\ngradient problem. So, it works best when each output\n$y^{\\left\\langle t \\right\\rangle}$ can be estimated using mainly “local”\ncontext (meaning information from inputs\n$x^{\\left\\langle t^{‘} \\right\\rangle}$ where $t’$ is not too far from\n$t$).\n\nLSTM stands for Long Short-Term Memory network. It was proposed in 1997\nby Sepp Hochreiter and Jürgen Schmidhuber. Here, you will build RNN\nusing LSTM cells which is a more complex than standard RNN model. It is\nbetter at addressing vanishing gradients as it is better remembering a\npiece of information and keep it saved for many time-steps. The\nfollowing figure shows the operations of an LSTM-cell.\n\n\n    \n\n\nAs you can see, the LSTM has a lot of modifications over RNN cell. In\nRNN cell, we had just two inputs ($x^{\\left\\langle t \\right\\rangle}$,\n$a^{\\left\\langle t - 1 \\right\\rangle}$) and there were no gates. In\nLSTM, there are three inputs ( input $x^{\\left\\langle t \\right\\rangle}$,\ncell state $c^{\\left\\langle t - 1 \\right\\rangle}$, and activation\n$a^{\\left\\langle t - 1 \\right\\rangle}$). The cell state here represents\nthe long short-term memory of this architecture.\n\nAll these equations can be summarized into the following ones knowing\nthat\n$\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack$\nmeans that the activating parameter of the previous time-step\n$a^{\\left\\langle t - 1 \\right\\rangle}$ is concatenated with the input\nvector $x^{\\left\\langle t \\right\\rangle}$:\n\n\\[c^{\\left\\langle t \\right\\rangle} = \\Gamma_{f}^{\\left\\langle t \\right\\rangle} \\ast c^{\\left\\langle t - 1 \\right\\rangle} + \\Gamma_{u}^{\\left\\langle t \\right\\rangle} \\ast tanh\\left( W_{c}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{c} \\right)\\]\n\n\\[a^{\\left\\langle t \\right\\rangle} = \\Gamma_{o}^{\\left\\langle t \\right\\rangle} \\ast tanh\\left( Wc^{\\left\\langle t \\right\\rangle} \\right)\\]\n\nwhere\n\n$\\Gamma_{f}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{f}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{f} \\right)$\n(Forget Gate)\n\n$\\Gamma_{u}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{u}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{u} \\right)$\n(Update Gate)\n\n$\\Gamma_{o}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{o}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{o} \\right)$\n(Output Gate)\n\nNow, let’s get into these three gates in more details:\n\n\n  \n    Forget Gate:\n\n    \n      This gate controls how much of the information of the\nprevious cell state\n$c^{\\left\\langle t - 1 \\right\\rangle}$ should be forgot or\nkept while calculating the current cell state\n$c^{\\left\\langle t \\right\\rangle}$.\n    \n  \n\n\n\\[\\Gamma_{f}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{f}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{f} \\right)\\]\n\n\n  \n    $W_{f}$ are weights that govern the gate’s behavior and they are\ntrainable.\n  \n  \n    The output vector $\\Gamma_{f}^{\\left\\langle t \\right\\rangle}$ has\nvalues from $0$ to $1$ since it uses the sigmoid activation\nfunction. So, if one of the features of\n$\\Gamma_{f}^{\\left\\langle t \\right\\rangle}$ is $0$ (or close to\n$0$), then it means that the LSTM should forget that\npiece of information in the corresponding component of\n$c^{\\left\\langle t - 1 \\right\\rangle}$ while calculating the value\nfor $c^{\\left\\langle t \\right\\rangle}$. If one of the features is\n$1$, then it will keep the information.\n  \n  \n    Update Gate (Input Gate):\n\n    \n      Similar to the forget gate, this gate controls how much of\nthe information of the current input state\n$\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack$\nshould be used while calculating the current cell\n$c^{\\left\\langle t \\right\\rangle}$ state matter now.\n    \n  \n\n\n\\[\\Gamma_{u}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{u}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{u} \\right)\\]\n\n\n  \n    $W_{u}$ are weights that govern the gate’s behavior and they are\ntrainable.\n  \n  \n    The output vector $\\Gamma_{u}^{\\left\\langle t \\right\\rangle}$ has\nvalues from $0$ to $1$ since it uses the sigmoid activation\nfunction. So, if one of the features of\n$\\Gamma_{u}^{\\left\\langle t \\right\\rangle}$ is $1$ (or close to\n$1$), then it means that the LSTM should update that\npiece of information in the corresponding component of the input\n$\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack$\nwhile calculating the value for $c^{\\left\\langle t \\right\\rangle}$.\nIf one of the features is $0$, then it won’t use this feature.\n  \n  \n    Output gate:\n\n    \n      This gate controls how much of the information of the current\ncell state $c^{\\left\\langle t \\right\\rangle}$ should be used for the\noutput/activation.\n    \n  \n\n\n\\[\\Gamma_{o}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{o}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{o} \\right)\\]\n\n\n  \n    $W_{o}$ are weights that govern the gate’s behavior and they are\ntrainable.\n  \n  \n    The output vector $\\Gamma_{o}^{\\left\\langle t \\right\\rangle}$ has\nvalues from $0$ to $1$ since it uses the sigmoid activation\nfunction. So, if one of the features of\n$\\Gamma_{o}^{\\left\\langle t \\right\\rangle}$ is $1$ (or close to\n$1$), then it means that the LSTM should use that piece of\ninformation in the corresponding component of the cell state\n$c^{\\left\\langle t \\right\\rangle}$ while calculating the value for\nthe output $a^{\\left\\langle t \\right\\rangle}$. If one of the\nfeatures is $0$, then it won’t use this feature.\n  \n\n\n\n  Note:\nSome researchers have found out that the parameter\n$c^{\\left\\langle t - 1 \\right\\rangle}$ needs to be concatenated in the\nforget gate. So, instead of using just\n$x^{\\left\\langle t \\right\\rangle}$ and\n$a^{\\left\\langle t \\right\\rangle}$ in the forget gate, we need also to\nuse $c^{\\left\\langle t - 1 \\right\\rangle}$ as shown in the following\nequation which could increase the accuracy. This is known as the\n“peephole connections”:\n\n\\[\\Gamma_{f}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{f}\\left\\lbrack c^{\\left\\langle t - 1 \\right\\rangle},a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{f} \\right)\\]\n\n\nThis link states some interesting\nfacts about LSTM, give it a read!!!\n\nGRU Cell\n\nGRU stands for Gated Recurrent Unit. GRU was introduced in 2014 by\nKyunghyun Cho as another solution to the vanishing gradient problem\nbeside LSTM cell. The main objective behind creating GRUs is to create a\nsimpler cell than LSTM without sacrificing the performance too much.\n\nGRU&#39;s performance on certain tasks of music modeling and speech signal\nmodeling was found to be similar to that of LSTM. GRUs have been shown\nto exhibit even better performance on certain smaller datasets. However,\nas shown by Gail Weiss &amp;amp; Yoav Goldberg &amp;amp; Eran Yahav, the LSTM is\n&quot;strictly stronger&quot; than the GRU as it can easily perform unbounded\ncounting, while the GRU cannot. That&#39;s why the GRU fails to learn\nsimple languages that are learnable by the LSTM.\n\nHe following The structure of GRU is like the following:\n\n\n    \n\n\nIn GRU, we don’t have cell states as the one with LSTM. Here, we only\nhave activation (hidden state). And GRUs have two gates (Update Gate,\nReset Gate) unlike LSTM which have three gates. The main equations used\nwith GRU are the following ones:\n\n\\[a^{\\left\\langle t \\right\\rangle} = c^{\\left\\langle t \\right\\rangle} = \\left( 1 - \\Gamma_{u}^{\\left\\langle t \\right\\rangle} \\right) \\ast c^{\\left\\langle t - 1 \\right\\rangle} + \\Gamma_{u} \\ast {\\overset{\\sim}{c}}^{\\left\\langle t \\right\\rangle}\\]\n\n\\[{\\overset{\\sim}{c}}^{\\left\\langle t \\right\\rangle} = tanh\\left( Wx^{\\left\\langle t - 1 \\right\\rangle} + U\\left( \\Gamma_{r} \\ast c^{\\left\\langle t \\right\\rangle} \\right) + b_{c} \\right)\\]\n\n$\\Gamma_{u}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{u}x^{\\left\\langle t - 1 \\right\\rangle} + U_{u}a^{\\left\\langle t - 1 \\right\\rangle} + b_{u} \\right)$\n(Update Gate)\n\n$\\Gamma_{r}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{r}x^{\\left\\langle t - 1 \\right\\rangle} + U_{r}a^{\\left\\langle t - 1 \\right\\rangle} + b_{u} \\right)$\n(Reset Gate)\n\nNow, let’s get into these three gates in more details:\n\n\n  \n    Update Gate:\n\n    \n      This gate controls the what is kept from previous hidden\nstate $c^{\\left\\langle t - 1 \\right\\rangle}$ and is\nupdated to the new candidate update\n${\\overset{\\sim}{c}}^{\\left\\langle t \\right\\rangle}$ while\ncalculating the current hidden state $c^{\\left\\langle t \\right\\rangle}.\n    \n  \n\n\n\\[\\Gamma_{u}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{u}x^{\\left\\langle t - 1 \\right\\rangle} + U_{u}a^{\\left\\langle t - 1 \\right\\rangle} + b_{u} \\right)\\]\n\n\n  \n    $W_{u}$ and $U_{u}$ are weights that govern the gate’s behavior and\nthey are trainable.\n  \n  \n    The output vector $\\Gamma_{u}^{\\left\\langle t \\right\\rangle}$ has\nvalues from $0$ to $1$ since it uses the sigmoid activation\nfunction. So, if one of the features of\n$\\Gamma_{u}^{\\left\\langle t \\right\\rangle}$ is $1$ (or close to\n$1$), then it means that the GRU should update that\npiece of information in the corresponding component of hidden\nstate $c^{\\left\\langle t \\right\\rangle}$ using the update\ncandidate ${\\overset{\\sim}{c}}^{\\left\\langle t \\right\\rangle}$. If\none of the features is $0$, then update using the old value\nof the hidden state $c^{\\left\\langle t - 1 \\right\\rangle}$.\n  \n  \n    Reset Gate:\n\n    \n      This gate controls what parts of the previous hidden state\nshould be used to compute the new hidden state\n$c^{\\left\\langle t \\right\\rangle}$.\n    \n  \n\n\n\\[\\Gamma_{r}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{r}x^{\\left\\langle t - 1 \\right\\rangle} + U_{r}a^{\\left\\langle t - 1 \\right\\rangle} + b_{u} \\right)\\]\n\n\n  \n    $W_{r}$ and $U_{r}$ are weights that govern the gate’s behavior and\nthey are trainable.\n  \n  \n    The output vector $\\Gamma_{r}^{\\left\\langle t \\right\\rangle}$ has\nvalues from $0$ to $1$ since it uses the sigmoid activation\nfunction. So, if one of the features of\n$\\Gamma_{r}^{\\left\\langle t \\right\\rangle}$ is $0$ (or close to\n$0$), then it means that the GRU should reset that\npiece of information in the corresponding component of\n$c^{\\left\\langle t - 1 \\right\\rangle}$ while calculating the value\nfor $c^{\\left\\langle t \\right\\rangle}$. If one of the features is\n$1$, then it will keep the information.\n  \n\n\n\n  Note:\nAs a rule of thumb, use LSTM cells in your model unless you care much\nabout the size of the model or the memory needed. GRUs have fewer\nparameters to compute\n\n\nHyper-parameters\n\nIn sequence models, there are three main choices that we need to make\nwhen we want to build an RNN:\n\n\n  \n    Choosing the cell type either standard RNN, GRU or\nLSTM. Now, it’s clear that both GRU and LSTM are both much\nbetter than standard RNN, but which one is even better? Actually,\nthat depends on the task and the type of the dataset. According to\nthis paper: “Visualizing and Understanding Recurrent\nNetworks” by Andrej\nKarpathy, both LSTM and GRU were tested on two datasets: in the\nfirst dataset, GRU was better in all used sizes and in the second,\nGRU was better in some sizes and worse in other. So, when creating\nyour own model, you should try both.\n  \n  \n    Choosing number of layers that we need to stack. And in the same\npaper, the number of layers stacked together is the best at two\nlayers and when increasing it to three layers, it gets mixed\nresults. So, when creating your own model, you should try using\ntwo and three layers. When creating advanced sequence models like\nCTC, we usually use five or even seven layers often with the use\nof LSTM cells.\n  \n  \n    And in case of using word embedding, then another hyper-parameter is\nadded which is the embedding size. Experimental results in this\npaper: “How to Generate a Good Word\nEmbedding?” have shown that\nthe larger the word embedding is, the better; at least we reach\nthe size of $200$. So, we should try different sizes starting from\n$50$ till $200$ or $300$ as google did in this paper:\n“Distributed Representations of Words and Phrases and their\nCompositionality”\nor even $500$.\n  \n\n\nIn the following table, we are going to see different RNN architectures\non different tasks:\n\n\n\n    \n        \n            Task\n            Cell\n            #Layers\n            Layer Size\n            Embedding Size\n            Source\n        \n    \n    \n        Speech Recognition (500K vocabulary)\n        LSTM\n        7\n        1000\n        -\n        paper\n    \n    \n        Speech Recognition (82K vocabulary)\n        LSTM\n        5\n        600\n        -\n        paper\n    \n    \n        Speech Recognition (small vocabulary)\n        LSTM\n        1, 3, 5\n        250\n        -\n        paper\n    \n    \n        Seq2Seq (160K → 80k)\n        LSTM\n        4\n        1000\n        1000\n        paper\n    \n    \n        Image Captioning\n        LSTM\n        -\n        512\n        -\n        paper\n    \n    \n        Image Generation\n        LSTM\n        -\n        256, 400, 800\n        -\n        paper\n    \n    \n        Question Answering\n        LSTM\n        2\n        500\n        300\n        paper\n    \n    \n        Text Summarization (119K → 68K)\n        GRU\n        -\n        200\n        100\n        paper\n    \n\n\n\n\nGradient Clipping\n\nRecall that our loop structure usually consists of a forward pass, a\ncost computation, a backward pass, and a parameter update. Before\nupdating the parameters, we will need to perform gradient clipping when\nneeded to make sure that your gradients are not &quot;exploding” (taking on\noverly large values).\n\n\n    \n\n\nSo, we will implement a function that takes in the gradients and returns\na clipped version of gradients if needed. There are different ways to\nclip gradients; we will use a simple element-wise clipping procedure, in\nwhich every element of the gradient vector is clipped to lie between\nsome range $\\left\\lbrack - N,N \\right\\rbrack$.\n\ndef clip(gradient_lst, max_value):\n... for gradient in gradient_lst:\n...     np.clip(gradient, -max_value, max_value, out= gradient)\n...     return gradient_lst\n\n\nIf you want to use it on our variables, we can do like so:\n\ndWax, dWaa, dWya, db, dby = clip(dWax, dWaa, dWya, db, dby, 10)\n\n\nHere, we provided the max_value as $10$. If any component of the\ngradient vector is greater than $10$, it would be set to $10$; and if\nany component of the gradient vector is less than $- 10$, it would be\nset to $- 10$. If it is between $- 10$ and $10$, it is left alone.\n\nSampling\n\nthe Sampling is the process of use the output of a certain neuron as an\ninput to the following neurons. It’s used with generative models where\nyou need to generate such as language models. Let’s see how it’s done:\n\n\n  \n    First, we input the usual $x^{\\left\\langle 1 \\right\\rangle}$ and\n$a^{\\left\\langle 0 \\right\\rangle}$, apply the activation function\nand get the output.\n  \n  \n    Now our first time stamp $y^{\\left\\langle 1 \\right\\rangle}$will have\nsome max probability over possible outputs, so we choose a randomly\nsample according to the probabilities of the possible outputs using,\nfor example, the numpy command np.random.choice.\n  \n  \n    Next we then go on to the second time step which is expecting\n$y^{\\left\\langle 1 \\right\\rangle}$ as input to the next time-step.\n  \n  \n    And so on…\n  \n\n\nExample, let&#39;s say that we are creating a language model. And after we\nsampled the first word, the first word happened to be “the”, which is\nvery common choice of first word. Then we pass “the” as\n$x^{\\left\\langle 2 \\right\\rangle}$. And now we’re trying to figure out\nwhat is the chance of the second word given that the first word is\n“the”. Then we again use this type of sampling function to sample\n$y^{\\left\\langle 2 \\right\\rangle}$ and so on.\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Neural N-gram Language Model",
        "url"       : "/language-modeling/Neural_N-gram",
        "date"      : "09/02/2003",
        "content": "As we discussed before, the n-gram language model has a few problems\nlike the data sparsity and the big storage need. That’s why these\nproblems were first tackled by Bengio et al in 2003 and published under\nthe name “A Neural Probabilistic Language\nModel”,\nwhich introduced the first large-scale deep learning for natural\nlanguage processing model. This model learns a distributed\nrepresentation of words, along with the probability function for word\nsequences expressed in terms of these representations. The idea behind\nthis architecture is to deal with the language model task as if it is a\nclassification problems where:\n\n\n  \n    The input is a window-sized subset of the previous tokens.\n  \n  \n    The output is the current token which could be any token from the\nvocabulary $V$.\n  \n  \n    Each token is represented using a one-hot vector.\n  \n  \n    The loss function is the cross entropy.\n  \n\n\nThe following figure shows a simplified version of the neural network\narchitecture that was created by Yoshua Bengio in 2003. In the original\nversion of the model, Bengio used the input word vectors with both the\nhidden layer and the output layer. This simplified version concatenated\nword embeddings for the input words:\n$e = \\left\\lbrack e^{\\left( 1 \\right)};e^{\\left( 2 \\right)};e^{\\left( 3 \\right)};e^{\\left( 4 \\right)} \\right\\rbrack$,\nthe red layer signifies the hidden layer:\n$h = f\\left( We + b_{1} \\right)$ , and the green output distribution is\na softmax over the vocabulary:\n$ŷ = \\text{softmax}\\left( Uh + b_{2} \\right)$.\n\n\n    \n\n\nAnd despite this model is way faster than other models and simpler to\nimplement, there were still some problems that need to be fixed:\n\n\n  \n    The fixed window is still a problem since some sentences need bigger\nwindows to catch the context. So, no matter how big your window\nis, it will never be enough for some sentences.\n  \n  \n    $e^{\\left( 1 \\right)},e^{\\left( 2 \\right)}$, … etc. are multiplied\nby completely different weights in W which means that the weight\nlearning in one section is not shared with the others and that’s\ncounter-intuitive. As shown in the following figure, we can see\nthat $e^{\\left( 1 \\right)}$ will only by multiplied by the blue\nregion of the weight matrix $W$, and $e^{\\left( 2 \\right)}$ will\nbe only multiplied by the green region, and so on.\n  \n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Tree Recursive Neural Network",
        "url"       : "/language-modeling/Tree_Recursive_NN",
        "date"      : "28/07/2011",
        "content": "Tree Recursive Neural Network is a model created by Richard Socher et al.\nand published in this paper: Parsing Natural Scenes and Natural Language\nwith Recursive Neural\nNetworks.\nThe main idea behind Tree Recursive Neural Network is to provide a\nsentence embedding that could represent the meaning of the sentence the\nsame way we did with word embedding. So, two sentences that of different\nwords like “the country of my birth” and “the place where I was born”\nwill have similar vector despite having totally different words. The\nmeaning vector of a sentence is determined by actually two things:\n\n\n  \n    The meaning of the words forming this sentence.\n  \n  \n    The rules/structure that combined these words.\n  \n\n\nBack in the old days, we used certain rules determined by linguists to\nform a grammatical structure for a given sentence. The algorithm that\nforms such a tree is called “CKY” and it was widely used for “parsing”\nwhich is to form a binary tree for a given sentence that describes the\nsyntactic structure of the given sentence. So, given a sentence like\n“the day of my birth”, CKY will return a binary tree that looks like\nthis:\n\n\n    \n\n\nBut now in Tree Recursive NN, we are going to use word embeddings and\nRNN to from a tree of a given sentence. The same sentence will appear\nlike the following figure. As we can see, it has the same structure as\nbefore but an additional information which is the “sentence embedding”.\nThe embedding vector at the top of the structure is what makes this\nmodel super important:\n\n\n    \n\n\nHow are we going to calculate these sentence embedding in Tree Recursive\nNN? The idea is when we want to build a representation or a larger unit,\nwe need to take the representation of its children and stick them to some\nkind of neural network which will return two things:\n\n\n    \n\n\n\n  \n    A vector that is going to represent the meaning of the children.\n  \n  \n    The score of how reasonable the new node would be.\n  \n\n\nNow, we can use this simple neural network to parse a sentence. The\neasiest way to do that is to run a greedy parser that is going to look\nat what seems best and make decisions of every action and proceed along.\nWe can start of the sentence “the cat sat on the mat” and we are going\nto take each pair of words and calculate the representation of that pair\njust like so:\n\n\n    \n\n\nThen, we are going to take the pair that score best and merge it into a\nconstituent. At that point, we could repeat the same step after\nconsidering the new pair as a new word. So, in this example, we can see\nthat the pair “the cat” has the highest score. Then, we need to combine\nit into one constituent and recalculate the word pairs like so:\n\n\n    \n\n\nNow, the highest score is “the mat” word pair. So, we are going to\ncombine them into one constituent and recalculate the word pairs like\nso:\n\n\n    \n\n\nWe keep doing that till we form the Tree Recursive neural network:\n\n\n    \n\n\nRecursive NN\n\nAs we can see, we will be able to form a recursive tree depending on\nthis neural network that could combine two word-vectors into one with\nadditional information which is the score of how plausible this merge\nis. Now, let’s talk about how to form such a neural network.\n\nStandard Recursive NN\n\nThe simplest way of Neural Network to use here in our Tree Recursive NN\nis to form one single weight matrix Tree RNN like so:\n\n\n    \n\n\nWhich can be represented like so putting in mind that $\\begin{bmatrix}\nc_{1} \nc_{2} \n\\end{bmatrix}$ means concatenating the two word-vectors of the two\nchildren $c_{1}$ and $c_{2}$. The weights that we need to learn in this\nsimple neural network are $W$ and $U$:\n\n\\[parent = tanh\\left( W\\begin{bmatrix}\nc_{1} \\\\\nc_{2} \\\\\n\\end{bmatrix} + b \\right),score = U^{T}\\text{.parent}\\]\n\nBut this simple representation has a few problems:\n\n\n  \n    This simple single matrix could capture some phenomena but not\nadequate for complex, higher order composition and long sentences.\n  \n  \n    There is no interaction between the input words.\n  \n  \n    The composition function $tanh\\left( W\\begin{bmatrix}\nc_{1} \nc_{2} \n\\end{bmatrix} + b \\right)$ is the same for all syntactic categories.\nSo, we have just one weight matrix and it doesn’t matter if we are\nputting (adjective and a noun) or (a verb and an object) or even (a\nword and a period). For all these cases, we are using the same\nweight matrix $W$.\n  \n\n\nCVG\n\nCVG or Compositional Vector Grammars is another way to make a better and\nfaster Recursive NN. The idea behind this method is to combine the\nProbabilistic Context Free Grammar (PCFG) with the Tree Recursive NN.\n\nTO BE CONTINUED\n\nMax-Margin Loss Function\n\nHere, we will discuss a popular error metric known as the maximum margin\nobjective. The idea behind using this objective is to ensure that the\nscore computed for &quot;true&quot; labeled data points is higher than the score\ncomputed for &quot;false&quot; labeled data points.\n\nFor example, we want the score computed for the &quot;true&quot; sentence\n&quot;Museums in Paris are amazing&quot; as s and the score computed for the\n&quot;false&quot; labeled window &quot;Not all museums in Paris&quot; as S~c~\n(subscripted as c to signify that the window is &quot;corrupt&quot;). Then, our\nobjective function would be to maximize (S − S~c~) or to minimize (S~c~\n− S):\n\nHowever, the above optimization objective is risky in the sense that it\ndoes not attempt to create a margin of safety. We would want the\n&quot;true&quot; labeled data point to score higher than the &quot;false&quot; labeled\ndata point by some positive margin Δ. In other words, we would want\nerror to be:\n\n\\[Loss = max\\left( \\Delta + S_{c} - S,0 \\right)\\]\n\nWhat we want to do is to find the best Tree which has the highest score,\nand we are kind of approximated this by getting the best constituent at\nevery particular point in a time. So, the final thing is to set a loss\nfunction that we need to optimize. The loss function we are going to use\nhere is called the “max-margin loss function”:\n\n\\[s\\left( x,y \\right) = \\sum_{n \\in nodes\\left( y \\right)}^{}s_{n}\\]\n\n\\[J = \\sum_{i}^{}{s\\left( x_{i},y_{i} \\right)} - \\max_{y \\in A\\left( x_{i} \\right)}\\left( s\\left( x_{i},y \\right) + \\Delta\\left( y,y_{i} \\right) \\right)\\]\n\nWhere, $x$ is a sentence, $y$ is a parse tree, $A\\left( x \\right)$ is a\nstructure search for the best tree. We have used the greedy search, but\na good substitute would be a beam search. The loss\n$\\Delta\\left( y,y_{i} \\right)$ penalizes all incorrect decisions.\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "GCNN: Gated CNN",
        "url"       : "/language-modeling/GCNN",
        "date"      : "23/12/2016",
        "content": "One of the major defects of Seq2Seq models is that it can’t process\nwords in parallel. For a large corpus of text, this increases the time\nspent translating the text. CNNs can help us solve this problem. In this\npaper: “Language Modeling with Gated Convolutional\nNetworks”, proposed by FAIR\n(Facebook AI Research) in 2017, the researchers developed a new\narchitecture that uses gating mechanism over stacked convolution layers\nthat outperforms the\nSeq2Seq model.\n\n\n    \n\n\nUsing stacked convolutions layers is more efficient since it allows\nparallelization over sequential tokens. Using a kernel size of $k$ over\na context of size $N$, this new architecture will perform\n$O\\left( \\frac{N}{k} \\right)$ operations unlike recurrent networks which\nwill perform a linear number $O\\left( N \\right)$ of operations. The\nformer figure illustrates the model architecture; where:\n\n\n  \n    The input to the model is a sequence of words $w_{0},\\ …w_{N}$.\n  \n  \n    Each word is represented by a vector embedding stored in a lookup\ntable $D^{\\left| V \\right| \\times e}$ where $\\left| V \\right|$ is\nthe vocabulary and $e$ is the embedding size.\n  \n  \n    After the lookup table, the input will be represented as word\nembeddings:\n  \n\n\n\\[E = \\left\\lbrack D_{w_{0}},\\ ...\\ D_{w_{N}} \\right\\rbrack\\]\n\n\n  The hidden layers $h_{0},\\ …h_{L}$, where $L$ is the number of\nlayers, are computed as:\n\n\n\\[h_{l}\\left( X \\right) = \\left( X*W + b \\right) \\otimes \\sigma\\left( X*V + c \\right)\\]\n\nWhere $X \\in \\mathbb{R}^{N \\times m}$ is the input of layer $h_{l}$\n(either word embeddings or the outputs of previous layers),\n$W \\in \\mathbb{R}^{k \\times m \\times n}$, $b \\in \\mathbb{R}^{n}$,\n$V \\in \\mathbb{R}^{k \\times m \\times n}$, and $c \\in \\mathbb{R}^{n}$ are\nlearned parameters, $m$ and $n$ are respectively the number of input and\noutput feature maps, $\\sigma$ is the sigmoid function and $\\otimes$ is\nthe element-wise product between matrices.\n\n\n  Note:\nWhen convolving inputs, they made sure that $h_{i}$ does not contain\ninformation from future words by shifting the convolutional inputs to\nprevent the kernels from seeing future context.\n\n\nAdaptive Softmax\n\nThe simplest choice to To obtain model’s predictions at the last layer\nis to use a softmax layer, but this choice is often computationally\ninefficient for large vocabularies. A better choice could be\nhierarchical softmax (Morin &amp;amp; Bengio, 2005).\n\nIn the paper, they chose an improvement of the latter known as adaptive\nsoftmax which assigns higher capacity to very frequent words and lower\ncapacity to rare words. This results in lower memory requirements as\nwell as faster computation at both training and test time. You can find\nan efficient implementation of Adaptive Softmax in Facebook Research’s\nofficial GitHub repository:\nfacebookresearch/adaptive-softmax.\n\nExperiments\n\nAll experiments in this paper were using two public large-scale language\nmodeling datasets: Google’s Billion\nword\ndataset (one billion token) and\nWikiText-103\ndataset (100M tokens). For both datasets, $\\left\\langle S \\right\\rangle$\nand $\\left\\langle /S \\right\\rangle$ tokens were added at the start and\nend of each line respectively. In terms of optimization, they\ninitialized the layers of the model with the He initialization with the\nlearning rate sampled uniformly in the interval\n$\\lbrack 1.,\\ 2.\\rbrack$, the momentum was set to $0.99$, and gradient\nclipping was set to $0.1$ to prevent gradient explosion. Also, weight\nnormalization was used to make training faster as seen in the following\nfigure:\n\n\n    \n\n\nFor model architecture, they selected the number of residual blocks\nbetween $\\left{ 1,\\ …10 \\right}$, the size of the embeddings with\n$\\left{ 128,\\ …256 \\right}$, the number of units between\n$\\left{ 128,\\ …2048 \\right}$, and the kernel width between\n$\\left{ 3,\\ …5 \\right}$ as shown in the following table:\n\n\n    \n\n\nThe following table shows the test perplexity over Google’s Billion word\ndataset; as we can see, GCNN outperforms all LSTMs with the same output\napproximation while only requiring a fraction of the operations:\n\n\n    \n\n\nSame thing happens with WikiText-103 dataset; GCNN outperforms LSTM\nmodels:\n\n\n    \n\n\nThe following figure shows a comparison between GCNN and the\nstate-of-the-art LSTM model back\nin 2016 which uses the full softmax, the adaptive softmax approximation\ngreatly reduces the number of operations required to reach a given\nperplexity:\n\n\n    \n\n\nGating Mechanism (GLU)\n\nGating mechanisms control the path through which information flows in\nthe network. LSTMs\nenable long-term memory via a separate cell controlled by different\ngates (forget, update, output gates). This allows information to flow\nthrough potentially many timesteps and without these gates, information\ncould easily vanish.\n\nIn contrast, convolutional networks do not suffer from the same kind of\nvanishing gradient and we find experimentally that they do not require\nforget gates. Therefore, they considered using only output\ngates, which allow the network to control what information\nshould be propagated through the hierarchy of layers.\n\nIn this paper, they tried four different output gating mechanisms on the\nWikiText-103\nbenchmark:\n\n\n  Tanh (not gating mechanism):\n\n\n\\[h_{l}\\left( X \\right) = \\tanh\\left( X*W + b \\right)\\]\n\n\n  ReLU (not gating mechanism):\n\n\n\\[h_{l}\\left( X \\right) = \\text{ReLU}\\left( X*W + b \\right)\\]\n\n\n  Gated Tanh Unit (GTU):\n\n\n\\[h_{l}\\left( X \\right) = \\tanh\\left( X*W + b \\right) \\otimes \\sigma\\left( X*V + c \\right)\\]\n\n\n  Gated Linear Unit (GLU):\n\n\n\\[h_{l}\\left( X \\right) = \\left( X*W + b \\right) \\otimes \\sigma\\left( X*V + c \\right)\\]\n\nAnd the results show that GLU achieves the best perplexity over the data\nas seen in the following figure. There is a gap of about 5 perplexity\npoints between the GLU and ReLU which is similar to the difference\nbetween the LSTM and RNN models on the same dataset.\n\n\n    \n\n\nNote:\nThe difference between GTU and $\\text{Tanh}$ models shows us the effect\nof gating mechanism since the $\\text{Tanh}$ model can be thought of as a\nGTU network with the sigmoid gating units removed:\n\nThe experiments so far have shown that the GLU benefits from the linear\npath the unit provides compared to other non-linearities such as GTU.\nThat’s why they decided to compare GLU to purely linear networks in\norder to measure the impact of the nonlinear path provided by the gates\nof the GLU. In the paper, they compared GLU to:\n\n\n  Linear (not gating mechanism):\n\n\n\\[h_{l}\\left( X \\right) = \\left( X*W + b \\right)\\]\n\n\n  Bi-linear:\n\n\n\\[h_{l}\\left( X \\right) = \\left( X*W + b \\right) \\otimes \\left( X*V + c \\right)\\]\n\nThe following figure shows the performance of the three mechanisms on\nGoogle’s Billion word\nbenchmark.\nAs we can see, GLU still outperforms other methods\n\n\n    \n\n\nContext Size $k$\n\nThe following figure shows the impact of context size for GCNN on\nGoogle’s Billion word dataset (left graph) and WikiText-103 dataset\n(right graph). Generally, larger contexts improve accuracy but returns\ndrastically diminish with windows larger than 40 words:\n\n\n    \n\n\nThe previous figure Figure 4 also shows that WikiText-103 benefits much\nmore from larger context size than Google Billion Word as the\nperformance degrades more sharply with smaller contexts.\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "GPT",
        "url"       : "/language-modeling/GPT",
        "date"      : "11/06/2018",
        "content": "Transform is a state-of-the-art architecture for machine translation.\nOpenAI tried to use this architecture for the language modeling task in\nthis paper “Improving Language Understanding by Generative\nPre-Training”\nunder the name “Improving Language Understanding by Generative\nPre-Training” which was published in 2018. Pre-training is the process\nof training a model with one task (language modeling in the paper) that\nis able to help it form parameters that can be used to make other tasks\neasier (four other tasks: natural language inference, question\nanswering, semantic similarity, and text classification).\n\nThe Encoder-Decoder structure of the transformer made it perfect for\nmachine translation. But how would you use it to pre-train a language\nmodel that can be fine-tuned for other tasks like sentiment analysis or\ntext classification? The way openAI team did it was pretty smart. It\nturns out that we don’t need the entire transformer architecture to\nadopt a language model. We can do it with just the decoder of the\ntransformer.\n\n\n    \n\n\nThe decoder is a good choice because it’s\na natural choice for language modeling (predicting the next word) since\nit’s built to mask future tokens. Since there is no encoder in this set\nup, the decoder layer would not have the encoder-decoder attention\nsub-layer that vanilla transformer decoder layers have. So, the decoder\narchitecture becomes as shown in the image on the right.\n\nIn the original paper, they stacked twelve decoder layers with a\nfeed-forward neural network at the end with Softmax loss function. With\nthis structure, we can proceed to train the model on the same language\nmodeling task: predict the next word using massive (unlabeled) datasets.\n\n\n    \n\n\nModel Specification\n\nThe language model was trained on the BooksCorpus dataset for training\nthe language model. This dataset contains over 7,000 unique unpublished\nbooks from a variety of genres. Crucially, it contains long stretches of\ncontiguous text, which allows the generative model to learn to condition\non long-range information.\n\nThe model, itself, has the following characteristics:\n\n\n  \n    12-layer decoder-only transformer.\n  \n  \n    Masked self-attention with multi-heads (768 dimensional states and\n12 attention heads).\n  \n  \n    For the position-wise feed-forward networks, they used 3072 neurons.\n  \n  \n    They used the Adam optimization scheme.\n  \n  \n    The learning rate was increased linearly from zero over the first\n2000 updates and annealed to 0 using a cosine schedule with a max\nlearning rate of 2.5e-4.\n  \n  \n    We train for 100 epochs on minibatches of 64 randomly sampled,\ncontiguous sequences of 512 tokens.\n  \n  \n    Since layer normalization is used extensively throughout the model,\na simple weight initialization of $\\mathcal{N}(0,\\ 0.02)$ was\nsufficient.\n  \n  \n    They used a bytepair encoding (BPE) vocabulary with 40,000 merges.\n  \n  \n    Residual, embedding, and attention dropouts with a rate of 0.1 for\nregularization.\n  \n  \n    They also employed a modified version of L2 regularization proposed\nin this paper, with w = 0.01 on all non bias or gain weights.\n  \n  \n    For the activation function, we used the Gaussian Error Linear Unit (GELU).\n  \n  \n    They used learned position embeddings instead of the sinusoidal\nversion proposed in the original work.\n  \n\n\nFine-Tuning\n\nThe OpenAI paper outlines a number of input transformations to handle\nthe inputs for different types of tasks. Since our language model was\ntrained on contiguous sequences of text, we require some modifications\nto apply it to the different NLP tasks.\n\nPrevious work proposed learning\ntask-specific architectures on top of transferred representations. We\nuse a traversal-style approach where we convert structured inputs into\nan ordered sequence that our pre-trained model can process which allows\nus to avoid making extensive changes to the architecture across\ndifferent tasks.\n\nThe following image shows the structures of the model and the input\ntransformations to carry out different tasks. In these transformations,\nwe are using the following special tokens;\n$\\left\\langle s \\right\\rangle$ for start token,\n$\\left\\langle e \\right\\rangle$ for extract token, and\n$\\left\\langle \\$ \\right\\rangle$ for delimiter token:\n\n\n    \n\n\nSo, for example the classification model will look like this:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "BERT",
        "url"       : "/language-modeling/BERT",
        "date"      : "11/10/2018",
        "content": "BERT stands for “Bidirectional Encoder Representations from\nTransformers” which is a model published by researchers at Google in\nthis paper: “BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding” in 2018.\nIt has caused a stir in the NLP community by presenting state-of-the-art\nresults in a wide variety of NLP tasks, including Question Answering\n(SQuAD v1.1), Natural Language Inference (MNLI), and others.\n\nBERT’s key technical innovation is applying the bidirectional training\nto the openAI Transformer. See, the openAI transformer gave us a\nfine-tunable pre-trained language model based on the Transformer. But\nsomething went missing in this transition from LSTMs to Transformers.\nLSTM language model was bi-directional, but the openAI transformer only\ntrains a forward language model. BERT introduced a novel technique to\ntrain the openAI transformer in bi-directional manner which is to train\nBERT using two unsupervised tasks:\n\n\n  \n    Masked LM (MLM): To catch the contextual relation between words.\nVery important for tasks like languge modeling, text\nclassification, ...etc.\n  \n  \n    Next Sentence Prediction (NSP): To catch the contextual relation\nbetween sentences. Very important for tasks like Question\nAnswering, Natural Language Inference... etc.\n  \n\n\nMLM\n\nSince bidirectional conditioning requires each word to indirectly see\nitself and others in a multi-layered context, researchers had to find a\nway to overcome this obstacle. And they did that by introducing\nmasks. Previously, this technique was called “Cloze\nprocedure” where the key idea is to remove words from the input and\npredict them with the remaining input. As you’ve probably guessed, this\nis very similar to the CBOW model for distributed word embeddings.\n\n\n    \n\n\nIn the paper; they said that before feeding word sequences into BERT,\n15% of the words in each sequence are replaced with a [MASK] token;\nand the type of mask will be different according to the following\ndistribution\n\n\n  \n    80% of the time: the mask will be [MASK].\n  \n  \n    10% of the time: the mask will be a random word.\n  \n  \n    10% of the time: The mask will be the original word.\n\n    The advantage of this procedure is that the Transformer-encoder does\nnot know which words it will be asked to predict or which have been\nreplaced by random words, so it is forced to keep a distributional\ncontextual representation of every input token. Additionally,\nbecause random replacement only occurs for 1.5% of all tokens (i.e.,\n10% of 15%), this does not seem to harm the model’s language\nunderstanding capability.\n\n    After that, the model attempts to predict the original value of the\nmasked words, based on the context provided by the other,\nnon-masked, words in the sequence. In technical terms, the\nprediction of the output words requires:\n  \n  \n    Adding a classification layer on top of the encoder output.\n  \n  \n    Multiplying the output vectors by the embedding matrix, transforming\nthem into the vocabulary dimension.\n  \n  \n    Calculating the probability of each word in the vocabulary with softmax.\n  \n\n\n\n    \n\n\nThe BERT loss function takes into consideration only the prediction of\nthe masked values and ignores the prediction of the non-masked words. As\na consequence, the model converges slower than directional models.\n\n\n  Note:\nThere is a new token [CLS] added to the start of the input sentence\nwhen passed to BERT. [CLS] stands for classification and this is just\na way to tell BERT we are using your architecture for classification.\n\n\nNow, let’s ask a very important question: what happens if we increased the\nmasking percentage to more than 15%? Actually, researchers at Princeton tried\nto answer this question in their paper “Should You Mask 15% in Masked Language\nModeling?” published in 2022. And they\nfound out that masking up to 40% of input tokens can outperform the 15%\nbaseline, and even masking 80% can preserve most of the performance,\nas measured by fine-tuning on downstream tasks. You can use this GitHub\nrepository: princeton-nlp/dinkytrain\nto reproduce their results.\n\nNSP\n\nMany important downstream tasks such as Question Answering and Natural\nLanguage Inference are based on understanding the relationship between\ntwo sentences, which is not directly captured by language modeling.\n\nIn order to make BERT better at handling relationships between multiple\nsentences, the pre-training process includes an additional task called\n“Next Sentence Prediction (NSP)” where the model receives pairs of\nsentences as input and learns to predict if the second sentence in the\npair is the subsequent sentence in the original document.\n\nDuring training, 50% of the inputs are a pair in which the second\nsentence is the subsequent sentence in the original document, while in\nthe other 50% a random sentence from the corpus is chosen as the second\nsentence. To help the model distinguish between the two sentences in\ntraining, the input is processed in the following way before entering\nthe model:\n\n\n  \n    A [CLS] token is inserted at the beginning of the first sentence\nand a [SEP] token is inserted at the end of each sentence.\n  \n  \n    A sentence embedding indicating Sentence A or Sentence B is added to\neach token. Sentence embeddings are similar in concept to token\nembeddings with a vocabulary of 2.\n  \n  \n    A positional embedding is added to each token to indicate its\nposition in the sequence. The concept and implementation of\npositional embedding are presented in the Transformer paper.\n  \n\n\nAnd all of this information can be seen in the following figure:\n\n\n    \n\n\nFine-tuning Tasks\n\n\n    \n\n\nFine-tuning in this context means using BERT for a specific task\nsuch as QA, text classification, language inference, ...etc. BERT can be used\nfor a wide variety of language tasks, while only adding a small layer to the\ncore model as explained in:\n\n\n  Text Classification:\nClassification tasks such as sentiment analysis are done\nsimilarly to Next Sentence classification, by adding a\nclassification layer on top of the Transformer output for the\n[CLS] token.\n\n\n\n    \n\n\n\n  Question Answering:\nA question-answering model can be trained by learning two extra\nvectors that mark the beginning and the end of the answer.\n\n\n\n    \n\n\n\n  Named Entity Recognition (NER):\nBERT can be trained by feeding the output vector of each token\ninto a classification layer that predicts the NER label.\n\n\nBERT Linguistic Patterns\n\nAccording to this paper “BERT Rediscovers the Classical NLP\nPipeline” published by Google in\n2019, the authors of this paper found out that different layers of BERT\ncapture different linguistic semantics. For example, they found out that\nlower layers of BERT encode more local syntax while higher layers\ncapture more complex semantics. They used a pre-trained BERT-base &amp;amp;\nBERT-Large on eight different tasks.\n\nThe following table shows the layer-wise metrics on BERT-base (left) and\nBERT-large (right) where blue bars are mixing weights that tell us\nwhich layers are most relevant when a probing classifier at this\nlayer has access to the whole BERT model, while purple\nbars are differential scores normalized for each task which \nmeasures how much better we do on the probing task if we observe\none additional encoder layer:\n\n\n    \n\n\nFrom the past figure, if we have access to the whole BERT model, we can\nsee the following with respect to each task:\n\n\n  \n    Part-of-speech (POS): The purple bars show that the first few\nlayers are the most important; and the blue bars show us that\nprobing the first layers will have the same effect as the last\nlayers.\n  \n  \n    Constituents (Consts.): The purple bars show that the first few\nlayers are the most important; and the blue bars show us that\nprobing the middle layers will have the highest effect.\n  \n  \n    Dependencies (Deps.): The purple bars show that the first few\nlayers of BERT-base are the most important while the middle layers\nof BERT-large are the most important; and the blue bars show us that\nprobing the middle layers will have the highest effect.\n  \n  \n    Entities: The purple bars show that the first few layers are the\nmost important; and the blue bars show us that probing the\nmiddle-last layers will have the highest effect.\n  \n  \n    Semantic role labeling (SRL): The purple bars show that the\nfirst few layers are the most important; and the blue bars show us\nthat probing the middle layers will have the highest effect.\n  \n  \n    Coreference (Coref.): The purple bars show that the last few\nlayers are the least important; and the blue bars show us that\nprobing the middle-last layers will have the highest effect.\n  \n  \n    Semantic proto-roles (SPR): The purple bars show all layers are\nimportant; and the blue bars show us that probing over all layers\nhas the same effect.\n  \n  \n    Relation classification (SemEval): The purple bars show all\nlayers are important; and the blue bars show us that probing over\nall layers has the same effect.\n  \n\n\nBase / Large BERT\n\nThe paper presents two model sizes for BERT:\n\n\n  \n    BERT BASE: Comparable in size to the OpenAI Transformer in order\nto compare performance.\n  \n  \n    BERT LARGE: A ridiculously huge model which achieved the state\nof the art results reported in the paper.\n  \n\n\nThe following summarizes the difference between both models:\n\n\n\n    \n        \n            \n            BERT SMALL\n            BERT BASE\n            BERT LARGE\n        \n    \n    \n        Transformer Blocks\n        4\n        12\n        24\n    \n    \n        Feed-Forward hidden neurons\n        312\n        768\n        1024\n    \n    \n        Attention Heads\n        12\n        12\n        16\n    \n    \n        Input Tokens\n        512\n        512\n        512\n    \n    \n        Parameters\n        14.5 million\n        110 million\n        345 million\n    \n    \n        Hardware for Training\n        -\n        4 TPU + 4 days\n        16 TPU + 4 days\n    \n    \n        Hardware for Inference\n        -\n        1 GPU\n        1 TPU\n    \n\n\n\nBERT Vs GPT\n\nThe most comparable existing pre-training method to BERT is OpenAI GPT,\nwhich trains a left-to-right Transformer LM on a large text corpus. In\nfact, many of the design decisions in BERT were intentionally made to\nmake it as close to GPT as possible so that the two methods could be\nminimally compared. However, there are several other differences:\n\n\n  \n    GPT is trained on the BooksCorpus (800M words); BERT is trained on\nthe BooksCorpus (800M words) and Wikipedia (2,500M words).\n  \n  \n    GPT uses a sentence separator ([SEP]) and classifier token\n([CLS]) which are only introduced at fine-tuning time; BERT\nlearns [SEP], [CLS] and sentence A/B embeddings during\npre-training.\n  \n  \n    GPT was trained for 1M steps with a batch size of 32,000 words; BERT\nwas trained for 1M steps with a batch size of 128,000 words.\n  \n  \n    GPT used the same learning rate of 5e-5 for all fine-tuning\nexperiments; BERT chooses a task-specific fine-tuning learning\nrate which performs the best on the development set.\n  \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Transformer-XL",
        "url"       : "/language-modeling/Transformer-XL",
        "date"      : "09/01/2019",
        "content": "Transformer-XL, stands for “Transformer Extra Long”, is a language model\npublished in this paper: “Transformer-XL: Attentive Language Models\nBeyond a Fixed-Length Context”\nby Google Brain in 2019.The official code for this paper can be found in the\nfollowing GitHub repository: transformer-xl\n.\n\nIn this paper, the authors are trying to increase the context-dependency scope.\nHence, the name of the paper: Transformer-XL Attentive Language Models Beyond a\nFixed-Length Context. A simple comparison between Transformer-XL and GPT and\nBERT can be summarized in the following figure:\n\n\n    \n\n\nIn the transformer architecture, we split the input paragraph into\nsentences, each sentence can’t exceed a certain length (it’s 512 in\nBERT). After splitting the paragraph into sentences or “segments”, then\nwe train our model as shown in the following image where we assume the\nallowed length is just four:\n\n\n    \n\n\nAs you can see, segment 2 is after segment 1 in the same paragraph. But\naccording to the transformer architecture, they are totally independent\nwhich causes another problem called “context fragmentation” where the\nmodel lacks the necessary contextual information to predict the first\nfew symbols due to the way the context was selected. Transformer-XL\nsolves this problem by providing a segment-level recurrence mechanism.\nAnd since transformer-XL uses larger context-dependency length, the\nauthors decided to use a different positional encoding than the vanilla\ntransformer.\n\nSo, the key innovations behind this paper can be summarized into two\nthings:\n\n\n  \n    Segment-level recurrence mechanism.\n  \n  \n    Relative positional encoding scheme.\n  \n\n\nSegment-level Recurrence\n\nThe goal of the recurrence mechanism is to enable long-term dependencies\nby using information from previous segments. Similarly to the vanilla\nversion, Transformer-XL processes the first segment of tokens but keeps\nthe outputs of the hidden layers. When the following segment is\nprocessed, each hidden layer receives two inputs:\n\n\n  \n    The output of the previous hidden layer of that segment, as in the\nvanilla version (the grey arrows in the chart below).\n  \n  \n    The output of the previous hidden layer from the previous segment\n(the green arrows) that allows the model to create long-term\ndependencies.\n  \n\n\nTechnically, the two inputs are concatenated and then used to calculate\nthe Key and the Value matrices of the (current Head of the current layer\nof the) current segment. This addition provides the network with more\ninformation in regards to the weights (importance) of each token, but it\ndoesn’t change the Value matrix.\n\n\n    \n\n\nIn each segment, each hidden layer receives the output of the previous\nhidden layer and the output of the previous segment. It increases the\nlargest possible dependency by using contextual information from several\nprevious segments.\n\n\n    \n\n\nThis mechanism can be applied at the decoding step with no problem as\nshown in the following figure:\n\n\n    \n\n\nRelative Positional Encoding\n\nNaively applying recurrence introduces another technical challenge. That\nis, the positional information is incoherent, and tokens from different\nsegments have the same positional encoding, which is referred to as\ntemporal confusion. To address this challenge,\nTransformer-XL employs novel relative positional encodings.\n\nIn the vanilla transformer, positional encodings were depending on the\nindex of the tokens. This positional encoding is depending on the\nrelative distance between tokens, hence the name: relative\npositional encoding. In the paper this was done by expanding the simple\nquery-key multiplication of the Attention Head’s Score.\n\nFirst, let’s recap what was the query-key multiplication in the\nattention mechanism of the vanilla transformer:\n\n\n    \n\n\nFollowing the idea of only relying on relative positional information,\nthey proposed to reparameterize the four terms as follows:\n\n\n    \n\n\nWith the following changes:\n\n\n  \n    The first change they made is to replace all appearances of the\nabsolute positional embedding $U_{j}$ for computing key vectors in\nterm $(b)$ and $(d)$ with its relative counterpart $R_{i - j}$ .\nNote that $R$ is a sinusoid encoding matrix without learnable\nparameters.\n  \n  \n    Secondly, we introduce a trainable parameter $u \\in \\mathbb{R}^{d}$\nto replace the query $U_{i}^{T}W_{q}^{T}$ in term $(c)$. In this\ncase, since the query vector is the same for all query positions,\nit suggests that the attentive bias towards different words should\nremain the same regardless of the query position.\n  \n  \n    With a similar reasoning, a trainable parameter\n$v \\in \\mathbb{R}^{d}$ is added to substitute $U_{i}^{T}W_{q}^{T}$\nin term $(d)$.\n  \n  \n    Finally, we deliberately separate the two weight matrices $W_{k,E}$\nand $W_{k,R}$ for producing the content-based key vectors and\nlocation-based key vectors respectively.\n  \n\n\nUnder the new parameterization, each term has an intuitive meaning:\n\n\n  \n    Term (a): represents content-based addressing.\n  \n  \n    Term (b): captures a content-dependent positional bias.\n  \n  \n    Term (c): governs a global content bias\n  \n  \n    Term (d): encodes a global positional bias.\n  \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Adapter Layers",
        "url"       : "/language-modeling/adapter",
        "date"      : "02/02/2019",
        "content": "At the current moment, the norm in NLP involves downloading and\nfine-tuning pre-trained models consisting of hundreds of millions, or\neven billions of parameters. Modifying these models, no matter how\nsimple the modification is, requires re-training the whole model. And\nre-training these huge models is expensive, slow, and time-consuming,\nwhich impedes the progress in NLP. Adapters are one way to fix this\nproblem.\n\nAdapters, proposed in this paper: Parameter-efficient transfer learning\nfor NLP by Google Research in\n20019, are small learned bottleneck layers inserted within each layer of\na pre-trained models to avoid full fine-tuning of the entire model. To\ndemonstrate adapter’s effectiveness, researchers in the paper have\ntransferred BERT model to 26 diverse text classification tasks achieving\nnear state-of-the-art performance. The official code for this paper can\nbe found in Google’s research official GitHub repository:\nadapter-bert.\n\n\n    \n\n\nAdapter Tuning\n\nAdapter Tuning is considered a new technique for transfer learning.\nBefore that, There are two common transfer learning techniques in NLP:\n\n\n  \n    Feature-based Transfer Learning:\nIt involves pre-training real-valued embeddings vectors. These\nembeddings may be at the word, sentence, or paragraph level. The\nembeddings are then fed to custom downstream models.\n  \n  \n    Fine-tuning:\nFine-tuning involves copying the weights from a pre-trained network\nand tuning them on the downstream task. Recent work shows that\nfine-tuning often enjoys better performance than feature-based\ntransfer.\n  \n\n\nNow, let’s get into adapter tuning. Consider a function (neural network) with\nparameters $\\phi_{w}\\left( x \\right)$, adapter tuning defines a new function\n$\\phi_{w,v}\\left( x \\right)$ where $v$ is anew set of parameters. The initial\nvalue of the parameters $v_{0}$ is set such that the new function resembles\nthe original $\\phi_{w,v_{0}}\\left( x \\right) \\approx \\phi_{w}\\left( x \\right)$.\nDuring training, the $w$ parameters are frozen and only $v$ is tuned.\n\nThe following figure shows the transformer layer on the left and how we are\ngoing to set the adapter tuning to it on the right. As we can see, the\nadapter is always applied directly to the output of the sub-layer, after the\nfeed-forward and before adding the skip connection back:\n\n\n    \n\n\nTo sum up, adapter tuning is a transfer learning technique that attains neat to\nstate-of-the-art performance. During adapter tuning, we only train the adapter\nlayers unlike fine-tuning where we train some of the layers, usually the top\nones. The following figure shows the trade-off between accuracy and number\nparameters, for adapter tuning and fine-tuning. The y-axis represents the\nperformance normalized in comparison with full fine-tuning on nine tasks from\nthe GLUE benchmark.\n\n\n    \n\n\n\n  Note:\nDuring inference, the adapter modules may be ignored if not\nrequired. That is possible because they have near-identity\ninitialization with the parameters in the original neural network.\n\n\n\n    \n\n\nAdapter Layer\n\nHere, we are going to describe the design of the adapter layer. The\nadapter layer first projects the original d-dimensional features into a\nsmaller dimension $m$, apply a non-linearity, then project back to $d$\ndimensions. The adapter module itself has a skip-connection internally.\n\nThe bottleneck dimension, $m$, is the only hyper-parameter which\nprovides a simple means to tradeoff performance with number of added\nparameters. In practice, they use around $0.5:8\\%$ of the parameters of\nthe original model.\n\nThe total number of parameters added per layer, including biases, is\n$md + m$ in the feed-forward down-project and $md + d$ in the\nfeed-forward up-project. So, the total is:\n\n\\[2md + d + m\\]\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "GPT-2",
        "url"       : "/language-modeling/GPT-2",
        "date"      : "14/02/2019",
        "content": "GPT-2 stands for “Generative Pre-trained Transformer” which is a\nlanguage model published in this paper: “Language Models are\nUnsupervised Multitask\nLearners”\nby OpenAI in 2019. In the paper, they tried to demonstrate that language\nmodels can perform down-stream tasks such as (question answering,\nmachine translation, reading comprehension, and summarization) in a\nzero-shot setting – without any parameter or architecture modification.\n\nOne great way to experiment with GPT-2 is using the AllenAI GPT-2\nExplorer. It uses GPT-2 to\ndisplay ten possible predictions for the next word (alongside their\nprobability score). You can select a word then see the next list of\npredictions to continue writing the passage.\n\nWebText\n\nMost prior work trained language models on a single domain of text, such\nas news articles, Wikipedia, or fiction books. Their approach is to\nbuild as large and diverse a dataset as possible in order to collect\nnatural language demonstrations of tasks in as varied of domains and\ncontexts as possible.\n\nA promising source of that kind of data is web scrapes such as Common\nCrawl, but they have significant data quality\nissues which is that a large amount of documents content are mostly\nunintelligible. That’s why in this paper, they created a new dataset\ncalled WebText. This data contains text subset of around 45 million\nlinks from Reddit where each link received at least 3 karma which\nindicates whether other users found the link interesting or not. Also,\nall Wikipedia links were removed since it’s a common data source.\n\nModel\n\nThe model in this paper is the same as the one in GPT with a few\nmodifications:\n\n\n  \n    Layer normalization was moved to the input of each sub-block. And an\nadditional layer normalization was added after the final\nself-attention block.\n  \n  \n    A modified initialization, which accounts for the accumulation on\nthe residual path with model depth, is used. They scaled the\nweights of residual layers at initialization by a factor of\n$\\frac{1}{\\sqrt{N}}$ where $N$ is the number of residual layers.\n  \n  \n    The vocabulary is expanded to 50,257 instead of 40,000.\n  \n  \n    The context size is increased to 1024 instead of 512.\n  \n  \n    The batch size is increased to 512 instead of 64.\n\n    Also, they trained different versions of GPT-2 models: The smallest\nmodel is equivalent to the original GPT, and the second smallest\nequivalent to the largest model from BERT.The learning rate of each\nmodel was manually tuned for the best perplexity on a 5% held-out\nsample of WebText.\n  \n\n\n\n    \n\n\nFine-Tuning\n\nAs we said earlier, the purpose of this paper is to demonstrate that\nlanguage models can perform down-stream tasks such as (question\nanswering, machine translation, reading comprehension, and\nsummarization) in a zero-shot setting – without any parameter or\narchitecture modification.\n\nNow, let’s see how they did that with different tasks:\n\n\n  Machine Translation:\n\n\n\n    \n\n\n\n  Summarization:\n\n\n\n    \n\n\nResult Search\n\nIn order to produce good results when using our model, there are\nmultiple ways that we can search for the best result and they are:\n\n\n  \n    Exhaustive Search: Considering the whole vocabulary\n  \n  \n    Greedy Search: Considering the top option at each time-step.\n  \n  \n    Beam Serach: Considering the top N options at each time-step.\n  \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "MASS",
        "url"       : "/language-modeling/MASS",
        "date"      : "07/05/2019",
        "content": "MASS, stands for “Masked Sequence to Sequence”, is a\npre-training scheme proposed by Microsoft in 2019 and published in this\npaper: “MASS: Masked Sequence to Sequence Pre-training for Language\nGeneration” and the code is\npublicly available on Microsoft’s official account on\nGitHub. Inspired by BERT, MASS\nencoder takes a sentence with a masked fragment as input, and its\ndecoder predicts this masked fragment.\n\n\n    \n\n\nUnlike BERT which pre-trains only the encoder or decoder, MASS is\ncarefully designed to pre-train the encoder and decoder jointly in two\nsteps:\n\n\n  \n    By predicting the fragment of the sentence that is masked on the\nencoder side, MASS can force the encoder to understand the meaning\nof the unmasked tokens, in order to predict the masked tokens in the\ndecoder side.\n  \n  \n    By masking the input tokens of the decoder that are unmasked in the\nencoder side, MASS can force the decoder rely more on the source\nrepresentation other than the previous tokens in the target side for\nnext token prediction, better facilitating the joint training\nbetween encoder and decoder.\n  \n\n\n\n  Note:\nWhile this method works for any neural network based encoder-decoder\nframeworks, they chose Transformer considering that it achieves\nstate-of-theart performances in multiple sequence to sequence learning\ntasks.\n\n\nMasked Sequence\n\nIn the paper, they introduced a novel unsupervised prediction task where\nthey mask $k$ consecutive tokens in the source sentence. Given an\nunpaired source sentence $x \\in \\mathcal{X}$ , they denote $x^{u:v}$ as\na modified version of $x$ where the tokens from position $u$ to $v$ are\nmasked using the special symbol $\\left\\lbrack \\mathbb{M} \\right\\rbrack$\nwhere $0 &amp;lt; u &amp;lt; v &amp;lt; \\text{len}\\left( x \\right)$. They denote the unmasked\npart of $x$ as $x^{\\backslash u:v}$ In this case, the log likelihood is\nused as the objective function:\n\n\\[L\\left( \\theta;\\mathcal{X} \\right) = \\frac{1}{\\left| \\mathcal{X} \\right|}\\sum_{x \\in \\mathcal{X}}^{}{\\log\\left( P\\left( x^{u:v} \\middle| x^{\\backslash u:v};\\theta \\right) \\right)}\\]\n\nFor example in the following figure, we can see that the input sequence\nhas 8 tokens with the fragment\n$x^{3:6} = \\left\\{ x_{3},\\ x_{4},\\ x_{5},\\ x_{6} \\right\\}$ being masked.\nNote that the model only predicts the masked fragment, given only\n$\\left\\{ x_{3},\\ x_{4},\\ x_{5} \\right\\}$ as the decoder input for\nposition $4:6$, and the decoder takes the special mask symbol\n$\\left[ \\mathbb{M} \\right]$ as inputs for the other\npositions (e.g., position $1:3$ and $7:8$.\n\n\n    \n\n\nThe start position $u$ is chosen randomly. The same as BERT, the masked\ntokens in the encoder will be replaced by:\n\n\n  \n    The $\\left\\lbrack \\mathbb{M} \\right\\rbrack$ token about 80% of the\ntime.\n  \n  \n    A random token 10% of the time.\n  \n  \n    Remains unchanged 10% of the time.\n  \n\n\nStudy of Different k\n\nThe length of the masked fragment $k$ is an important hyper-parameter of\nMASS and they explored different values of $k$ from 10% to 90%\npercentage of the sentence length $m$ with a step size of 10%. They\nfound out that the best value for k is around 50% of the sentence length\n$m$ in multiple pre-training and fine-tuning tasks.\n\n\n    \n\n\nActually, the masked language modeling in BERT and the standard language\nmodeling in GPT can be viewed as special cases of MASS. The following\ntable shows how tuning the hyper-parameter $k$ can convert MASS to\neither BERT or OpenAI GPT:\n\n\n    \n\n\nPre-training\n\nWe choose Transformer as the basic model structure, which consists of\n6-layer encoder and 6-layer decoder with 1024 embedding/hidden size and\n4096 feed-forward filter size. Since MASS is a pre-training method\nmainly for language generation, the pre-training method changes based on\nthe fine-tuning task:\n\n\n  \n    For neural machine translation task:\nThey pre-trained MASS on the monolingual data of the source and\ntarget languages. They conducted experiments on three language\npairs: English-French, English-German, and English-Romanian. To\ndistinguish between the source and target languages, they added a\nlanguage embedding to each token of the input sentence for the\nencoder and decoder, which is also learned end-to-end. Also, they\nused a vocabulary of 60,000 sub-word units with Byte-Pair Encoding\nbetween source and target languages\n  \n  \n    For text summarization &amp;amp; conversational response generation:\nThey pre-trained the model with only English monolingual data.\n  \n\n\nAll of the monolingual data used in this pre-training are from WMT News\nCrawl datasets, which covers 190M, 62M and 270M sentences from year 2007\nto 2017 for English, French, German respectively. Also, they used all of\nthe available Romanian sentences from News Crawl dataset and augment it\nwith WMT16 data, which results in 2.9M sentences.\n\nFine-tuning\n\nIn this section, we are going to discuss the performance of MASS over\nvarious tasks such as:\n\n\n  Unsupervised NMT:\nFor unsupervised NMT, we use only monolingual data to train\nMASS with back-translation (no bilingual data). And the following\ntable shows the results of MASS (fine-tuned using Adam optimizer\nwith initial learning rate $10^{- 4}$ and the batch size is set as\n2000 tokens for each GPU) on newstest2014 for English-French, and\nnewstest2016 for English-German and English-Romanian:\n\n\n\n    \n\n\n\n  Low-resource NMT:\nIn the low-resource NMT setting, we respectively sample 10K,\n100K, 1M paired sentence from the bilingual training data of WMT14\nEnglish-French, WMT16 English-German and WMT16 English-Romanian. The\nfollowing table shows the performance of MASS (fine-tuned for 20,000\nsteps with Adam optimizer and the learning rate is set as 10−4) on\nthe same testsets used in the unsupervised setting; The baseline\nmodel here is MASS but without pre-training.\n\n\n\n    \n\n\n\n  Text Summarization:\nText summarization is the task of creating a short and fluent\nsummary of a long text document, which is a typical sequence\ngeneration task. We fine-tune the pre-trained model on text\nsummarization task with different scales (10K, 100K, 1M and 3.8M) of\ntraining data from the Gigaword corpus, which consists of a total of\n3.8M article-title pairs in English. We take the article as the\nencoder input and title as the decoder input for fine-tuning. We\nreport the F1 score of ROUGE-1, ROUGE2 and ROUGE-L on the Gigaword\ntestset during evaluation. We use beam search with a beam size of 5\nfor inference. The baseline here is MASS but without pre-training:\n\n\n\n    \n\n\n\n  Conversational Response Generation:\nConversational response generation generates a flexible response for\nthe conversation. We conduct experiments on the Cornell movie dialog\ncorpus that contains 140K conversation pairs. We randomly sample\n10K/20K pairs as the validation/test set and the remaining data is\nused for training. We adopt the same optimization hyper-parameters\nfrom the pre-training stage for fine-tuning:\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "XLNet",
        "url"       : "/language-modeling/XLNet",
        "date"      : "19/06/2019",
        "content": "XLNet stands for “Extra-Long Net” which is a model that integrates both\nGPT and BERT introduced in 2019 by Google Brain and published in this\npaper: “XLNet: Generalized Autoregressive Pretraining for Language\nUnderstanding” by the same\nauthors of Transformer-XL. The official code for this paper can be found in\nthe following GitHub repository: xlnet.\n\nIn Transformer-XL, they extended the context-dependency length by\nintroducing the segment-level recurrence mechanism which uses the hidden\nstate of the former segments when predicting the current segment. In\nthis paper, they are trying to make the model uses the hidden state of\nthe former and following segments when predicting the current segment.\n\nAnd that explains the meaning of the paper’s name. An auto-regressive\nlanguage model is is a language model that is able to predict the next\npossible word based on the before-context or predict the previous word\nbased on the after-context. And it’s generalized because it considers\nboth; the before-context and the after-context. The way to do that as\nproposed by the paper is by using “Permutation Language Modeling”.\n\nPLM\n\nPLM stands for “Permutation Language Modeling” which is the idea of\ncapturing bidirectional context by training an auto-regressive model on\nall possible permutation of words in a sentence. Instead of a fixed\nleft-right or a right-left modeling, XLNET maximizes expected log\nlikelihood over all possible permutations of the sequence which means\nthat each position will learn to utilize contextual information from all\npositions thereby capturing bidirectional context.\n\nThis mechanism is better than “MLM (Masked Language Modeling)” used with\nBERT. And that’s because MLM corrupts the input with masks which affects\nreal life applications since we do not have inputs that are masked.\nAlso, MLM ignores the relation between masked tokens. For example, let’s\nconsider this sentence: “She met [mask] and [mask] friends. So, if\nthe first [mask] is “Adam”, then the second [mask] has to be\n“his”. And this will change when the first [mask] is “Sara” for\nexample.\n\nSo, consider a sequence\n$x = \\left[ “This”,\\ “is”,\\ “a”,\\ “sentence” \\right]$ with $T = 4$\ntokens. Now consider the set of all $4!$ permutations\n$Z = \\left\\{ z_{1},\\ z_{2},\\ …z_{4!} \\right\\} =\n{\\lbrack 1,\\ 2,\\ 3,\\ 4\\rbrack,\\ \\lbrack 1,\\ 2,\\ 4,\\ 3\\rbrack,.\\ .\\ .,\\ \\lbrack 4,\\ 3,\\ 2,\\ 1\\rbrack}$.\n\nThe XLNet model calculates the probability of token $x_{t}$ given\npreceding tokens $x_{&amp;lt; t}$ from any order which makes the objective\nfunction as follows:\n\n\\[\\max_{\\theta}\\left( \\mathbb{E}_{z\\sim Z_{T}}\\left\\lbrack \\sum_{t = 1}^{T}{\\log\\left( p_{\\theta}\\left( x_{z_{t}} \\middle| x_{z_{&amp;lt; t}} \\right) \\right)} \\right\\rbrack \\right)\\]\n\nSo, if $t = 3$ and the current permutation is\n$z = \\lbrack 3,\\ 2,\\ 4,\\ 1\\rbrack$, it means XLNet will consider zero\nwords $x_{z_{&amp;lt; t}} = \\lbrack\\rbrack$ when predicting the probability of\nthe third word which corresponds to\n$p_{\\theta}\\left( “a” \\middle| \\varnothing \\right)$. While if the\ncurrent permutation is $z = \\lbrack 2,\\ 4,\\ 3,\\ 1\\rbrack$, it means\nXLNet will consider the second and the fourth words\n$x_{z_{&amp;lt; t}} = \\lbrack 2,\\ 4\\rbrack$ when predicting the probability of\nthe third word which corresponds to\n$p_{\\theta}\\left( “a” \\middle| “is”,\\ “sentence” \\right)$.\n\nAs you have probably figured out, there is something missing from the\nway the model has been presented so far: how does the model know about\nword order? The model can compute\n$p_{\\theta}\\left( “This” \\middle| “a” \\right)$ as well as\n$p_{\\theta}\\left( “This” \\middle| “is” \\right)$. Ideally it should know\nsomething about the relative position of “This” and “is” and also of\n“a”. Otherwise it would just think all tokens in the sequence are\nequally likely to be next to one-another. And that’s what the attention\nmask does!\n\nAttention Mask\n\nThe transformer architecture addresses this problem by adding\nmasking/zeroing the words that are not in the provided context. As a\nconcrete example, consider the following permutation\n$z = \\lbrack 3,\\ 2,\\ 4,\\ 1\\rbrack$. When calculating the probability of\nthe $1^{st}$ element in that order, the model has no context as the other\ntokens have not yet been seen. So the mask would be\n$\\lbrack 0,\\ 0,\\ 0,\\ 0\\rbrack$ as shown below:\n\n\n    \n\n\nFor the 2nd element (token 2), the mask is\n$\\lbrack 0,\\ 0,\\ 1,\\ 0\\rbrack$ as its only context is token 3. Following\nthat logic, the $3^{rd}$ and $4^{th}$ elements (tokens 4 and 1) have masks\n$\\lbrack 0,\\ 1,\\ 1,\\ 0\\rbrack$ and $\\lbrack 0,\\ 1,\\ 1,\\ 1\\rbrack$\nrespectively as shown in the following figures:\n\n\n    \n\n\nAnother way to look at this is that the training objective will contain\nthe following terms in case of the $z = \\lbrack 3,\\ 2,\\ 4,\\ 1\\rbrack$\npermutation; where underscores represent what has been masked:\n\n\\[p_{\\theta}\\left( &quot;a&quot; \\middle| \\_\\_,\\ \\_\\_,\\ \\_\\_,\\ \\_\\_ \\right)\\]\n\n\\[p_{\\theta}\\left( &quot;is&quot; \\middle| \\_\\_,\\ \\_\\_,\\ &quot;a&quot;,\\ \\_\\_ \\right)\\]\n\n\\[p_{\\theta}\\left( &quot;sentence&quot; \\middle| \\_\\_,\\ &quot;is&quot;,\\ &quot;a&quot;,\\ \\_\\_ \\right)\\]\n\n\\[p_{\\theta}\\left( &quot;This&quot; \\middle| \\_\\_,\\ &quot;is&quot;,\\ &quot;a&quot;,\\ &quot;sentence&quot; \\right)\\]\n\nBut wait a minute! There remains one oversight to address: As you can\nsee, the probability of “sentence” in $4^{th}$ position the previous\npermutation should be different than when “sentence” is in the $1^{st}\nposition. In other words, we need to use the current word position when\ncalculating the probability; like so:\n\n\\[p_{\\theta}\\left( &quot;sentence&quot; \\middle| \\_\\_,\\ &quot;is&quot;,\\ &quot;a&quot;,\\ 4 \\right)\\]\n\nAnd this paper deals with problem by providing a “two-stream” self\nattention mechanism.\n\nTwo-stream Self-Attention\n\nThe solution to this problem is a two-stream self-attention mechanism;\nwhere the standard self-attention is divided into two parts or streams:\n\n\n  \n    Content Stream: The content stream (denoted by $h$) cares about\nthe context of the preceding tokens including the current token,\nin other words the “content’.\n  \n  \n    Query Stream: The query stream (denoted by $g$) cares about the\ncontext of the preceding tokens including the position of the\ncurrent token.\n  \n\n\nThe following figure shows just two layers of this two-stream self-attention:\n\n\n    \n\n\nContent Stream\n\nThe content vector of a token at position $i$ and at self-attention\nlayer $m$ is denoted by $h_{i}^{m}$. All content stream vectors are\ninitialized with token embeddings. It’s calculated according to the\nfollowing formula:\n\n\\[p_{\\theta}\\left( x \\middle| x_{z_{&amp;lt; t}} \\right) = \\frac{\\exp\\left( {e(x)}^{T}.h_{\\theta}\\left( x_{z_{&amp;lt; t}} \\right) \\right)}{\\sum_{x&#39;}^{}{\\exp\\left( {e(x&#39;)}^{T} \\right).h_{\\theta}\\left( x_{z_{&amp;lt; t}} \\right)}}\\]\n\nWhere:\n\n\n  \n    $x$: is the current token at position $t$ in the current permutation$z$.\n  \n  \n    $e(x)$: is the word embedding of the current token.\n  \n  \n    $x_{z_{&amp;lt; t}}$: is the preceding tokens to the current one.\n  \n  \n    $h_{\\theta}\\left( x_{z_{&amp;lt; t}} \\right)$: denotes the hidden\nrepresentation of $x_{z_{&amp;lt; t}}$ produced by the shared Transformer\nnetwork after proper masking.\n  \n\n\nConsidering the $z = \\lbrack 3,\\ 2,\\ 4,\\ 1\\rbrack$ permutation, at each\nlayer, the content vector $h_{i}$ is updated using the other context\nvectors that remained unmasked and itself. Thus, $h_{1}$ is\nupdated with the knowledge of $x_{3}$, $x_{2}$ , $x_{4}$ and $x_{1}$ as\nshown in the following figure:\n\n\n    \n\n\nAnd the following figure shows that all content vectors are contributing\nin calculating the key-value pair in the attention mechanism, while the\ncurrent content vector is used for the query vector of the attention\nmechanism.\n\n\n    \n\n\n\n  Note:\nThe content stream is the same as the standard self-attention found in\nthe vanilla transformer architecture.\n\n\nQuery Stream\n\nThe query vector of a token at position $i$ and at self-attention layer\n$m$ is denoted by $g_{i}^{m}$. All query stream vectors are initialized\nwith a generic embedding vector $w$ added to positional embeddings. Note\nthat $w$ is the same no matter the token. It’s calculated according to\nthe following formula:\n\n\\[p_{\\theta}\\left( x \\middle| x_{z_{&amp;lt; t}} \\right) = \\frac{\\exp\\left( {e(x)}^{T}.g_{\\theta}\\left( x_{z_{&amp;lt; t}},\\ z_{t} \\right) \\right)}{\\sum_{x&#39;}^{}{\\exp\\left( {e(x&#39;)}^{T} \\right).g_{\\theta}\\left( x_{z_{&amp;lt; t}},\\ z_{t} \\right)}}\\]\n\nWhere:\n\n\n  \n    $x$: is the current token at position $t$ in the current permutation $z$.\n  \n  \n    $e(x)$: is the word embedding of the current token.\n  \n  \n    $x_{z_{&amp;lt; t}}$: is the preceding tokens to the current one.\n  \n  \n    $g_{\\theta}\\left( x_{z_{&amp;lt; t}},\\ z_{t} \\right)$: denotes the hidden\nrepresentation of $x_{z_{&amp;lt; t}}$ produced by the shared Transformer\nnetwork after proper masking which additionally take the target\nposition $z_{t}$ as input.\n  \n\n\nConsidering the same permutation in the content stream\n$z = \\lbrack 3,\\ 2,\\ 4,\\ 1\\rbrack$, at each layer, the query\nvector $g_{i}$ is updated using the other context vectors that remained\nunmasked and itself. Thus, $h_{1}$ is updated with the\nknowledge of $x_{3}$, $x_{2}$ , $x_{4}$ and $w_{1}$ without considering\n$x_{1}$ as shown in the following figure:\n\n\n    \n\n\nAnd the following figure shows that all content vectors (except the\ncurrent one) are contributing in calculating the key-value pair in the\nattention mechanism, while the current query vector is used for the\nquery vector of the attention mechanism.\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "SpanBERT",
        "url"       : "/language-modeling/SpanBERT",
        "date"      : "24/07/2019",
        "content": "SpanBERT is a model created by Facebook AI and Allen Institute in\nJanuary 2019 and published in this paper “SpanBERT: Improving\nPre-training by Representing and Predicting\nSpans”. SpanBERT is just an\nextension to BERT where it better represents and predict continuous\nrandom spans of text, rather than random tokens. This is crucial since\nmany NLP tasks involve spans of text rather than single tokens. SpanBERT\nis different from BERT in both the masking scheme and the training\nobjectives:\n\n\n  \n    Span Masking:\nSpanBERT masks random contiguous spans, rather than random\nindividual tokens which forces the model to predict entire spans\nsolely using the context in which they appear.\n  \n  \n    SBO:\nSpanBERT uses a novel span-boundary objective (SBO) so the\nmodel learns to predict the entire masked span from the observed\ntokens at its boundary which encourages the model to store this\nspan-level information at the boundary tokens, which can be easily\naccessed during the fine-tuning stage.\n  \n  \n    No NSP:\nSpanBERT doesn’t use the NSP objective unlike BERT.\n  \n\n\n\n    \n\n\nSpan Masking\n\nGiven a sequence of tokens X, they selected a subset of tokens by\niteratively sampling spans of text until the masking budget (e.g. 15% of\nX) has been spent. And they following the following steps when masking a\nsubset of tokens:\n\n\n  They randomly sample a span length (number of words) from a\ngeometric distribution\n$\\ell \\sim Geo(p) = p.\\left( 1 - p \\right)^{n - 1}$ where\n$p = 0.2$ and $\\ell_{\\max} = 10$ which is skewed towards shorter\nspans as shown in the following figure. :\n\n\n\n    \n\n\n\n  \n    Then, they randomly select the starting point for the span to be\nmasked from a uniform distribution. They always sample a sequence\nof complete words (instead of subword tokens) and the starting\npoint must be the beginning of one word.\n  \n  \n    As in BERT, they also masked 15% of the tokens in total: replacing\n80% of the masked tokens with [MASK], 10% with random tokens and\n10% with the original tokens.\n  \n\n\nSBO\n\nSpan selection models typically create a fixed-length representation of\na span using its boundary tokens (start and end). To support such\nmodels, we would ideally like the representations for the end of the\nspan to summarize as much of the internal span content as possible. We\ndo so by introducing a Span Boundary\nObjective (SBO) that involves predicting each token of a\nmasked span using only the representations of the observed tokens at the\nboundaries.\n\nFormally, they calculated the SBO loss function by following these\nsteps:\n\n\n  \n    Given an input sequence of $X = x_{1},\\ …,\\ x_{n}$ and a masked\nspan of tokens $\\left( x_{s},…,\\ x_{e} \\right) \\in Y\\ $, where\n$\\left( s,\\ e \\right)$ indicates its start and end positions\nrespectively.\n  \n  \n    They represented each token $x_{i}$ in the span using the output\nencodings of the external boundary tokens $x_{s - 1}$ and\n$x_{e + 1}$, as well as the position embedding of the target token\n$p_{i - s + 1}$:\n  \n\n\n\\[h_{0} = \\left\\lbrack x_{s - 1};x_{e + 1};p_{i - s + 1} \\right\\rbrack\\]\n\n\n  Then, they implemented the representation function as a 2-layer\nfeed-forward network with GeLU activations and layer normalization\n\n\n\\[h_{1} = \\text{LayerNorm}\\left( \\text{GeLU}\\left( W_{1}.h_{0} \\right) \\right)\\]\n\n\\[y_{i} = \\text{LayerNorm}\\left( \\text{GeLU}\\left( W_{2}.h_{1} \\right) \\right)\\]\n\n\n  Finally, they used the vector representation $y_{i}$ to predict the\ntoken $x_{i}$ and compute the cross-entropy loss exactly like the\nMLM objective.\n\n\n\\[\\mathcal{L}_{\\text{SBO}}\\left( x_{i} \\right) = - log\\ P\\left( x_{i}\\  \\middle| \\ y_{i} \\right)\\]\n\nFor example, given the following sequence “Super Bowl 50 was an American\nfootball game to determine the champion” where the span “an American\nfootball game” is masked. The span boundary objective (SBO) uses the\noutput representations of the boundary tokens, x4 and x9 (in blue), to\npredict each token in the masked span.\n\n\n    \n\n\nThe equation shows the MLM and SBO loss terms for predicting the token,\nfootball (in pink), which as marked by the position embedding $p_{3}$,\nis the third token from $x_{4}$.\n\n\\[\\mathcal{L}\\left( \\text{football} \\right) = \\mathcal{L}_{\\text{MLM}}\\left( \\text{football} \\right) + \\mathcal{L}_{\\text{SBO}}\\left( \\text{football} \\right) = - log\\ P\\left( \\text{football} \\middle| \\ x_{7} \\right) - log\\ P\\left( \\text{football} \\middle| \\ x_{4},x_{9},p_{3} \\right)\\]\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "RoBERTa",
        "url"       : "/language-modeling/RoBERTa",
        "date"      : "26/07/2019",
        "content": "RoBERTa, stands for “Robustly optimized BERT approach”,\nis an approach to train BERT created by Facebook AI in 2019 and\npublished in this paper: “RoBERTa: A Robustly Optimized BERT\nPretraining Approach”. The official code\nfor this paper can be found on Facebook’s FairSeq official GitHub repository:\nfairseq/roberta.\n\n\n    \n\n\nThe authors of this paper has found out that BERT, when published, was\nsignificantly under-trained. So, they wrote this paper introducing an\napproach with the following:\n\n\n  \n    Training the BERT model longer, on longer sequences, with bigger\nbatches, over more data which increased the character-level BPE\nvocabulary size from 30K to 50K.\n  \n  \n    Dynamically changing the masking pattern applied to the training data.\n  \n  \n    Removing the next sentence prediction (NSP) objective.\n  \n\n\nAnd just by doing these simple modifications, RoBERTa was able to\nexceed BERT and XLNet in almost all tasks on GLUE:\n\n\n    \n\n\nIn the following sections, we are going to discuss each modification out\nof the proposed three modifications.\n\nData\n\nBERT is trained on a combination of BooksCorpus and English Wikipedia\nwhich totals 16GB of uncompressed text. On the other hand, RoBERTa is\ntrained on a combination of the following data which totals 160 GB of\nuncompressed text:\n\n\n  \n    BooksCorpus and English\nWikipedia (This is the original data used to train BERT).\n  \n  \n    CC-NEWS:\ncollected from the English portion of the CommonCrawl News\ndataset. The data contains 63 million English news articles\ncrawled between September 2016 and February 2019. (76GB after\nfiltering).\n  \n  \n    OpenWebText: it’s\nthe web content extracted from URLs shared on Reddit with at least\nthree upvotes. (38GB).\n  \n  \n    Stories: a dataset containing a subset of CommonCrawl data filtered\nto match the story-like style of Winograd schemas. (31GB).\n  \n\n\nThe following table contains a simple comparison between RoBERTa and\nBERT using different amount of data:\n\n\n    \n\n\nBatch Size\n\nPast work in NMT has shown that training with very large mini-batches\ncan both improve optimization speed and end-task performance when the\nlearning rate is increased appropriately. BERT is also amenable to large\nbatch training.\n\nThe original BERT model was trained for 1 million steps with batch size\nof 256 sequences. So, in this paper the publishers increased the batch\nsize and compared the performance of BERT on the development set of\nBooksCorpus and English Wikipedia as shown in the following table:\n\n\n    \n\n\nWe can see that training with large batches improves perplexity for the\nmasked language modeling objective, as well as end-task accuracy. Large\nbatches are also easier to parallelize via distributed data parallel\ntraining.\n\nAnd that was the first modification in the paper which is increasing the\namount of data used for training BERT with bigger batch sizes. Now,\nlet’s get to the second one.\n\nStatic Vs. Dynamic Masking\n\nAs discussed before, BERT relies on randomly masking and predicting\ntokens. The original BERT implementation performed masking once during\ndata preprocessing, resulting in a single static mask.\n\nIn the paper, they proposed two different techniques for masking:\n\n\n  \n    Static Masking: Where they duplicate training data\n10 times and mask each time with different mask pattern.\n  \n  \n    Dynamic Masking: Where they generated the masking\npattern every time a sequence is fed to the model.\n  \n\n\nAnd the following table shows the result in comparison with the official\nresults from BERT where we can see clearly that both proposed methods before\nbetter than the single masking:\n\n\n    \n\n\nNSP\n\nIn the original BERT paper, the model is trained to predict whether the\nobserved sentence is next to the previous sentence or not via an\nauxiliary Next Sentence Prediction (NSP) loss. The NSP loss was\nhypothesized to be an important factor in training the original BERT\nmodel and removing it hurts the performance. However, some recent work\nhas questioned the necessity of the NSP loss.\n\nSo, to put this issue to rest, the publishers of the paper compared\nseveral alternative training formats:\n\n\n  \n    Segment-pair + NSP:\nThis follows the original input format used in BERT. Each input\ncontains a pair of segments; each segment could contain multiple\nsentences.\n  \n  \n    Sentence-pair + NSP:\nEach input contains a pair of sentences.\n  \n  \n    Full-sentences:\nEach input is packed with full sentences, such that the\ntotal length is at most 512 tokens. Inputs may cross document\nboundaries. When we reach the end of one document, we begin\nsampling sentences from the next document and add an extra\nseparator token between documents. The NSP loss is removed.\n  \n  \n    Doc-sentences:\nEach input is packed with sentences of the same document, such\nthat the total length is at most 512. The NSP loss is also\nremoved.\n  \n\n\nAnd the following table contains a comparison of these different\ntraining format on four different tasks.\n\n\n    \n\n\nAnd in the table, we can see the following:\n\n\n  \n    We find that using individual sentences hurts performance, which\nthey hypothesized is because the model is not able to learn\nlong-range dependencies.\n  \n  \n    Removing the NSP loss matches or slightly improves the performance.\n  \n  \n    Restricting sequences to come from a single document\n(Doc-sentences) performs slightly better than packing\nsequences from multiple documents (full-sentences).\n  \n\n\nBase Vs Large RoBERTa\n\nThe same as BERT, the published or RoBERTa created two models sizes for\nit in order to compare performance:\n\n\n  \n    RoBERTa BASE: Comparable in size to the BERT-base.\n  \n  \n    RoBERTa LARGE: Comparable in size to BERT-large.\n  \n\n\nThe following summarizes the difference between both models:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Big Models pollute Earth",
        "url"       : "/language-modeling/pollution",
        "date"      : "02/08/2019",
        "content": "Recent progress in hardware and methodology for training neural networks\nhas ushered in a new generation of large networks. These models have\nobtained notable gains in accuracy across many NLP tasks. However, these\naccuracy improvements depend on the availability of exceptionally large\ncomputational resources that necessitate similarly substantial energy\nconsumption. As a result, these models are costly to train both\nfinancially and environmentally.\n\nThis paper: Energy and Policy Considerations for Deep Learning in\nNLP, published in 2019 by the\nUniversity of Massachusetts Amherst, is explaining that these big models\nare costly environmentally due to the carbon footprint required to\ngenerate electricity as it’s the main source of electricity in the top\nthree cloud service providers according to the following table:\n\n\n    \n\n\nTo heighten the awareness of the NLP community to this issue,\nresearchers of this paper have characterized the dollar cost and carbon\nemissions that result from training popular off-the-shelf NLP models.\nThey have done that by estimating the kilowatts of electrical energy\ngenerated to power the required hardware which can be calculated via the\nfollowing formula:\n\n\\[p_{t} = \\frac{1.58t\\left( p_{c} + p_{r} + gp_{g} \\right)}{1000}\\]\n\nWhere:\n\n\n  \n    $p_{t}$: total power consumption (in watts).\n  \n  \n    $p_{c}$: the average power draw (in watts) from all CPU sockets\nduring training.\n  \n  \n    $p_{r}$: the average power draw from all DRAM (main memory) sockets\nduring training.\n  \n  \n    $p_{g}$: the average power draw of one GPU during training. For $g$\nGPUs, the average power draw will be $gp_{g}$.\n  \n  \n    $1.58t$: the Power Usage Effectiveness (PUE), which accounts for the\nadditional energy required to support the compute infrastructure\n(mainly cooling). In 2018 (one year before the publish date of this\npaper), the global average PUE for data centers was 1.58 per hour.\n  \n  \n    $\\frac{1}{1000}$: to convert the power to kilo watt.\n  \n\n\nAccording to EPA (Environmental Protection Agency) in 2018, each one kilowatt\nproduces 0.95 pound of CO~2~. Then, the average CO~2~emissions will be:\n\n\\[Co_{2}e = 0.95\\ p_{t}\\]\n\nIn the paper, they analyzed five different models as shown in the\nfollowing table. These four models were BERT, ELMO, standard Transformer\n(T2T), Evolved Transformer (NAS), and finally GPT-2. The following table\nshows the estimated cost of training a model in terms of CO~2~ emissions\n(lbs) and cloud compute cost (USD) according to the previous equations:\n\n\n    \n\n\nBased on the previous table and knowing that an air travel from New York\nto San Francisco emits around 1984 lbs of CO~2~, we can clearly see that\ntraining BERT~base~ for around 90 hours will have the same effect. And\ndon’t get me started on the fact that training NAS on 8 Tesla P100 GPUs\nemits the same CO~2~ as approximately 315 flights from New York to San\nFrancisco.\n\nYou can use this tool:\nexperiment-impact-tracker\nto track energy usage, carbon emissions, and compute utilization of your\nsystem.\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "StructBERT",
        "url"       : "/language-modeling/StructBERT",
        "date"      : "13/08/2019",
        "content": "StructBERT stands for “Structural BERT” which is an extension of\nBERT created by\nincorporating language structures into pre-training. StructBERT was\nproposed in 2019 by Alibaba Group and published in their “StructBERT:\nIncorporating Language Structures Into Pre-Training For Deep Language\nUnderstanding” paper. The\nofficial code for this paper can be found in the following GitHub\nrepository:\nalibaba/StructBERT.\n\nStructBERT incorporates the language structures into BERT by\npre-training BERT with two tasks/objectives that make the most use of\nthe sequential order of words and sentences along with the Masked\nLanguage Modeling (MLM) objective. These two objectives are:\n\n\n  \n    Word Structural Objective.\n  \n  \n    Sentence Structural Objective.\n  \n\n\nMLM Recap\n\nBefore getting into more details about StructBERT, let’s first recap how\nMLM objective worked in\nBERT. Given an\ninput sequence, 15% of the tokens in that sequence are replaced with a\n[MASK] token; and the type of mask will be different according to the\nfollowing distribution\n\n\n  \n    80% of the time: the mask will be [MASK].\n  \n  \n    10% of the time: the mask will be a random word.\n  \n  \n    10% of the time: The mask will be the original word.\n  \n\n\nThen, BERT will have to learn to predict these masked tokens correctly while\npre-training.\n\n\n    \n\n\nWord Structural Objective\n\nThis new word objective is jointly trained together with the original\nMLM objective; the new objective takes the word order into\nconsideration. The way this objective works is that they randomly choose\n$5\\%$ of the unmasked trigrams to be shuffled and then StructBERT is\npre-trained to predict the original order of the tokens. As shown in the\nfollowing figure, the trigram (t2, t3, and t4) was shuffled to (t3, t4,\nand t2).\n\n\n    \n\n\nGiven a randomly shuffled span of $K$ tokens (trigram means $K = 3$),\nthe word structural objective is equivalent to maximizing the likelihood\nof placing every shuffled token in its correct position. So, given a set\nof trainable parameters $\\theta$, this objective can be defined as:\n\n\\[\\underset{\\theta}{\\text{arg}\\max}{\\sum_{}^{}{\\log\\left( P\\left( \\text{pos}_{1} = t_{1},\\ \\text{... pos}_{K} = t_{K} \\middle| t_{1},\\ ...t_{K}; \\theta \\right) \\right)}}\\]\n\nThey studied the effect of this objective in comparison with MLM in BERT\nduring self-supervised pre-training. The following figure illustrates\nthe loss (left) and accuracy (right) of word and sentence prediction\nover the number of pre-training steps for StructBERT-Base and BERT-Base:\n\n\n    \n\n\nWe can see that the the shuffled token prediction objective (blue line)\nled to lower loss and higher accuracy more than MLM (red line).\n\nSentence Structural Objective\n\nThe Next Sentence Prediction (NSP) task is\nconsidered easy for the original BERT model (the prediction accuracy of\nBERT can easily achieve 97%-98% in this task). Therefore, they extended\nthe NSP task by considering both the next sentence and the previous\nsentence to make the pre-trained language model aware of the sequential\norder of the sentences in a bidirectional manner.\n\nAs illustrated in the following figure, they sampled the data for this\ntask in pairs $\\left( S_{1},\\ S_{2} \\right)$ where the two sentences are\nconcatenated together with the separator token [SEP] in between, as\ndone in BERT. Given a first sentence $S_{1}$, they sampled $S_{2}$ to\nbe:\n\n\n  \n    The next sentence $\\frac{1}{3}$ of the time.\n  \n  \n    The previous sentence $\\frac{1}{3}$ of the time.\n  \n  \n    A random sentence from the data $\\frac{1}{3}$ of the time.\n  \n\n\n\n    \n\n\nAnd given the input sequence, the model has to predict whether $S_{2}$\nis the next sentence that follows $S_{1}$ (class label=1), or the\nprevious sentence that precedes $S_{1}$ (class label=2), or a random\nsentence from a different document (class label=0).\n\nThey studied the effect of this objective during self-supervised\npre-training. The following figure illustrates the loss (left) and\naccuracy (right) of word and sentence prediction over the number of\npre-training steps for StructBERT-Base and BERT-Base:\n\n\n    \n\n\nAs we can see, the new sentence structural objective in StructBERT leads\nto a more challenging prediction task than that in BERT enabling\nStructBERT to exploit inter-sentence structures, which benefits\nsentence-pair downstream tasks.\n\nExperiments\n\nIn this paper, they pre-trained StructBERT on documents from English\nWikipedia (2,500M words) and BookCorpus using WordPiece models. The\nmaximum length of input sequence was set to 512. They used Adam\noptimizer ($\\beta_{1} = 0.9,\\ \\beta_{2} = 0.999$). They used L2 weight\ndecay regularization of $0.01$. The learning rate was set to $1e^{- 4}$\nwith warm-up over the first 10% of the total steps, and linear decay at\nthe rest. The dropout probability was set to $0.1$ for every layer. They\nused GeLU activation. They pre-trained two model sizes:\n\n\n\n    \n        \n            \n            $$N$$\n            $$d_{\\text{ff}}$$\n            $$h$$\n            # parameters\n            Hardware\n        \n    \n    \n        Base\n        12\n        768\n        12\n        110 M\n        64 GPU V100 + 38 hours\n    \n    \n        Large\n        24\n        1024\n        16\n        340 M\n        64 GPU V100 + 7 days\n    \n\n\n\nNow, we are going to see the performance of StructBERT over multiple\ndownstream tasks:\n\n\n  Natural Language Understanding:\nThe following table shows the results of StructBERT on the\nGLUE test set, which are scored by the GLUE evaluation server. The\nnumber below each task denotes the number of training examples. The\nstate-of-the-art results are in bold. All the results are obtained\nfrom the leaderboard\n(StructBERT submitted under a different model name ALICE):\n\n\n\n    \n\n\n\n  Natural Language Inference:\nThe following table shows the accuracy of multiple models on SNLI\ndataset. As seen from the table, StructBERT outperformed all\nexisting systems on SNLI, creating new state-of-the-art results\n91.7%, which amounts to 0.4% absolute improvement over the previous\nstate-of-the-art model SJRC and 0.9% absolute improvement over BERT.\n\n\n\n    \n\n\n\n  Question Answering:\nThe following table shows the results of SQuAD dataset where\nwe can see that StructBERT model is superior to all other models\nexcept\nXLNet+DA. It\ndemonstrates the effectiveness of StructBERT in modeling the\nquestion-paragraph relationship.\n\n\n\n    \n\n\nTo study the effect of the new pre-training tasks over fine-tuning, they\nperformed ablation study using StructBERT-Base architecture on six\ndifferent downstream tasks as shown in the following table:\n\n\n    \n\n\nBased on this study, they found out the following:\n\n\n  \n    The two structural objectives were both critical to most of the\ndownstream tasks, except for the word structural objective in the\nSNLI task.\n  \n  \n    The StructBERT model with structural pre-training consistently\noutperformed the original BERT model, which shows the effectiveness\nof the proposed structural objectives.\n  \n  \n    For the sentence-pair tasks such as MNLI, SNLI, QQP and SQuAD,\nincorporating the sentence structural objective significantly\nimproved the performance.\n  \n  \n    For the single-sentence tasks such as CoLA and SST-2, the word\nstructural objective played the most important role. Especially in\nthe CoLA task, which is related to the grammatical error correction,\nthe improvement was over 5%. The ability of reconstructing the order\nof words in pre-training helped the model better judge the\nacceptability of a single sentence.\n  \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "TinyBERT",
        "url"       : "/language-modeling/TinyBERT",
        "date"      : "23/09/2019",
        "content": "TinyBERT is a distilled version of BERT using a novel knowledge\ndistillation method called “Transformer distillation” that was specially\ndesigned for Transformer models such as\nBERT. TinyBERT was\nproposed in 2019 by Huawei Noah’s Ark Lab and published in this paper\nunder the same name “TinyBERT: Distilling Bert For Natural Language\nUnderstanding”. The official code\nfor this paper can be found in the following GitHub repository:\nTinyBERT.\n\n\n    \n\n\nKnowledge distillation (KD) is a commonly used technique for reducing\nthe size of big deep learning models. This technique was proposed by\nGeoffrey Hinton back in 2015 in this paper: Distilling the knowledge in\na neural network. KD aims to\ntransfer the knowledge embedded in a large network (called “teacher”) to\na small network (called “student”) where the student network is trained\nto reproduce the behaviors of the teacher network.\n\nThe proposed Transformer Distillation method works differently based on\nthe type of layer to be distilled. Additionally, this method performs\nthe knowledge distillation at both the pre-training and fine-tuning\nstages which ensures that TinyBERT can capture both the general-domain\nand task-specific knowledge of the teacher BERT.\n\nThe following table shows a comparison between BERT-base,the proposed TinyBERT and other distilled variations:\n\n\n    \n\n\nTransformer Distillation\n\nAs said earlier, Transformer Distillation is a novel Knowledge\nDistillation technique for compressing transformer-based models and\nconsidering all types of layers including transformer layers, the\nembedding layer, and the prediction layer. Each layer will be distilled\ndifferently as we are going to see later.\n\nAssuming that the student model has M Transformer layers and teacher\nmodel has N Transformer layers, the student can acquire knowledge from\nthe teacher by minimizing the following objective:\n\n\\[\\mathcal{L}_{\\text{model}} = \\sum_{m = 0}^{M + 1}{\\lambda_{m}\\mathcal{L}_{\\text{layer}}\\left( S_{m},\\ T_{g\\left( m \\right)} \\right)}\\]\n\nWhere:\n\n\n  \n    $S$ and $T$ refer to the Student model and the Teacher model\nrespectively.\n  \n  \n    $\\lambda_{m}$ is the hyper-parameter that represents the importance\nof the m-th layer’s distillation. In the paper, they used\n$\\lambda = 1$.\n  \n  \n    $g\\left( m \\right)$ is a mapping function that maps m-th student\nlayer to a certain Teacher layer. In the paper, they used\n$g\\left( m \\right) = 3m$; knowing that $g\\left( 0 \\right) = 0$ which\nis the mapping of the embedding layer and\n$g\\left( M + 1 \\right) = N + 1$ which is the mapping of the\nprediction layer.\n  \n  \n    $\\mathcal{L}_{\\text{layer}}$ refers to the loss function of a given\nmodel layer which changes based on the layer type; it can be\ndescribed in the following formula:\n  \n\n\n\\[\\mathcal{L}_{\\text{layer}}\\left( S_{m},\\ T_{g\\left( m \\right)} \\right) = \\left\\{ \\begin{matrix}\n\\mathcal{L}_{\\text{embd}}\\left( S_{0},\\ T_{0} \\right),\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ m = 0 \\\\\n\\mathcal{L}_{\\text{hid}}\\left( S_{m},\\ T_{g\\left( m \\right)} \\right) + \\mathcal{L}_{\\text{attn}}\\left( S_{m},\\ T_{g\\left( m \\right)} \\right),\\ \\ \\ M \\geq m &amp;gt; 0 \\\\\n\\mathcal{L}_{\\text{pred}}\\left( S_{M + 1},\\ T_{N + 1} \\right),\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ m = M + 1 \\\\\n\\end{matrix} \\right.\\]\n\nEmbedding-Layer Distillation\n\nHere, the student embedding layer $E^{S} \\in \\mathbb{R}^{l \\times d}$\nhas the same size as the teacher embedding layer\n$E^{T} \\in \\mathbb{R}^{l \\times d}$ and it acquires knowledge by\nminimizing the following mean-squared error function where the matrix\n$W_{e} \\in \\mathbb{R}^{d \\times d}$ is a learnable linear transformation\nmatrix, $l$ is the length of the input text and $d$ is the model\ndimension:\n\n\\[\\mathcal{L}_{\\text{embd}} = \\text{MSE}\\left( E^{S}W_{e},\\ E^{T} \\right)\\]\n\nTransformer-Layer Distillation\n\nAs show in the following figure, the transformer-layer distillation\nconsists of two parts:\n\n\n    \n\n\n\n  Attention-based distillation $\\mathcal{L}_{\\text{attn}}$:\nThat ensures that the linguistic knowledge is transferred from\nteacher BERT to student TinyBERT where the student learns to fit the\nattention matrices of multi-head attention from the teacher, and the\nobjective is defined as:\n\n\n\\[\\mathcal{L}_{\\text{attn}} = \\frac{1}{h}\\sum_{i = 1}^{h}{\\text{MSE}\\left( A_{i}^{S},\\ A_{i}^{T} \\right)}\\]\n\n    Where $h$ is the number of attention heads and\n$A_{i}^{S} \\in \\mathbb{R}^{l \\times 1}$ refers to the attention matrix\ncorresponding to the i-th head of the student model and $l$ is the input\ntext.\n\n\n  Hidden states based distillation $\\mathcal{L}_{\\text{hid}}$:\nHere, the student hidden layer $H^{S} \\in \\mathbb{R}^{l \\times d’}$\nhas the smaller size than the teacher embedding layer\n$H^{T} \\in \\mathbb{R}^{l \\times d}$ and it acquires knowledge by\nminimizing the following mean-squared error function where the\nmatrix $W_{f} \\in \\mathbb{R}^{d’ \\times d}$ is a learnable linear\ntransformation matrix:\n\n\n\\[\\mathcal{L}_{\\text{hid}} = \\text{MSE}\\left( H^{S}W_{h},\\ H^{T} \\right)\\]\n\nPrediction-Layer Distillation\n\nIn addition to imitating the behaviors of intermediate layers, they also\nused the knowledge distillation to fit the predictions of teacher model\naccording to the following loss function\n\n\\[\\mathcal{L}_{\\text{pred}} = \\text{softmax}\\left( z^{T} \\right).\\text{log\\_softmax}\\left( \\frac{z^{S}}{t} \\right)\\]\n\nWhere $z^{S}$ and $z^{T}$ are the logits vectors predicted by the\nstudent and teacher respectively, $\\text{log_softmax}$ means the log\nlikelihood, and $t$ means the temperature value. According to the paper,\n$t = 1$ performs well.\n\nTinyBERT Learning\n\nAs shown in the following figure, TinyBERT learning consist of two stages that\nare complementary to each other which are General\nDistillation and Task-specific Distillation.\nAlthough there is a big gap between BERT and TinyBERT in model size, by\nperforming the proposed two-stage distillation, the TinyBERT can achieve\ncomparable performances as large BERT in various NLP tasks.\n\n\n    \n\n\nGeneral Distillation\n\nIn general distillation, the original BERT without fine-tuning is used\nas the teacher model along with a large-scale text corpus as the basic\nlearning material. By performing the proposed Transformer distillation\non the text from general domain, we obtain a general TinyBERT that can\nbe fine-tuned for downstream tasks later.\n\n\n  Note:\nDue to the big reductions in the hidden/embedding size and the layer\nnumber, general TinyBERT performs relatively worse than the original\nBERT\n\n\nTask-specific Distillation:\n\nIn the task-specific distillation, the fine-tuned BERT is used as the\nteacher model along with an augmented data to extend the task-specific\ntraining set. With learning more task-related materials, the\ngeneralization capabilities of student model can be further improved.\n\nTo augment the task-specific training set, they used a pre-trained\nlanguage model BERT and GloVe word embeddings to do word-level\nreplacement as shown in the following algorithm:\n\n\n    \n\n\nWhich can be explained in the following steps:\n\n\n  \n    First, mask each word piece in a sentence.\n  \n  \n    If the selected word is a word-piece, then use BERT as a language\nmodel to predict $N$ most-likely words. Otherwise, use GloVe\nembeddings to get the top $N$ similar words.\n  \n  \n    Then, choose a random number uniformally. If the number is less than\na certain threshold number $p_{t}$, then choose a random candidate\nfrom the suggested one. Otherwise, keep the word as it is.\n  \n  \n    In the paper, they applied this data augmentation method $N = 20$\ntimes to all the sentences of a downstream task while setting\n$p_{t} = 0.4$ for all our experiments.\n  \n\n\nExperiments &amp;amp; Results\n\nThey evaluated TinyBERT on the General Language Understanding Evaluation\n(GLUE) benchmark, which is a collection of diverse natural language\nunderstanding tasks.The evaluation results are presented in the\nfollowing table which shows that TinyBERT is consistently better than\nBERT-SMALL in all the GLUE tasks despite being same in size:\n\n\n    \n\n\nThe following table shows a comparison among three wider and deeper\nvariants of TinyBERT and their evaluation results on different\ndevelopment sets.\n\n\n    \n\n\nWe can clearly see that:\n\n\n  \n    All the three TinyBERT variants can consistently outperform the\noriginal smallest TinyBERT, which indicates that the proposed KD\nmethod works for the student models of various model sizes.\n  \n  \n    For the CoLA task, the improvement is slight when only increasing\nthe number of layers (from 49.7 to 50.6) or hidden size (from 49.7\nto 50.5). To achieve more dramatic improvements, the student model\nshould become deeper and wider (from 49.7 to 54.0).\n  \n  \n    Another interesting observation is that the smallest 4-layer\nTinyBERT can even outperform the 6-layers baselines, which further\nconfirms the effectiveness of the proposed KD method.\n  \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "ALBERT",
        "url"       : "/language-modeling/ALBERT",
        "date"      : "26/09/2019",
        "content": "ALBERT, stands for “A Lite BERT”, reduced version of BERT which is a\nsmaller, faster, cheaper and easier to scale. ALBERT was created by\nGoogle &amp;amp; Toyota Technical Institute in February 2019 and published in\nthis paper: “ALBERT: A Lite Bert For Self-Supervised Learning Of\nLanguage Representations” and you can\nfine the official code for this paper in Google Research’s official GitHub\nrepository: google-research/ALBERT.\n\n\n    \n\n\nALBERT incorporates two parameter reduction techniques that act as a form of\nregularization that stabilizes the training and helps with generalization; and\none new loss function:\n\n\n  \n    Factorized Embedding Parameterization\n  \n  \n    Cross-layer Parameter Sharing\n  \n  \n    Sentence-Order Prediction Loss\n  \n\n\nAlso, The parameter reduction techniques also act as a form of\nregularization that stabilizes the training and helps with\ngeneralization.\n\nThese three modifications significantly reduce the number of\nparameters for BERT without seriously hurting performance. An ALBERT\nconfiguration similar to BERT-large has 18x fewer parameters and can\nbe trained about 1.7x faster as shown in the following table:\n\n\n    \n\n\nAnd since longer training usually leads to better performance, the\nfollowing table shows a comparison between the performance BERT and\nALBERT after the same training time which shows that ALBERT still\nperforms better:\n\n\n    \n\n\nFactorized Embedding Parameterization\n\nIn BERT, the Word embedding size $E$ is tied with the hidden layer size\n$H$, i.e., $E\\  \\equiv \\ H$. This decision appears sub-optimal for the\nfollowing reasons:\n\n\n  \n    Word embeddings are meant to learn context-independent\nrepresentations, whereas hidden-layer embeddings are meant to\nlearn context-dependent representations. And that dictates that\n$H \\gg E$.\n  \n  \n    NLP usually requires the vocabulary size $V$ to be large. If\n$E \\equiv H$, then increasing $H$ increases the size of the\nembedding matrix, which has size $V \\times E$. This can easily\nresult in a model with billions of parameters, most of which are\nonly updated sparsely during training.\n  \n\n\nSo, instead of projecting the one-hot vectors directly into the hidden\nspace of size H, they first projected them into a lower dimensional\nembedding space of size E, and then projected it to the hidden space. By\nusing this decomposition, we reduce the embedding parameters from\n$O(V \\times H)$ to\n$O\\left( \\left( V \\times E \\right) + \\left( E \\times H \\right) \\right)$.\nThis parameter reduction is significant when $H \\gg E$.\n\nCross-layer Parameter Sharing\n\nThe default decision for ALBERT is to share all parameters across\nlayers. The following figure shows the L2 distances and cosine\nsimilarity of the input and output embeddings for each layer. We observe\nthat the transitions from layer to layer are much smoother for ALBERT\nthan for BERT.\n\nThese results show that weight-sharing has an effect on stabilizing\nnetwork parameters. Although there is a drop for both metrics compared\nto BERT, they nevertheless do not converge to 0 even after 24 layers.\n\n\n    \n\n\nSentence-Order Prediction (SOP)\n\nIn this paper, they proposed a new loss function called sentence-order\nprediction (SOP) loss as a replacement for next sentence prediction\n(NSP) used with BERT. The same as NSP, SOP loss uses two consecutive\nsegments from the same document as a positive example. And as negative\nexamples, it uses the same two consecutive segments but with their order\nswapped.\n\nThis forces the model to learn finer-grained distinctions about\ndiscourse-level coherence properties. As shown in the following table,\nit turns out that NSP cannot solve the SOP task at all, while SOP can\nsolve the NSP task to a reasonable degree.\n\n\n    \n\n\nHyper-parameters\n\nIn this part, we are going to talk about the effect of some\nhyper-parameters on ALBERT:\n\n\n  Network Depth (number of layers):\nIf we compare a 3-layer ALBERT model with a 1-layer ALBERT model,\nalthough they have the same number of parameters, the performance\nincreases significantly. However, there are diminishing returns\nwhen continuing to increase the number of layers: the results of a\n12-layer network are relatively close to the results of a 24-layer\nnetwork, and the performance of a 48-layer network appears to\ndecline.\n\n\n\n    \n\n\n\n  Network Width (hidden size):\nUsing a 3-layers ALBERT, we can see that as we increase the hidden\nsize, we get an increase in performance with diminishing returns.\nAt a hidden size of 6144, the performance appears to decline\nsignificantly:\n\n\n\n    \n\n\n\n  Dropout:\nUsing ALBERT-xxlarge at around 1M training steps states that\nremoving dropout helps the downstream tasks which indicates that\nALBERT models didn’t overfit the data.\n\n\n\n    \n\n\n\n  Data:\nAdding more data improvements on the downstream tasks, except\nfor the SQuAD benchmarks (which are Wikipedia-based, and therefore\nare negatively affected by out-of-domain training material).\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Google&#39;s T5",
        "url"       : "/language-modeling/T5",
        "date"      : "23/10/2019",
        "content": "T5 stands for “Text-to-Text Transfer Transformer” which is a\ntext-to-text framework proposed by Google in 2019 and published in this\npaper: “Exploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer”. The\nofficial code for this paper can be found on Google Research’s official\nGitHub repository:\ngoogle-research/text-to-text-transfer-transformer.\n\nFrom the name of the paper, we can see that this 64-pages research paper\ndiscusses transfer learning and its limitations. Transfer learning is\nwhere a model is first pre-trained on a data-rich task using in an\nunsupervised-fashion before being fine-tuned on a downstream task.\nDownstream tasks like machine translation, text classification, ..etc.\nThis has turned out to be as a powerful technique in natural language\nprocessing.\n\nAnd also from the name, we can see it focuses on text-to-text tasks\nwhich are tasks that take text as an input and return text as an output\nlike machine translation, language generation, text summarization,\nquestion answering ...etc. This allows us to use the same model, loss\nfunction, hyper-parameters across diverse set of tasks.\n\nIn order to train a single model on the diverse set of tasks, they fed\nthe model some text for context before feeding the input text and then\nasked to produce some output text. For example, to ask the model to\ntranslate the sentence “That is good.” from English to German, the model\nwould be fed the sequence “translate English to German: That is good.”\nand would be trained to output “Das ist gut.” as shown below:\n\n\n    \n\n\nAgain, this paper does not propose new methods but instead provides a\ncomprehensive perspective on text-to-text tasks. And to perform\nexperiments, it introduces the\nC4 (Colossal Clean\nCrawled Corpus) dataset consisting of hundreds of gigabytes of clean\nEnglish text scraped from the web.\n\nC4\n\nC4 stands for “Colossal Clean Crawled Corpus” which is a cleaned-up\nversion of the publicly-available Common\nCrawl dataset. Common Crawl provides web\nextracted text by crawling the internet producing around 20TB of scraped\ntext data each month.\n\nTo assemble the C4 dataset, they downloaded the web extracted text from\nCommon Crawl that was posted on April 2019, which is about 750 GB. And\nthen they applied the following filtering steps:\n\n\n  \n    They only retained lines that ended in a terminal punctuation mark\n(i.e. a period, exclamation mark, question mark, or end quotation\nmark).\n  \n  \n    They discarded any page with fewer than 5 sentences and only\nretained lines that contained at least 3 words.\n  \n  \n    They removed any page that contained any word on the “List of\nDirty, Naughty, Obscene or Otherwise Bad\nWords”.\n  \n  \n    They removed any line with the word JavaScript since it usually\nindicates an error on the web.\n  \n  \n    Some pages had placeholder “lorem ipsum” text; they removed any page\nwhere the phrase “lorem ipsum” appeared.\n  \n  \n    Some pages inadvertently contained code. Since the curly bracket “{“\nappears in many programming languages (such as Javascript, widely\nused on the web) but not in natural text, they removed any pages\nthat contained a curly bracket.\n  \n  \n    To de-duplicate the dataset, they discarded all but one of any\nthree-sentence span occurring more than once in the data set.\n  \n  \n    Since most of downstream tasks are focused on English-language text,\nthey used langdetect tool\nto filter out any pages that were not classified as English with a\nprobability of at least $0.99$.\n  \n\n\n\n  Note:\nThe C4 dataset is released as part of TensorFlow Datasets. You can\nfind it in this\nlink.\n\n\nModels\n\nAll the models mentioned in this paper are based on the\nTransformer\narchitecture. The implemented version of the transformer used in this\npaper follows closely the originally-proposed form with a few changes:\n\n\n  \n    They used a simplified version of layer normalization where the\nactivations are only re-scaled and no additive bias is applied.\n  \n  \n    While the original Transformer used a sinusoidal position signal or\nlearned position embeddings, they used a simplified form of\nrelative-position embeddings where each “embedding” is simply a\nscalar that represents the offset between the “key” and “query”\nbeing compared. This scalar is a number from 1 to 32 increasing\nlogarithmically. So, if the offset is 4, the embedding is 1. If the\noffset is 32, the embedding is 4. If the offset is 128, the\nembedding is 32. Beyond 128, all relative positions to the same\nembedding which is 32.\n  \n  \n    Also, they shared the position embedding parameters across all\nlayers knowing that each attention head uses a different learned\nposition embedding.\n\n    In this paper, the researchers pre-trained different variations of\nthe Transformer architecture and compared their performance in\ncomparison to the baseline models. First, let’s talk about the\nbaseline model first.\n  \n\n\nBaseline Model\n\nThe baseline model used in this paper is a standard encoder-decoder\nTransformer architecture. It is designed so that the encoder and decoder\nare each similar in size and both are configured to a BERT~BASE~\nconfiguration which means:\n\n\n  \n    Both the encoder and decoder consist of 12 layers (each layer\ncomprising self-attention, optional encoder-decoder attention, and a\nfeed-forward network).\n  \n  \n    The feed-forward networks is a dense layer with an output\ndimensionality of $d_{\\text{ff}} = 3072$ followed by a ReLU\nactivation and another dense layer.\n  \n  \n    The “key” and “value” matrices of all attention mechanisms have an\ninner dimensionality of $d_{\\text{kv}} = 64$ and all attention\nmechanisms have $12$ heads. All other sub-layers and embeddings have\na dimensionality of $d_{\\text{model}} = 768$.\n  \n  \n    For regularization, they used a dropout probability of 0.1\neverywhere in the model.\n\n    In total, this results in a model with about 220 million parameters\nwhich is roughly twice the number of parameters of BERT~BASE~ since\nthis baseline model contains an encoder and a decoder.\n  \n\n\nArchitectural Variants\n\nNow that we have discussed the baseline model used in this paper, let’s\ndiscuss the other variations they tried. The only thing at which these\nvariations differ is the self-attention mechanism.\n\nRecall that the self-attention operation in a Transformer takes a\nsequence as input $X$ and outputs a new sequence of the same length $Y$.\nEach entry of the output sequence $y_{i}$ is produced by computing a\nweighted average of entries of the input sequence\n$\\sum_{j}^{}w_{i,j}x_{j}$ where $w_{i,j}$ is the scalar weight produced\nby the self-attention mechanism as a function of $x_{i}$ and $x_{i}$.\nThe attention mask is then used to zero out certain weights\n$w_{i,j} = 0$ in order to constrain which entries of the input can be\nattended to at a given output timestep.\n\nThe different variations of masking in self-attention mechanism can be\nseen in the following figure:\n\n\n    \n\n\n\n  \n    Fully-visible:\nThis allows a self-attention mechanism to attend to any\nentry of the input when producing each entry of its output.\n  \n  \n    Causal:\nThis allows a self-attention mechanism to attend to only previous\ntokens. So, when producing the $i^{\\text{th}}$ entry of the output\nsequence, causal masking prevents the model from attending to the\n$j^{\\text{th}}$ entry of the input sequence for $j &amp;gt; i$. This\nmasking can be used as a language model (LM), i.e. a model trained\nsolely for next-step prediction\n  \n  \n    Causal with prefix:\nThis is a special case of the causal masking where a fully-visible\nmasking will be used during the prefix portion of the sequence and\ncausal masking for the rest.\n  \n\n\nUsing these three different masking techniques with the standard\nencoder-decoder transformer, we will get three different variations that\nwill be used:\n\n\n    \n\n\n\n  \n    Encoder-decoder (left):\nThis is the same as the baseline; the encoder has no masking\n(fully-visible) while the decoder has a “causal masking”.\n  \n  \n    Language Model (middle):\nThis architecture consists of a single Transformer layer stack and\nis fed the concatenation of the input and target using a causal mask\nthroughout.\n  \n  \n    Prefix LM (right):\nThis architecture is similar to language model with\nprefix-parameters (red rectangle) use fully-visible masking and the\nrest (green rectangle) use causal masking. This closely resembles\nBERT\n  \n\n\n\n  Note:\nThe Prefix LM is similar to an encoder-decoder model with parameters\nshared across the encoder and decoder and with the encoder-decoder\nattention replaced with full attention across the input and target\nsequence.\n\n\nPre-training\n\nIn this paper, pre-training is done by running the model for\n$2^{19} = 524,288$ steps on C4 before fine-tuning. A maximum sequence\nlength of $512$ is used and a batch size of $128$ sequences and they\npacked multiple sequences into one entry of the batch whenever\npossible. Roughly speaking, models in this paper are pre-trained on\n$2^{35} \\approx 34B$ tokens which is considerably less than BERT\n($137B\\ $tokens) or RoBERTa ($2.2T$ tokens).\n\n\n  Note:\nThat $2^{35}$ tokens only covers a fraction of the entire C4 dataset\nwhich means they never repeated any data during pre-training.\n\n\nAlso, they used an “inverse square root” learning rate schedule:\n$\\frac{1}{\\sqrt{\\max\\left( n,k \\right)}}$ where $n$ is the current\ntraining iteration and $k$ is the number of warm-up steps which was set\nto $10^{4}$. This sets a constant learning rate of $0.01$ for the first\n$10^{4}$ steps, then exponentially decays the learning rate until\npre-training is over.\n\nSince fine-tuning will be done on other languages other than English\nlike (German, French, and Romanian), they classified the pages in C4\nthat are either German, or French, or Romanian. Then, they trained the\nSentencePiece model on a mixture of 10 parts of English C4 data with 1\npart for each language forming a vocabulary of $32,000$ word-pieces.\n\n\n  Very Important Note:\nBased on this vocabulary setup, T5 models can only process a\npredetermined, fixed set of languages which are English, German, French,\nand Romanian.\n\n\nObjectives\n\nThe choice of unsupervised objective is of central importance as it\nprovides the mechanism through which the model gains general-purpose\nknowledge to apply to downstream tasks. All objectives mask one or\nmultiple tokens from the input to produce a (corrupted) input that the\nmodel will learn to predict the target sequence with the maximum\nlikelihood. The following table summarizes all pre-training objectives\ndiscussed in this paper and they are:\n\n\n    \n\n\n\n  \n    Prefix Language Modeling:\nThis technique splits a span of text into two components, one to use\nas inputs to the encoder and the other to use as a target sequence\nto be predicted by the decoder.\n  \n  \n    BERT-style:\nBERT-MLM objective takes a span of text and corrupts $15\\%$ of the\ntokens. BERT had only an encoder without a decoder. So, in this\nencoder-decoder setup, they adapted MLM from BERT by simply using\nthe entire uncorrupted sequence as the target.\n  \n  \n    De-shuffling:\nThis approach takes a sequence of tokens, shuffles it, and\nthen uses the original de-shuffled sequence as a target.\n  \n  \n    MASS-style:\nThis approach masks a consecutive sequence of tokens in the input\nand passes it to the encoder, then use the uncorrupted sequence as\nthe target. This looks like the Masked-Sequence objective discussed\nin MASS with a\nfew changes.\n  \n  \n    i.i.d. noise, replace spans:\nThis approach avoid predicting the whole uncorrupted text by\nmasking a few tokens in the encoder. Then, the target sequence\nbecomes the concatenation of the “corrupted” spans, each prefixed by\nthe mask token used to replace it in the input.\n  \n  \n    i.i.d. noise, drop tokens:\nIt’s the same as I.i.d. noise, replace spans but\nwith dropping the corrupted tokens from the input sequence, then use\nthese dropped tokens (in order) as the target.\n  \n\n\n\n  Notes:\n\n  \n    \n      $\\left\\langle M \\right\\rangle$ denotes a shared mask token (with\n  same ID) while $\\left\\langle X \\right\\rangle$,\n  $\\left\\langle Y \\right\\rangle$, and $\\left\\langle Z \\right\\rangle$\n  denote sentinel tokens that are assigned unique token IDs.\n    \n    \n      The greyed-out word “apple” in the previous table shows that this\n  token is a random token used as a replacement.\n    \n    \n      All objectives mentioned earlier except the prefix LM are called\n  “denoising” objectives, since they add a noise to the input and the\n  model has to de-noise it.\n    \n    \n      The replace spans approach later will be called “Span\n  Corruption”.\n    \n    \n      The “i.i.d” written before the objective name of the last two\n  objectives indicates that for each input token, a decision will be\n  made to either corrupt the token or leave it as it is.\n    \n  \n\n\nFine-Tuning\n\nAll models in this paper were fine-tuned for $2^{18} = 262,144$ steps on\nall tasks. This value was chosen as a trade-off between the\nhigh-resource tasks (i.e. those with large data sets), which benefit\nfrom additional fine-tuning, and low-resource tasks (smaller data sets),\nwhich overfit quickly. Like pre-training, they used batches with $128$\nlength-$512$ sequences. And unlike pre-training, they used a constant\nlearning rate of $0.001$.\n\nThey saved a checkpoint every $5,000$ steps and report results on the\ncheckpoint that got the highest validation performance. For models\nfine-tuned on multiple tasks, they chose the best checkpoint for each\ntask independently.\n\nBenchmarks\n\nThey used a diverse set of benchmark (all sourced from TensorFlow\ndatasets) that is able to measure\nthe general language learning; these datasets are:\n\n\n  \n    GLUE and SuperGLUE for text classification.\n  \n  \n    CNN/Daily Mail for abstractive summarization.\n  \n  \n    SQuAD for question answering.\n  \n  \n    WMT (English → German), (English → French), and (English → Romanian) for\nmachine translation.\n  \n\n\nResults\n\nIn this part, we are going to discuss all the experiments they tried in\nthe paper and what we can learn from them. The results tables are all\nformatted so that each row corresponds to a particular experimental\nconfiguration with columns giving the scores for each benchmark. The\nbaseline configuration is marked with ★. Any score that is within two\nstandard deviation of the best score in a given experiment will be\nbold-faced. Also, all results are reported on the validation set of\neach benchmark dataset.\n\n\n  Baseline (with/without pre-training):\nThe following table shows the average and standard deviation\nof the baseline model with and without pre-training for the same\nnumber of steps:\n\n\n\n    \n\n\n\n  Architectural Variants:\nThe following table shows the performance when trying different\narchitectural variants pre-trained on a certain objective and\nfine-tuned on the benchmark. To provide a reasonable means of\ncomparison, they referred to the number of layers in\nBERT~BASE~-sized layer stack as $L$ and the number of parameters as\n$P$ and the number of FLOPs (Floating-point Operations) required for\nan $L + L$-layer encoder-decoder model or $L$-layer decoder-only\nmodel as $M$.\n\n\n\n    \n\n\n\n  Objective Functions:\nThe following table shows the performance of the baseline model\nusing different objective functions; from the table we can see that\nall BERT-style variants (BERT-style + the last three) perform\nsimilarly.\n\n\n\n    \n\n\n\n  BERT-style Corruption Rate:\nAs you remember, BERT-style corruption rate masks 15% of the input\nsequence; 80% with the $\\left\\langle M \\right\\rangle$ token; 10%\nwith a random token and 10% with the original token. In the\nfollowing table, they tried different corruption rate. From the\ntable, we can see that the corruption rate had a limited effect on\nthe model’s performance. The only exception is (50%), it results in\na significant degradation of performance on GLUE and SQuAD.\n\n\n\n    \n\n\n\n  Span Length:\nSpan length is the number of consecutive tokens that will be masked\nwhen applying the denoising objective. Using a corruption rate of\n$15\\%$ in all cases, the following table compares average span\nlengths of 2, 3, 5 and 10. The baseline (i.i.d) means that for each\ninput token, we will have to make a decision whether to corrupt it\nor not. Again, this shows limited difference except with an average\nspan length of 10 which slightly under-performs the other values in\nsome cases.\n\n\n\n    \n\n\n\n  \n    Pre-training datasets:\nThe following table measures the effect of different C4 filtration\nmethods (first four entries) alongside with common pre-training\ndatasets (last two) on downstream tasks performance:\n\n    \n      \n        Unfiltered C4: ignoring all C4 filtration steps mentioned\nearlier except the one using the\nlangdetect tool to extract\nEnglish data.\n      \n      \n        RealNews-like: using standard filtration on C4 and to only\ninclude content from one of the domains used in the “RealNews”\ndataset.\n      \n      \n        WebText-like: using standard filtration on C4 and to only\nuse content originated from a URL that appeared in the list\nprepared by the\nOpenWebText. This\nwas relatively small (around 2GB). Therefore, they used 12\nmonths (August 2018 to July 2019) from CommonCrawl instead of\njust one months as the original C4.\n      \n      \n        Wikipedia: using the English Wikipedia text data from\nTensorFlow\nDatasets,\nwhich omits any markup or reference sections from the\narticles.\n      \n      \n        Wikipedia + Toronto Books Corpus: A drawback of using\npre-training data from Wikipedia is that it represents only\none possible domain of natural text (encyclopedia articles).\nTo mitigate this, they combined the Wikipedia data with the\nToronto Books Corpus (TBC). TBC contains text extracted from\neBooks, which represents a different domain of natural\nlanguage. This is the same pre-training data used with BERT.\n      \n    \n  \n\n\n\n    \n\n\n\n  Pre-training data size:\nThe following table measures the effect of limited unlabeled dataset\nsizes. These results were obtained by truncating the first $2^{29}$,\n$2^{27}$, $2^{25}$, and $2^{23}$ tokens of the C4 dataset. Knowing\nthat all models in this paper were trained using $2^{35}$ tokens,\nthese data sizes had to be repeated to match that. From the table,\nthe performance degrades as the data set size shrinks which is\ntotally expected:\n\n\n\n    \n\n\n\n  Alternative fine-tuning Methods:\nThe following table compares different alternative fine-tuning\nmethods that only update a subset of the model’s parameters. For\nadapter layers, $d$ refers to the inner dimensionality of the\nadapters. As we can see, lower-resource tasks like SQuAD work well\nwith a small value of $d$ whereas higher resource tasks require a\nlarge dimensionality to achieve reasonable performance.\n\n\n\n    \n\n\n\n  Note:\nThe past experiment suggests that adapter layers could be a\npromising technique for fine-tuning on fewer parameters as long as\nthe dimensionality is scaled appropriately to the task size.\n\n\nMulti-task Learning\n\nSo far, we have been pre-training our model on a single unsupervised\nlearning task before fine-tuning it individually on each downstream\ntask. An alternative approach, called “multitask learning”, is to train\nthe model on multiple tasks at once. In this paper, they relaxed that\ngoal somewhat and instead investigated methods for training on multiple\ntasks at once in order to eventually produce separate parameter settings\nthat perform well on each individual task which makes it comparable to\nthe pre-train-then-fine-tune approach.\n\nMulti-task learning is performed on the same datasets as the fine-tuning\nAn extremely important factor in multi-task learning is how much data\nfrom each task the model should be trained on. In this paper, they have\ntried three different mixing methods to make sure each task gets enough\ndata:\n\n\n  \n    Equal mixing:\nIn this case, each example in each batch is sampled uniformly at\nrandom from one of the datasets.\n  \n  \n    Example-proportional Mixing:\nIf the number of examples in each of the $N$ task’s data sets is\n$e_{n}$ where $n \\in \\left\\{ 1,\\ …N \\right\\}$, then the\nprobability of sampling an example from the $m^{\\text{th}}$ task\nduring training is\n$r_{m} = \\frac{\\min\\left( e_{n},\\ K \\right)}{\\sum_{i = 1}^{N}{\\min\\left( e_{i},\\ K \\right)}}$\nwhere $K$ is the artificial data set size limit.\n  \n  \n    Temperature-scaled mixing:\nTemperature up-samples the relatively low-resource tasks. This is\ndone by raising each task’s mixing rate $r_{m}$ to the power of\n$\\frac{1}{T}$.\n  \n\n\nTo compare these mixing strategies on equal footing with the\npre-train-then-fine-tune results, they trained multi-task models for\nthe same total number of steps: $2^{19} + 2^{18} = 786,432$. The\nresults are shown in the following table:\n\n\n    \n\n\nFrom the table, we can find out that multi-task training\nunder-performs pre-training followed by fine-tuning on most tasks.\nThe “equal” mixing strategy in particular results in dramatically\ndegraded performance.\n\nSince they relaxed the multi-task learning to make it look similar\nto the pre-train-then-fine-tune-approach, they decided to combine\nfine-tuning with multi-task learning once and combine multi-task\nlearning with pre-training another time. The following table shows\nthat the pre-train and then fine-tune technique is still better:\n\n\n    \n\n\nAll multi-task learning performed in the previous table were mixed\nusing examples-proportional method (with $K = 2^{19}$).\nLeave-one-out multi-task training was done by pre-training the model\non all tasks except one and then fine-tune it on the task that was\nleft out during pre-training.\n\nScaling\n\nIn deep learning, there is an argument that says that scaling up models\nproduces improved performance. However, there are a variety of possible\nways to scale (to make model bigger) like using more\nparameters, training the model for more steps,\nand ensembling. In the paper, they compared these\ndifferent approaches with the baseline model.\n\nTo increase the number of parameters, they experimented with the\nBERT~LARGE~ setup with $d_{\\text{ff}} = 4096$,\n$d_{\\text{model}} = 1024$, $d_{\\text{kv}} = 64$, and $16$-head attention\nmechanism and $16$-layers encoder and $16$-layers decoder. This setup\nproduces twice ($2 \\times$) the number of parameters as the baseline\nmodel. Using $32$-layers encoder and $32$-layers decoder will produce\nroughly four times ($4 \\times$) the number of parameters as the baseline\nmodel. Also, created an ensemble model by training the baseline 4\ndifferent times and averaging their results.\n\nThe performance achieved after applying these various scaling methods is\nshown in the following table which shows that increasing the model size\nresulted in an additional bump in performance compared to solely\nincreasing the training time or batch size:\n\n\n    \n\n\n\n  Important Notes:\n\n  \n    \n      Different scaling methods have different trade-offs that are\n  separate from their performance. For example, using a larger model\n  can make downstream fine-tuning and inference more expensive. In\n  contrast, the cost of pre-training a small model for longer is\n  effectively amortized if it is applied to many downstream tasks.\n    \n    \n      Ensembling $N$ separate models has a similar cost to using a model\n  that has an $N \\times$ higher computational cost.\n    \n  \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "BART",
        "url"       : "/language-modeling/BART",
        "date"      : "29/10/2019",
        "content": "BART stands for “Bidirectional Auto-regressive Transformer” which is a\npre-training scheme for models created by Facebook AI in 2019 and\npublished in this paper: “BART: Denoising Sequence-to-Sequence\nPre-training for Natural Language Generation, Translation, and\nComprehension”. Pre-training is\nthe process of training a model with one task that is able to help it\nform parameters that can be used to make other tasks easier. And this is\nwhat we, human beings, do. We use our old knowledge of what we have\nlearned in the past to understand new knowledge and handle a variety of\nnew tasks.\n\nBART was published in a paper under the name: “Denoising\nSequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension”. From, the name of the paper, we can\nconclude the following:\n\n\n  \n    The pre-training method in BART is denoising; denoising is when\nyou corrupt the input text with an arbitrary noising function, and\ntry to reconstruct the original text.\n  \n  \n    BART is a sequence-to-sequence model or encoder-decoder architecture.\n  \n  \n    BART is for any sequence-to-sequence data such as natural language\ngeneration, machine translation, and machine comprehension.\n  \n\n\nPut in simple words, BART is the same as the standard transformer that\nwe discussed with NMT with a few differences:\n\n\n  \n    BART uses Bi-directional encoder (same as BERT) and a\nuni-directional decoder from left to right (same as GPT).\n  \n  \n    BART uses GELU as an activation function instead of ReLU.\n  \n  \n    The parameters are initialized from a $\\mathcal{N}\\left( 0,\\ 0.02 \\right)$\ndistribution.\n  \n  \n    BART is pre-trained.\n  \n\n\n    \n\n\nPre-training\n\nThe encoder and the decoder are pre-trained in BART using different\ntechniques. The encoder is pre-trained by masking some of the input\ntokens and trying to predict these masked tokens (same as BERT). While\nthe decoder is trained by giving the preceding tokens and trying to\npredict the next token (same as GPT).\n\n\n    \n\n\nUnlike any other model, BART allows us to apply any type of document\ncorruption. In the paper, they experimented with several previously\nproposed and novel transformations which are summarized below:\n\n\n  \n    Token Masking: Following BERT random tokens are sampled and\nreplaced with [MASK] token.\n  \n  \n    Token Deletion: Random tokens are deleted from the input. In\ncontrast to token masking, the model must decide which positions\nare missing inputs.\n  \n  \n    Sentence Permutation: A document is divided into sentences based\non full stops, and these sentences are shuffled in a random order.\n  \n  \n    Document Rotation: A token is chosen uniformly at random, and\nthe document is rotated so that it begins with that token. This\ntask trains the model to identify the start of the document.\n  \n  \n    Text Infilling: Following SpanBERT, a number of text spans are\nsampled, with span lengths drawn from a Poisson distribution\n($\\lambda = 3$). Each span is replaced with a single [MASK]\ntoken. 0-length spans correspond to the insertion of [MASK]\ntokens. Text infilling teaches the model to predict how many\ntokens are missing from a span.\n  \n\n\nAnd the following table summarizes which noising function perform\nbest at which task. All models are of comparable size and are\ntrained for 1M steps on a combination of books and Wikipedia data.\nPerformance varies considerably across tasks, but the BART models\nwith text infilling demonstrate the most consistently strong\nperformance:\n\n\n    \n\n\n\n  Note:\nThese noising functions can be combined as seen in the last entry of\nthe above table.\n\n\nFine-tuning Tasks\n\nSame as BERT, The representations produced by BART can be used in\nseveral ways for downstream applications.\n\n\n  Text Classification:\nThe same input is fed into the encoder and decoder, and the\nfinal hidden state of the final decoder token is fed into new\nmulti-class linear classifier. This approach is related to the CLS\ntoken in BERT.\n\n\n\n    \n\n\n\n  \n    Sequence Generation:\nBecause BART has an autoregressive decoder, it can be directly\nfine tuned for sequence generation tasks such as abstractive\nquestion answering and summarization.\n  \n  \n    Machine Translation:\nWe replace BART’s encoder embedding layer with a new randomly\ninitialized encoder. The model is trained end-to-end, which trains\nthe new encoder to map foreign words into an input that BART can\nde-noise to English. The encoder part is trained in two steps, in\nboth cases backpropagating the cross-entropy loss from the output\nof the BART model:\n\n    \n      \n        In the first step, we freeze BART parameters and only update the\nrandomly initialized source encoder, the BART positional\nembeddings, and the self-attention input projection matrix of\nBART’s encoder first layer.\n      \n      \n        In the second step, we train all model parameters for a small\nnumber of iterations.\n      \n    \n  \n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "DistilBERT",
        "url"       : "/language-modeling/DistilBERT",
        "date"      : "01/03/2020",
        "content": "DistilBERT is a smaller, faster, cheaper and lighter version of BERT\ncreated by Hugging Face in March 2020 and published in this paper:\n“DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\nlighter”. In this paper, they\nused knowledge distillation to reduce the size of a BERT by 40%, while\nretaining 97% of its language understanding capabilities and being 60%\nfaster. This was possible by using a triple loss function that combines\nlanguage modeling, distillation and cosine-distance losses.\n\nThe following figure shows a simple comparison between DistilBERT and\nother models with respect to the number of parameters; which shows that\nDistilBERT is small enough to run on the edge, e.g. on mobile devices.\n\n\n    \n\n\nKnowledge Distillation\n\nKnowledge distillation is a compression technique in which a compact\nmodel (the student) is trained to reproduce the behaviour of a larger\nmodel (the teacher). The teacher is trained on the hard labels (labels\nthat belongs to one class) while the student is trained on soft labels\n(the class probabilities) which resulted from the teacher. In other\nwords, the teacher is trained on the true labels from the training data\nand results probabilities that will be used to teach the student.\n\n\n    \n\n\nTriple Loss\n\nIn this paper, the student (DistilBERT) is trained with a triple loss\nwhich is a linear combination of three different losses which are:\n\n\n  $\\mathcal{L}_{\\text{ce}}$:\na distillation loss over the soft target probabilities of the teacher\n(BERT) where $t_{i}$ is the probability estimated by the teacher and\n$s_{i}$ is the probability estimated by the student.\n\n\n\\[\\mathcal{L}_{\\text{ce}} = \\sum_{i}^{}{t_{i}\\text{.log}\\left( s_{i} \\right)}\\]\n\n   Calculating the probability ($t_{i}$ and $s_{i}$) is done using the\nsoftmax-temperature function where T is the temperature that controls\nthe smoothness of the output distribution knowing that the same\ntemperature will be used for the student and the teacher:\n\n\\[p_{i} = \\frac{\\exp\\left( \\frac{z_{i}}{T} \\right)}{\\sum_{j}^{}{\\exp\\left( \\frac{z_{j}}{T} \\right)}}\\]\n\n\n  $\\mathcal{L}_{\\text{MLM}}$:\na masked language model loss; the same as BERT where ${\\widehat{x}}{i}$ is\nthe predicted word and $x{i}$ is the true word.\n\n\n\\[\\mathcal{L}_{\\text{MLM}}\\left( x_{i} \\right) = - \\log\\left( {\\widehat{x}}_{i} \\middle| x_{i} \\right)\\]\n\n\n  $\\mathcal{L}_{\\cos}$:\na cosine embedding loss which will tend to align the directions of the\nstudent and teacher hidden states vectors.\n\n\nArchitecture\n\nThe student (DistilBERT) has the same general architecture as BERT with\nthese changes:\n\n\n  \n    The token-type embeddings and the pooler are removed.\n  \n  \n    The number of layers is reduced by a factor of 2.\n  \n  \n    Most of the operations used in the Transformer architecture (linear\nlayer and layer normalisation) are highly optimized in modern\nlinear algebra frameworks.\n  \n  \n    Taking advantage of the common dimensionality between teacher and\nstudent networks, they initialized the student from the teacher by\ntaking one layer out of two.\n  \n  \n    Following best practices, they used a very large batches (up to 4K\nexamples per batch) using dynamic masking and without the next\nsentence prediction (NSP) objective.\n  \n\n\nAnd to show how efficient that was, let’s look on a simple\ncomparison between DistilBERT and BERT on GLUE:\n\n\n    \n\n\nAs we can see, DistilBERT retains 97% of BERT’s performance while being\n40% smaller and 60% faster as shown in the following table:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "ELECTRA",
        "url"       : "/language-modeling/ELECTRA",
        "date"      : "23/03/2020",
        "content": "ELECTRA stands for “Efficiently Learning an Encoder that Classifies\nToken Replacements Accurately” which is a discriminator language\nmodel unlike the widely-used generative language models such as\nBERT, GPT, ...etc. ELECTRA was proposed by Stanford University in\ncollaboration with Google Brain in 2020 and published in their paper:\nELECTRA: Pre-training text Encoders.\nThe official code of this paper can be found on Google Research’s official\nGitHub repository:\ngoogle-research/electra.\n\nGenerative language models such as BERT was pre-trained using MLM\nobjective where some tokens of the input sentence get masked, then the\nmodel is trained to reconstruct them back. These generative models\nproduce good results but they require large amounts of data and\ncomputation power to be effective.\n\nThis paper suggests an alternative pre-training task called “Replaced\nToken Detection” (RTD for short) that won’t require that much data or\ncomputation power. According to the following table, the “Replaced Token\nDetection” method outperforms MLM pre-training given the same model size, data,\nand compute budget:\n\n\n    \n\n\nReplaced Token Detection (RTD)\n\nReplaced Token Detection is a pre-training task that was introduced in\nthis paper as a replacement for MLM. Instead of masking, this method\ncorrupts the input by replacing some tokens with plausible alternatives\nsampled from a small generator network. And the discriminator (ELECTRA)\nlearns to distinguish between the original tokens and the synthetically\ngenerated ones as shown below:\n\n\n    \n\n\nAs seen in the above figure, this task trains two neural networks; each\nconsists of a transformer-encoder architecture that maps a sequence of\ninput tokens $X = \\left\\lbrack x_{1},\\ …\\ x_{n} \\right\\rbrack$ into a\nsequence of vectors\n$h\\left( X \\right) = \\left\\lbrack h_{1},\\ …\\ h_{n} \\right\\rbrack$.\n\n\n  Generator $G$:\nGiven an input $X = \\left\\lbrack x_{1},\\ …\\ x_{n} \\right\\rbrack$,\nMLM first selects a random set of positions (integers from 1 to n)\nwith a probability of $15\\%$ to mask out tokens $X^{\\text{masked}}$.\nThen, the generator learns to predict the original tokens of the\nmasked-out ones. For a given position $t$ where\n$x_{t} = \\left\\lbrack \\text{MASK} \\right\\rbrack$, the generator\noutputs a probability for generating a particular token $x_{t}$ with\na softmax layer where $e\\left( x \\right)$ is the embedding of token\n$x$:\n\n\n\\[p_{G}\\left( x_{t} \\middle| X \\right) = \\text{softmax}\\left( {e\\left( x_{t} \\right)}^{T}.h_{G}\\left( X \\right)_{t} \\right)\\]\n\n\n  Discriminator (ELECTRA) $D$:\nFor a given position $t$, the discriminator predicts whether the\ntoken $x_{t}$ is “real” or generated $X^{\\text{corrupt}}$ using a\nsigmoid layer:\n\n\n\\[D\\left( X,t \\right) = \\text{sigmoid}\\left( w^{T}.h_{D}\\left( X \\right)_{t} \\right)\\]\n\nAnd when training the model on the Replaced Token Detection task, the\nmodel tries to minimize the following loss function over a large corpus\n$\\mathcal{X}$ of raw text:\n\n\\[\\mathcal{L} = \\min_{\\theta_{G},\\ \\theta_{D}}\\left( \\sum_{X \\in \\mathcal{X}}^{}{\\mathcal{L}_{\\text{MLM}}\\left( X,\\ \\theta_{G} \\right) + \\lambda\\mathcal{L}_{\\text{Disc}}\\left( X,\\ \\theta_{D} \\right)} \\right)\\]\n\n\\[\\mathcal{L}_{\\text{MLM}}\\left( X,\\ \\theta_{G} \\right) = \\mathbb{E}\\left( \\sum_{i \\in m}^{}{- log\\ p_{G}\\left( x_{i} \\middle| X^{\\text{masked}} \\right)} \\right)\\]\n\n\\[\\mathcal{L}_{\\text{Disc}}\\left( X,\\ \\theta_{D} \\right) = \\mathbb{E}\\left( \\sum_{t = 1}^{n}{- \\mathbb{l}\\left( x_{t}^{\\text{corrupt}} = x_{t} \\right)\\text{.log}\\left( D\\left( X^{\\text{corrupt}},\\ t \\right) \\right) - \\mathbb{l}\\left( x_{t}^{\\text{corrupt}} \\neq x_{t} \\right)\\text{.log}\\left( 1 - D\\left( X^{\\text{corrupt}},\\ t \\right) \\right)} \\right)\\]\n\n\n  Note:\nThis task looks like a GAN (General Adversarial Network) but it is not\nadversarial since the generator doesn’t try to fool the discriminator.\nIts job is to generate replace the masked tokens with the exact tokens\nthat were masked.\n\n\nExperiments\n\nFor most of the experiments, ELECTRA was pre-trained on the same data as\nBERT, which\nconsists of 3.3 Billion tokens from Wikipedia and BooksCorpus. However,\nLarge models were pre-trained on the data used for\nXLNet, which\nextends the BERT dataset to 33B tokens by including data from ClueWeb,\nCommonCrawl, and Gigaword. For fine-tuning on GLUE, a simple linear\nclassifiers is added on top of ELECTRA.\n\nThe following table compares various small models on the GLUE dev set.\nBERT-Small/Base use the same hyper-parameters as ELECTRA-Small/Base. The\ntable shows that ELECTRA performs better than BERT scoring 5 GLUE points\nhigher.\n\n\n    \n\n\n\n  Note:\nIn the original\nBERT paper, there\nwere no BERT~Small~. They created BERT~Small~ using a smaller\nhyper-parameters of BERT~Base~; they reduced the sequence length (from\n512 to 128), with smaller batch size (from 256 to 128), smaller hidden\ndimension size (from 768 to 256), and smaller token embeddings (from 768\nto 128). To provide a fair comparison, they created BERT-Small model\nusing the same hyper-parameters and trained it 1.5M steps, so it uses\nthe same training FLOPs as ELECTRA-Small, which was trained for 1M\nsteps.\n\n\nThe following table compares ELECTRA-Large with BERT~Large~ on the GLUE\ndev set. ELECTRA-Large was trained for longer steps; ELECTRA-400k was\ntrained for 400k steps which is 25% of RoBERTa training time and\nELECTRA-1.74M was trained for 1.74M steps which is similar to RoBERTa\ntraining time. The table shows that ELCETRA-400k performs comparably to\nRoBERTa and XLNet and ELECTRA-1.75M outperforms the other models on\nvarious tasks.\n\n\n    \n\n\nSame applies for SQuAD benchmark, ELECTRA-1.75M scores better than all\nmasked-language modelings given the same compute resources:\n\n\n    \n\n\n\n  Note:\nThe following is the complete list of all hyper-parameters used for\nELECTRA on pre-training (left) and fine-tuning (right):\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Longformer: Long Transformer",
        "url"       : "/language-modeling/Longformer",
        "date"      : "10/04/2020",
        "content": "Transformer-based models are unable to process long sequences due to\ntheir self-attention operation, which has a time complexity of\n$O\\left( n^{2} \\right)$ where $n$ is the input length. Longformer stands\nfor “Long Transformer” which is a encoder-side transformer with a novel\nattention mechanism that scales linearly with sequence length making it\neasy to process documents of thousands of tokens or longer. Longformer\nwas proposed by Allen Institute in 2020 and published in their paper:\nLongformer: The Long-Document\nTransformer. The official code\nfor this paper can be found in the official GitHub page of Allen\nInstitute: allenai/longformer.\n\nAttention Patterns\n\nLongformer sparsifies the full-attention mechanism matrix according to\nan “attention pattern” specifying pairs of input locations attending to\none another. According to the paper, there are four different patterns\nconsidered as shown below:\n\n\n  Sliding Window:\nThis is a fixed-window attention surrounding each token. Using\nmultiple stacked layers of such windowed attention results in a\nlarge receptive field. Given a fixed window size $w$, each token\nattends to $\\frac{1}{2}w$ tokens on each side. The computation\ncomplexity of this pattern is $O\\left( n \\times w \\right)$. In a\ntransformer with $l$ layers, the receptive field size at the top\nlayer is $l \\times w$ (assuming $w$ is fixed for all layers).\n\n\n\n    \n\n\n\n  Dilated Sliding Window:\nTo further increase the receptive field without increasing\ncomputation, the sliding window can be “dilated” where the window\nhas gaps of size dilation $d$. Assuming a fixed $d$ and $w$ for all\nlayers, the receptive field is $l \\times d \\times w$, which can\nreach tens of thousands of tokens even for small values of $d$.\n\n\n\n    \n\n\n\n  Global Attention:\nThe windowed and dilated attention attend to subset of the\nsequence. That’s why they decided to add “global attention” on few\npre-selected input tokens. This figure shows an example of a sliding\nwindow attention with global attention at a few tokens at custom\nlocations.\n\n\n\n    \n\n\n\n  Note:\n\n  \n    \n      In multi-headed setup of dialted window attention, using different\n  dilation configurations per head improves performance by allowing\n  some heads without dilation to focus on local context, while others\n  with dilation focus on longer context.\n    \n    \n      Implementing Longformer’s dilated sliding widow attention requires a\n  form of banded matrix multiplication (matrix multiplication where\n  the output is all zero except certain diagonals) that is not\n  directly supported in existing deep learning libraries like\n  PyTorch/Tensorflow. So, they tried to implement it in three\n  different ways:\n\n      \n        \n          Longformer-loop: Naive implementation using for-loops in PyTorch.\n        \n        \n          Longformer-chunks: Chunks $Q$ and $K$ into overlapping blocks of\n  size $w$ and overlap of size $\\frac{1}{2}w$, multiplies the blocks,\n  then mask out the diagonals.\n        \n        \n          Longformer-cuda: is a custom CUDA kernel that they implemented\n  using TVM (Tensor Virtual Machine).\n        \n      \n\n      They compared these different implementations according to time and memory\nand found out that longformer-chunks is the fastest.\n\n      \n    \n\n    \n    \n      They used small window sizes for the lower layers and increase window\n  sizes as they moved to higher layers.\n    \n    \n      Also, they didn’t use dilated sliding windows for lower layers to\n  maximize their capacity to learn and utilize the immediate local\n  context. For the higher layers, they used a small amount of\n  increasing dilation only on $2$ heads. This gives the model the\n  ability to directly attend to distant tokens without sacrificing\n  local context.\n    \n  \n\n\nCharacter-level LM\n\nTraining Longformer is done over 5 phases where they started with a\nshort sequence length of 2,048 and small window size, then the attention\nwindow size and sequence length across multiple is doubled on each\nsubsequent phase while halving the learning rate till they ended with a\nlength of 23,040 on the last phase.\n\nLongformer implementation is based on the\nTransformer-XL\nfound here with the\nmemory mechanism disabled. They used relative position embeddings with\nsinusoidal weights. They used two different model sizes; each with a\ndifferent set of hyper-parameters:\n\n\n    \n\n\nTo compare longformer with previous character-level language modeling,\nthey trained it on text8 &amp;amp; enwik8 benchmark, both contain 100M\ncharacters from Wikipedia split into 90M, 5M, 5M for train, dev, test\nrespectively. Longformer outperforms all other models and achieves\nstate-of-the-art results on both datasets:\n\n\n    \n\n\nPre-training &amp;amp; Fine-tuning\n\nLongformer was pre-trained using masked language modeling (MLM), where\nthe goal is to recover randomly masked tokens in a sequence. Since MLM\npre-training is expensive, they continued pre-training from the\nRoBERTa released\ncheckpoint, and only making the minimal changes necessary to support\nLongformer’s attention mechanism. And since RoBERTa’s input is limited\nto $512$, they decided to copy them till it matches the input to\nlongformer. After that, longformer was pre-trained on the following\ndata:\n\n\n    \n\n\nVery Important Note:\nLongformer’s attention pattern can be plugged into any pre-trained\ntransformer model without the need to change the model architecture.\n\nAfter pre-training, longformer was fine-tuned on on six tasks resulting\nin a model that can process sequences up to 4,096 tokens long (8 times\nlonger than BERT). These six tasks are QA (WikiHop, TriviaQA, HotpotQA),\nCoreference Resolution () and document classification (IMDB,\nHyperpartisan):\n\n\n    \n\n\nLED\n\nLED stands for “Longformer Encoder Decoder” which is a variant of the\nLongformer model that follows an encoder-decoder architecture similar to\nthe original\nTransformer\nmodel; instead of an encoder-only Transformer architecture as the\nLongformer model. LED is intended for long sequence-to-sequence tasks\nsuch as text summarization.\n\nSince pre-training LED is expensive, they initialized LED parameters\nfrom the BART\nfollowing BART’s exact architecture in terms of number of layers and\nhidden sizes. The only difference is that they extend position embedding\nto 16K tokens (BART has only 1K tokens). Also, they initialized the new\nposition embedding matrix by repeatedly copying BART’s 1K position\nembeddings 16 times.\n\nFollowing BART, they released two model sizes, LED-base and LED-large,\nwhich respectively have 6 and 12 layers in both encoder and decoder\nstacks. LED was evaluated on the summarization task using the arXiv\nsummarization dataset which focuses on long document summarization in\nthe scientific domain. The following table shows that LED-large achieves\nstate-of-the-art results, slightly outperforming\nBigBird.\n\n\n\n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "ETC: Extended Transformer Construction",
        "url"       : "/language-modeling/ETC",
        "date"      : "17/04/2020",
        "content": "ETC stands for “Extended Transformer Construction” which is a new\nTransformer architecture for language modeling over long sentences and\nachieves state-of-the-art performance on various long-sentence tasks as\nshown in the following table. ETC was proposed by Google in 2020 and\npublished in this paper: “ETC: Encoding Long and Structured Inputs in\nTransformers”. The official code\nfor this paper can be found on Google Research’s official GitHub\nrepository: research-etc-model\n.\n\n\n    \n\n\nMany variants of the original\nTransformer\nmodel have been proposed for language modeling such as\nBERT,\nRoBERTa,\nALBERT, or even\nT5 limit inputs to\n$n = 512$ tokens due to the $O\\left( n^{2} \\right)$ cost of attention.\nETC scales to longer input sentences up to $n = 8192$ tokens or more.\nETC follows the encoder-side of the original Transformer architecture\nwith three key modifications to tackle long inputs:\n\n\n  \n    Relative Position Encoding\n  \n  \n    Global-local Attention.\n  \n  \n    CPC pre-training task.\n  \n\n\nRelative Position Encoding\n\nInspired by the work of this paper: Self-attention with relative\nposition representations, ETC\nreplaces absolute position encodings with relative position encodings.\nThere will be $2k + 1$ relative positions given a maximum clipping\ndistance $k$, no matter the length of the input sentence. Given the\ninput sequence $x = \\left\\{ x_{1},\\ …\\ x_{n} \\right\\}$, the relative\nposition between two tokens $x_{i}$ and $x_{j}$ will be:\n\n\\[\\left\\{ \\begin{matrix}\nl_{j - i}\\ \\ \\ \\ \\  - k &amp;lt; j - i &amp;lt; k \\\\\nl_{- k}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j - i \\leq - k \\\\\nl_{k}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j - i \\geq - k \\\\\n\\end{matrix} \\right.\\]\n\nEach relative position then becomes a learnable vector $a_{l}^{K}$,\nwhich modifies the attention mechanism as we are going to see later.\nNow, we can clearly see that these relative position encodings are\nindependent of input length, so it is easy to adapt a model to greater\ninput lengths.\n\nGlobal-local Attention\n\nGiven the input sequence $x = \\left\\{ x_{1},\\ …\\ x_{n} \\right\\}$, ETC\nmodel will split it into two separate sequences:\n\n\n  The long input $x^{l}$: which contains the input that standard\nTransformer would get.\n\n\n\\[x^{l} = \\left\\{ x_{1}^{l},\\ ...x_{n_{l}}^{l} \\right\\}\\]\n\n\n  The global input $x^{g}$: which is a much smaller number of\nauxiliary tokens $\\left( n_{g} \\ll n_{l} \\right)$:\n\n\n\\[x^{g} = \\left\\{ x_{1}^{g},\\ ...x_{n_{g}}^{g} \\right\\}\\]\n\nThen, attention will be split into four separate pieces:\nglobal-to-global (g2g),\nglobal-to-long (g2l),\nlong-to-global (l2g),\nand long-to-long (l2l).\nAttention in the l2l piece is restricted to a fixed radius as we can see\nin the following figure (part c); because it is the most computationally\nexpensive part of the whole attention mechanism.\n\n\n    \n\n\nAttention in ETC is\n$O\\left( n_{g}\\left( n_{g} + n_{l} \\right) + n_{l}\\left( n_{g} + 2r + 1 \\right) \\right)$.\nSince $n_{l} \\gg n_{g},\\ r$, then the attention mechanism becomes\n$O\\left( n_{g}^{2} + n_{g}n_{l} \\right)$. As we can see, the attention\nmechanism is linear in the size of the long input . The attention\nmechanism is applied via the following steps:\n\n\n  \n    Per-instance, four Boolean attention matrices\n$M^{g2g} \\in \\mathbb{R}^{n_{g} \\times n_{g}}$,\n$M^{g2l} \\in \\mathbb{R}^{n_{g} \\times n_{l}}$,\n$M^{l2g} \\in \\mathbb{R}^{n_{l} \\times n_{g}}$, and\n$M^{l2l} \\in \\mathbb{R}^{n_{l} \\times n_{l}}$are created with zeroes\nfor those pairs of tokens that should not attend to one another and\nones for the rest.\n  \n  \n    Given the global input $x^{g}$ which is a sequence of token\nrepresentations $x_{i}^{g} \\in \\mathbb{R}^{d_{x}}$, a large constant\n$C = 10,000$ (in the paper), $a_{\\text{ij}}^{K}$ learnable vectors\nrepresenting the relative positions, and $W^{Q},\\ W^{K}$ learnable\nweight matrices, the attention embedding between $x_{i}$ and $x_{j}$\nin the global-to-global part can be calculated as:\n  \n\n\n\\[e_{\\text{ij}}^{g2g} = \\frac{x_{i}^{g}W^{Q}\\left( x_{j}^{g}W^{K} + a_{\\text{ij}}^{K} \\right)^{T}}{\\sqrt{d_{z}}} - \\left( 1 - M_{\\text{ij}}^{g2g} \\right)C\\]\n\n\n  A softmax is used to calculate $\\alpha_{\\text{ij}}^{g2g}$:\n\n\n\\[\\alpha_{\\text{ij}}^{g2g} = \\frac{\\exp\\left( e_{\\text{ij}}^{g2g} \\right)}{\\sum_{l = 1}^{n}{\\exp\\left( e_{\\text{il}}^{g2g} \\right)}}\\]\n\n\n  The attention output of the global-to-global part is\n$z^{g} = \\left\\{ z_{1}^{g},\\ …z_{n_{g}}^{g} \\right\\}$ where\n$z_{i}^{g} \\in \\mathbb{R}^{d_{z}}$ which uses $W^{V}$ as a learnable\nweight matrix is calculated as follows:\n\n\n\\[z_{i}^{g} = \\sum_{j = 1}^{n_{g}}{\\alpha_{\\text{ij}}^{g2g}.x_{j}^{g}.W^{V}}\\]\n\n\n  \n    Attention for the other 3 pieces is analogous to this one.\n  \n  \n    At the end, a single softmax is used to jointly calculate\n$\\alpha_{\\text{ij}}^{g2g}$ and $\\alpha_{\\text{ij}}^{g2l}$ while\nanother one for $\\alpha_{\\text{ij}}^{l2g}$ and\n$\\alpha_{\\text{ij}}^{l2l}$. I DON’T KNOW HOW :(\n  \n  \n    Thus, the output of global-local attention is a sequence of length\n$n_{g}$ and one of length $n_{l}$. These sequences go through a\nlayer normalization and feed forward layer in the same way as in the\nstandard transformer.\n  \n\n\nCPC\n\nETC model uses two pre-training tasks. The first one is Masked Language\nModeling (MLM) with whole word masking which means if one word piece\ntoken is masked, then all other tokens of the same word are masked. The\nsecond one is Contrastive Predictive Coding (CPC). The goal of CPC is to\npredict subsequent inputs in latent space, i.e., to predict internal\nhidden representations of blocks of tokens. We adapted this idea in ETC\nby using global input sentence summary tokens.\n\nGiven an input sequence containing $n$ sentences, we mask all the tokens\ncorresponding to a subset of sentences (but leave the sentence summary\ntokens in the global input). Then, the model is trained to minimize the\ndifference between the hidden representation of the global sentence\nsummary tokens for the masked sentences with respect to that of a global\nsummary token that can see the unmasked sentence and nothing else. A\nNoise Contrastive Estimation (NCE) loss is used:\n\nExperiments\n\nETC model was initialized using RoBERTa parameters. This is doable since\nBERT’s attention is a special case of the global-local attention used in\nETC. Same as RoBERTa, they created two basic configurations:\n\n\n\n    \n        \n            \n            Layers\n            Hidden Size\n            Attention Heads\n            # Parameters\n            $$r$$\n            $$k$$\n        \n    \n    \n        Base\n        12\n        768\n        12\n        166 M\n        84\n        12\n    \n    \n        Large\n        24\n        1024\n        16\n        558 M\n        169\n        24\n    \n\n\n\nFor pre-training; they used original BERT datasets, except that\ndocuments with fewer than 7 sentences were filtered out with WordPiece\nvocabulary of 30k uncased wordpieces . ETC-base was pre-trained with the\nsame total number of tokens as the original BERT, while ETC-large was\npre-trained with twice as many. As an optimizer, they used LAMB\noptimizer with learning rate set to $\\sqrt{8} \\times 10^{- 3}$.\n\nThe following are the results obtained by ETC in various benchmarks; the\ndefaulted configuration is marked with “-“:\n\n\n  Natural Questions (NQ):\n\n\n\n    \n\n\n\n  OpenKP:\n\n\n\n    \n\n\n\n  HotpotQA and WikiHop:\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Adapter Fusion",
        "url"       : "/language-modeling/adapter_fusion",
        "date"      : "01/05/2020",
        "content": "AdapterFusion is a new variant of the Adapter layers\n where it extends\nthe functionality of adapters to be multi-tasking instead of being per a\nsingle task. AdapterFusion is proposed by researchers in UKP Lab,\nTechnical University of Darmstadt and New York University and\npublished in their paper: AdapterFusion: Non-Destructive Task\nComposition for Transfer Learning\nin May 2020.\n\nAdapterFusion is a novel two stage learning algorithm that shares knowledge\nacross multiple tasks while avoiding catastrophic forgetting. The AdapterFusion\narchitecture, illustrated in the following figure, has two components:\n\n\n  \n    Adapter: trained on a task\n  \n  \n    AdapterFusion layer: combines the representations from several\nadapters in order to improve the performance on the target task.\n  \n\n\n\n    \n\n\nTask Definition\n\nGiven a single-task model that is pre-trained on a task with training\ndata $D_{0}$ and a loss function $L_{0}$, the weights $\\Theta_{0}$ of\nthis model are learned as follows:\n\n\\[\\Theta_{0} = \\underset{\\Theta}{\\arg\\min}{L_{0}\\left( D_{0};\\Theta \\right)}\\]\n\nGiven a multi-task model that is pre-trained on a set of $N$ tasks\nhaving labeled data of varying sizes and different loss functions\n$C = \\left\\{ \\left( D_{1},L_{1} \\right),\\ …\\left( D_{N},L_{N} \\right) \\right\\}$,\nthe aim of the model is to leverage the set of $N$ tasks and learn a\nshared representation $\\Theta_{0 \\rightarrow \\left\\{ 1,\\ …N \\right\\}}$\nthat will enable the model to generalize better on each task; this is\nusually obtained by starting with an initial parameters $\\Theta_{0}$ and\nfine-tune on tasks $\\left\\{ 1,\\ …N \\right\\}$:\n\n\\[\\Theta_{0 \\rightarrow \\left\\{ 1,\\ ...N \\right\\}} = \\underset{\\Theta}{\\arg\\min}\\left( \\sum_{n = 1}^{N}{L_{n}\\left( D_{n};\\Theta_{0} \\right)} \\right)\\]\n\nIn AdapterFusion, the aim is to be able to leverage a set of $N$ tasks\nto improve on a target task $m$ with\n$C_{m} = \\left( D_{m},L_{m} \\right)$ where\n$m \\in \\left\\{ 1,\\ …N \\right\\}$. This is done in two stags:\n\n\n  Knowledge Extraction:\nWe train different adapters for each of the N tasks obtaining:\n\n\n\\[\\left\\{ \\Phi_{1},\\ ...,\\ \\Phi_{N} \\right\\}\\]\n\n\n  Knowledge Composition:\nWe combine the set of $N$ adapters using AdapterFusion Layer while\nfixing both the model parameters $\\Theta$ as well as all adapters\n$\\Phi$ obtaining parameters $\\Psi$ that learn to combine the $N$\ntask adapters to solve the target task\n$C_{m} = \\left( D_{m},L_{m} \\right)$:\n\n\n\\[\\Psi_{m} = \\underset{\\Psi}{\\arg\\min}{L_{m}\\left( D_{m};\\Theta,\\ \\Phi_{1},\\ ...\\Phi_{N},\\ \\Psi \\right)}\\]\n\n\n  Note:\nThe training dataset of the target task $m$ is used twice: once for\ntraining the adapters $\\Phi_{m}$ and again for training Fusion\nparameters $\\Psi_{m}$ which learns to compose the information stored in\nthe $N$ task adapters.\n\n\nAdapterFusion Layer\n\nAs discussed earlier, AdapterFusion learns to compose the $N$ task adapters\n$\\left\\{ \\Phi_{1},\\ …,\\ \\Phi_{N} \\right\\}$ and the shared pre-trained\nmodel $\\Theta$, by introducing a new set of weights $\\Psi$. As\nillustrated in the following figure, they defined the AdapterFusion\nparameters $\\Psi$ to consist of Key, Value and Query matrices at each\nlayer $l$, denoted by $K_{l}$ , $V_{l}$ and $Q_{l}$ respectively.\n\n\n    \n\n\nAt each layer $l$ of the transformer and each time-step $t$, the output\nof the feed-forward sub-layer of layer $l$ is taken as the query vector.\nThe output of each adapter $z_{l,t}$ is used as input to both the value\nand key transformations. Similar to the attention mechanism, we learn a\ncontextual activation of each adapter $n$ using the following formula\nwhere $n \\in \\left\\{ 1,\\ …N \\right\\}$ , $\\bigotimes$ represents dot\nproduct, $\\left\\lbrack .,. \\right\\rbrack$ indicates the concatenation of\nvectors, and $z^{T}$ is the transpose of $z$:\n\n\\[s_{l,t} = \\text{softmax}\\left( h_{l,t}^{T}Q_{l}\\ \\bigotimes\\ z_{l,t,n}^{T}K_{l} \\right)\\]\n\n\\[{z&#39;}_{l,t,n} = z_{l,t,n}^{T}V_{l}\\]\n\n\\[{Z&#39;}_{l,t} = \\left\\lbrack {z&#39;}_{l,t,0},\\ ...{z&#39;}_{l,t,N} \\right\\rbrack\\]\n\n\\[o_{l,t} = s_{l,t}^{T}{Z&#39;}_{l,t}\\]\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "GPT-3",
        "url"       : "/language-modeling/GPT-3",
        "date"      : "28/05/2020",
        "content": "GPT-3 is an enormous model built on the transformer-decoder architecture\npublished in 2020 by OpenAI in this paper: “Language Models are\nFew-Shot Learners” whose title is\nvery indicative of what the paper wanted to show. The paper didn’t\nprovide any new architecture, they used the same architecture as GPT-2.\nThey just made it way bigger and trained over more data.\n\nThe whole purpose of this paper is to show that GPT-3 can be used with a\nvariety of tasks using either zero-shot, or one-shot or a few-shots\nlearning schemes and even reaching competitiveness with prior\nstate-of-the-art fine-tuned models. Before getting into more details\nabout the model, let’s first discuss what do I mean by these learning\nschemes and how they are different from fine-tuning:\n\n\n  Few-shot (FS):\nIt’s the setting where the model is given K (usually from 10\nto 100) examples of the task at inference time as conditioning,\nbut no weight updates are allowed. As we can see in the following\nfigure, GPT-3 was given three different examples along with\nthe task description:\n\n\n\n    \n\n\n\n  One-shot (1S):\nIt’s the same as few-shot except that only one\ndemonstration is allowed, in addition to the task description. The\nreason to distinguish one-shot from few-shot is that it most\nclosely matches the way in which some tasks are communicated to\nhumans:\n\n\n\n    \n\n\n\n  Zero-shot (0S):\nIt’s the same as one-shot except that no demonstrations are\nallowed, just the task description. This method provides maximum\npotential for robustness but is also the most challenging setting\neven for humans.\n\n\n\n    \n\n\n\n  Fine-Tuning (FT):\nIt has been the most common approach in recent years, and involves\nupdating the weights of a pre-trained model by training on a\nsupervised dataset specific to the desired task. This setting\nlacks from poor generalization out-of-distribution:\n\n\n\n    \n\n\nModel\n\nAs said earlier, they used the same model and architecture as GPT-2. To\nstudy the dependence of performance on model size, they trained 8\ndifferent sizes of model as shown in the following table\n\n\n    \n\n\nWhere:\n\n\n  \n    $n_{\\text{params}}$: is the total number of trainable parameters.\n  \n  \n    $n_{\\text{layers}}$: is the total number of layers.\n  \n  \n    $d_{\\text{model}}$: is the number of units in each bottleneck layer\n(we always have the feed-forward layer four times the size of the\nbottleneck layer,\n$d_{\\text{feedforward}} = 4 \\times d_{\\text{model}}$).\n  \n  \n    $n_{\\text{heads}}$: is the number of attention heads/layers, since\neach layer has just one attention head.\n  \n  \n    $d_{head}$: is the dimension of each attention head.\n  \n\n\nAs you can see, GPT3 is massive as its context-widow\n$n_{\\text{ctx}} = 2048$ tokens wide with about 175 billion learnable\nparameters spread over 96 transformer-decoder layers.\n\n\n    \n\n\nThe data used for this models are according to the following table\n\n\n    \n\n\nAnd the following is a comparison between the training time taken to\ntrain BERT, RoBERTa, T5 and GPT-3. As we can see from the graph, it took\nalmost 5000 days to train GPT-3.\n\n\n    \n\n\nResults\n\nThe following is a comparison among the different learning schemes used\nwith GPT-3 and the state or the art (fine-tuned) model on various tasks:\n\n\n  \n    Language Modeling:\n\n    \n      \n        Dataset: Penn Tree Bank\n      \n      \n        Evaluation Metric: perplexity\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Long-Range Language Modeling:\n\n    \n      \n        Dataset: LAMBADA\n      \n      \n        Evaluation Metric: perplexity / Accuracy\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Story Completion:\n\n    \n      \n        Dataset: StoryCloze &amp;amp; HellaSwag\n      \n      \n        Evaluation Metric: Accuracy\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Question Answering:\n\n    \n      \n        Dataset: NaturalQS, WebQS &amp;amp; TriviaQA\n      \n      \n        Evaluation Metric: Accuracy\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Machine Translation:\n\n    \n      \n        Dataset: WMT’14 (Fr↔En), WMT’16 (De↔En) &amp;amp; WMT’16 (Ro↔En).\n      \n      \n        Evaluation Metric: BLEU\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Winograd-Style Tasks: determining to which word a\npronoun refers\n\n    \n      \n        Dataset: Winograd &amp;amp; WinogradXL\n      \n      \n        Evaluation Metric: Accuracy\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Common Sense Reasoning:\n\n    \n      \n        Dataset: PIQA, ARC, OpenBookQA\n      \n      \n        Evaluation Metric: Accuracy\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Reading Comprehension:\n\n    \n      \n        Dataset: CoQA, DROP, QuAC, SQuADv2, RACE-h, RACE-m.\n      \n      \n        Evaluation Metric: Accuracy for RACE-h &amp;amp; RACE-m, and F1 for\nthe rest.\n      \n    \n  \n\n\n\n    \n\n"
      },
    
  
  
    
    
  
  
    
    
  
  
    
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "t-SNE",
        "url"       : "/word-embedding/t-SNE",
        "date"      : "25/11/2008",
        "content": "One of the popular things to do with word embedding, is to take this\nN-dimensional data and embed it in a two-dimensional space so that we\ncan visualize them. The most common algorithm for doing this is the\nt-SNE algorithm created by Laurens van der Maaten and Geoffrey\nHinton in 2008 and published in this paper: Visualizing data using\nt-SNE.\n\nt-SNE stands for t-distributed Stochastic Neighbor Embedding which is a\nmachine learning algorithm for visualization, often used to visualize\nhigh-level representations learned by an artificial neural network,\ndeveloped .\n\n\n    \n\n\nAnd if you look at one of these embeddings, you find that words like\n“man” and “woman” tend to get grouped together, “king” and “queen” tend\nto get grouped together, and these four are the people which tends to\nget grouped together. “dog”, “cat” and “fish” are animals which can get\ngrouped together. Fruits will tend to be close to each other. Numbers\nlike “one”, “two”, “three”, “four”, will be close to each other.\n\nTO BE CONTINUED…\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "Word2Vec",
        "url"       : "/word-embedding/word2vec",
        "date"      : "07/09/2013",
        "content": "Word2Vec stands for “word-to-vector” is a model architecture created by\nTomáš Mikolov from Google in 2013 and published in the paper: Efficient\nEstimation of Word Representations in Vector\nSpace. This model aims at\ncomputing continuous vector representations of words from very large\ndata sets.\n\nIt uses a principle in distributional semantics that says that “the\nword’s meaning is given by the words that frequently appear close-by”.\nAnd this is the main idea behind “word2vec”. Also, this means that\nsimilar words will appear in similar context which will make their\nvectors similar and that’s a great feature in word2vec as we are going\nto see later.\n\nThe way we define these vectors is actually a trick you may have seen\nelsewhere in machine learning. We’re going to train a simple neural\nnetwork with a single hidden layer to perform a certain task, but then\nwe’re not actually going to use that neural network for the task we\ntrained it on! Instead, the goal is actually just to learn the weights\nof the hidden layer, these weights are actually the “word vectors” that\nwe’re trying to learn.\n\nSo now we need to talk about this “fake” task that we’re going to build\nthe neural network to perform, and then we’ll come back later to how\nthis indirectly gives us those word vectors that we are really after.\nWe’re going to train the neural network to be a language model. To be\nmore specific, the model should do the following; given nearby words\n(before and after), the model should predict the most probable word that\nfits. At first, it will be random but after a few iterations over the\ntraining data it will produce more appropriate results. When we say\n&quot;nearby&quot;, there is actually a &quot;window size&quot; parameter to the\nalgorithm. A typical window size might be 5, meaning 5 words behind and\n5 words ahead (10 in total).\n\nWord2vec comes in two flavors, the Continuous Bag-of-Words model (CBOW)\nand the Skip-Gram model. Algorithmically, these models are similar,\nexcept for only one thing which is the shape of the input layer and the\noutput layer. As CBOW predicts target words from source context\nwords, while the skip-gram does the inverse and predicts source\ncontext-words from the target words.\n\nThis inversion might seem like an arbitrary choice, but statistically it\nhas the effect that CBOW smooths over a lot of the distributional\ninformation (by treating an entire context as one observation). For the\nmost part, this turns out to be a useful thing for smaller datasets.\nHowever, skip-gram treats each context-target pair as a new observation,\nand this tends to do better when we have larger datasets.\n\nI know this seems difficult, so let’s see the Skip-Gram and CBOW models\nin more details.\n\nSkip-Gram\n\nSkip-Gram model is a model and how to prepare the training data. In\nSkip-Gram, we’ll train the neural network by feeding it word pairs found\nin our training documents. The below example shows some of the training\nsamples (word pairs) we would take from the sentence “The quick brown\nfox jumps over the lazy dog”. Here, we’ve used a small window size of\n$2$ just for the example. A more reasonable size would be $5$ or more.\nThe word highlighted in blue is the input word (center) and the other\ntwo on the left and right are the context words, each is called\n(target).\n\n\n    \n\n\nThe network is going to learn the statistics from the number of times\neach pairing shows up. So, for example, the network is probably going to\nget many more training samples of (“Soviet”, “Union”) than it is of\n(“Soviet”, “Apple”). When the training is finished, if you give it the\nword “Soviet” as input, then it will output a much higher probability\nfor “Union” or “Russia” than it will for “Apple”.\n\nNow, how is this all represented? First of all, we know we can’t feed a\nword just as a text string to a neural network, so we need a way to\nrepresent the words to the network. To do this, we first build a\nvocabulary of words from our training documents, let’s say we have a\nvocabulary of $10,000$ unique words. Then, we’re going to represent the\ninput word as a one-hot vector. This vector will have $10,000$\ncomponents (one for every word in our vocabulary) and we’ll place a $1$\nin the position corresponding to the word, and $0$s in all of the other\npositions.\n\nThe output of the network is a single vector (also with $10,000$\ncomponents) containing, for every word in our vocabulary, the\nprobability that a randomly selected nearby word is that vocabulary\nword. Here’s the architecture of our neural network, noting that there\nis no activation function on the hidden layer neurons, but the output\nneurons use Softmax:\n\n\n    \n\n\nWhen training this network on word pairs, the input is a one-hot vector\nrepresenting the input word and the training output is also a one-hot\nvector representing the output word. But when we evaluate the trained\nnetwork on an input word, the output vector will actually be a\nprobability distribution (i.e., a bunch of floating point values, not a\none-hot vector).\n\nFor our example, we’re going to say that we’re learning word vectors of\n$300$ features. So the hidden layer is going to be represented by a\nweight matrix of $10,000$ rows (one for every word in our vocabulary)\nand $300$ columns (one for every hidden neuron). “$300$ features” is\nwhat Google used in their published model trained on the Google news\ndataset, but we can tune this number if we want; we can try different\nvalues and see what yields the best results. As a matter of fact, the\nweights of the hidden layer after finishing training, is the word\nembedding we are looking for. This matrix is often called\n“Embedding Matrix”\n\n\n    \n\n\nSo, the end goal of all of this is really just to learn this hidden\nlayer weight matrix, the output layer we’ll just toss when we’re done!\nLet’s get back, though, to working through the definition of this model\nthat we’re going to train. The one-hot vector is almost all zeros,\nwhat’s the effect of that? If we multiply a $1 \\times 10,000$ one-hot\nvector by a $10,000 \\times 300$ matrix, it will effectively just select\nthe matrix row corresponding to the 1. This means that the hidden layer\nof this model is really just operating as a lookup table.\n\nHere’s a small example to give you a visual:\n\n\n    \n\n\nThe output layer is a Softmax regression classifier of $10,000$ neurons.\nEach output neuron has a weight vector which it multiplies against the\nword vector from the hidden layer, then it applies the Softmax\nactivation function. Here’s the formula that we are going to follow to\ntrain this neural network:\n\n\\[p\\left( \\text{target} \\middle| \\text{context} \\right) = \\frac{\\exp\\left( u_{\\text{target}}^{T}.v_{\\text{context}} \\right)}{\\sum_{i = 1}^{V}{\\exp\\left( u_{i}^{T}.v_{\\text{context}} \\right)}}\\]\n\nSuch that:\n\n\n  \n    $\\text{target}$ is the target word.\n  \n  \n    $\\text{target}$ is the context word.\n  \n  \n    $V$ is the number of words in our vocabulary.\n  \n  \n    $u_{\\text{target}}$ refers to the word vector of the target word.\nIts shape should be $d \\times 1$ where $d$ is the number of\nfeatures/dimensions ($300$ in our case). $u_{\\text{target}}^{T}$\nshape should be $1 \\times d$.\n  \n  \n    $v_{\\text{context}}$ refers to the output-layer vector (outside\nvector) of the context word. Its shape should be\n$1 \\times d$ where $d$ is the number of features ($300$ in our\ncase).\n  \n\n\nSo, as we can see each word has two representations in our Word2Vec\nmodel; one when it’s a target word and another when it’s a context word.\nHere’s an illustration of calculating the output of the output neuron\nfor an input word.\n\n\n    \n\n\nIf two different words have very similar contexts, then our model needs\nto output very similar results for these two words. So, if two words\nhave similar contexts, then our network is motivated to learn similar\nword vectors for these two words. And what does it mean for two words to\nhave similar contexts? I think we could expect that synonyms like\n“intelligent” and “smart” would have very similar contexts. Or that\nwords that are related, like “engine” and “transmission”, would probably\nhave similar contexts as well. This can also handle stemming for us –\nthe network will likely learn similar word vectors for the words “ant”\nand “ants” because these should have similar contexts. The following\nformula is cost function that we are going to use to learn our model:\n\n\\[J = \\frac{- 1}{S}\\sum_{i = 1}^{S}\\left( \\sum_{- m \\leq j \\leq m,j \\neq 0}^{}{\\log\\left( p\\left( w_{i + j} \\middle| w_{i} \\right) \\right)} \\right)\\]\n\nSuch as that:\n\n\n  \n    $J$ represents the objective function.\n  \n  \n    $S$ represents the total number of sentences that we are going to\ntrain our model upon.\n  \n  \n    $m$ is the window size.\n  \n  \n    $w_{i}$ is the context word.\n  \n  \n    $w_{j + i}$ is the target word.\n  \n  \n    $p\\left( w_{i + j} \\middle| w_{i} \\right)$ represents the probability of\nthe target word given the context word. In other words, it’s equal to:\n  \n\n\n\\[p\\left( \\text{target} \\middle| \\text{context} \\right) = \\frac{\\exp\\left( u_{\\text{target}}^{T}.v_{\\text{context}} \\right)}{\\sum_{i = 1}^{V}{\\exp\\left( u_{i}^{T}.v_{\\text{context}} \\right)}}\\]\n\nYou may have noticed that the skip-gram neural network contains a huge\nnumber of weights. For our example with $300$ features and a vocab of\n$10,000$ words, that’s $3,000,000$ weights in the hidden layer and\noutput layer each! Running gradient descent on a neural network that\nlarge is going to be slow. And to make matters better, you need a huge\namount of training data in order to tune that many weights and avoid\nover-fitting. Millions of weights times billions of training samples\nmeans that training this model is going to be a beast. The authors of\nWord2Vec addressed these issues in their second paper: “Distributed\nRepresentations of Words and Phrases and their\nCompositionality”.\n\nThere are three innovations in this second paper that made training a\nbit faster:\n\n\n  \n    Treating common word pairs or phrases as single “words” in their model.\n  \n  \n    Subsampling frequent words to decrease the number of training examples.\n  \n  \n    Modifying the optimization objective with a technique they called\n“Negative Sampling”, which causes each training sample to update\nonly a small percentage of the model’s weights. It’s worth noting\nthat subsampling frequent words and applying Negative Sampling not\nonly reduced the compute burden of the training process, but also\nimproved the quality of their resulting word vectors as well.\n  \n\n\nLet’s talk about these three modifications with more details:\n\nWord Pairs\n\nHere, we are going to explain the first innovation (or modification)\nthat the authors of Word2Vec have produced in their second paper. The\nauthors pointed out that a word pair like “Boston Globe” (a newspaper)\nor “Chicago Bulls” (a Basketball Team) has a much different meaning than\nthe individual words “Chicago” and “Bulls”. So it makes sense to treat\nthem as a single word with its own word vector representation. We can\nsee the results in their published model, which was trained on $100$\nbillion words from a Google News dataset. The addition of phrases to the\nmodel swelled the vocabulary size to $3$ million words!\n\nBut the question here is “how did they detect these word-pairs?”. The\nanswer is very straight forward. Each pass of their model only looks at\ncombinations of two words, but you can run it multiple times to get\nlonger phrases. So, the first pass will pick up the phrase “New_York”,\nand then running it again will pick up “New_York_City” as a combination\nof “New_York” and “City” and so on. The tool counts the number of times\neach combination of two words appears in the training text, and then\nthese counts are used in an equation to determine which word\ncombinations to turn into word-pair.\n\nThe equation is designed to pay attention to the word-pairs that occur\ntogether often relative to the number of individual occurrences. It also\nfavors pairs made of infrequent words in order to avoid making phrases\nout of common words like “and the” or “this is”.\n\nSubsampling Frequent Words\n\nWe have seen how training samples were created from the source text. The\nbelow example shows some of the training samples (word pairs) we would\ntake from the sentence “The quick brown fox jumps over the lazy dog.”\nusing a small window size of 2. The word highlighted in blue is the\ninput word:\n\n\n    \n\n\nBut, in this representation we have two problems with common words like\n“the”:\n\n\n  \n    When looking at word pairs, (“fox”, “the”) doesn’t tell us much\nabout the meaning of “fox”. “the” appears in the context of pretty\nmuch every word.\n  \n  \n    We will have many more samples of (“the”, …) than we need to learn\na good vector for “the”.\n  \n\n\nWord2Vec implements a “subsampling” scheme to address this. They\nimplemented an equation for calculating a probability with which to keep\na given word in the vocabulary. This probability that we cut the word is\nrelated to the word’s frequency as we can see:\n\n\\[P\\left( w_{i} \\right) = \\frac{\\left( \\sqrt{\\frac{z\\left( w_{i} \\right)}{s}} + 1 \\right) \\ast s}{z\\left( w_{i} \\right)}\\]\n\nWhere $w_{i}$ represent the word, $z\\left( w_{i} \\right)$ represents the\nword frequency ratio. For example, if the word “peanut” occurs $1,000$\ntimes in a one-billion-word corpus, then\n$z\\left( ‘peanut’ \\right) = \\frac{10^{3}}{10^{9}} = 10^{- 6}$. And $s$\nis the ‘sampling rate’ which is a hyper-parameter that controls how much\nsubsampling occurs, and the default value is $0.001$. Smaller values of\n‘sampling rate’ mean words are less likely to be kept. And finally,\n$P\\left( w_{i} \\right)$ represents the probability of keeping the word\n$w_{i}$.\n\nNow, if we have a window size of 10, and we remove a specific instance\nof “the” from our text. Then, we will train on the remaining words, as\n“the” will not appear in any of their context windows. And we’ll have 10\nfewer training samples where “the” is the input word.\n\nNegative Sampling\n\nTraining a neural network means taking a training example and adjusting\nall of the neuron weights slightly so that it predicts that training\nsample more accurately. In other words, each training sample will tweak\nall of the weights in the neural network. As we discussed above, the\nsize of our word vocabulary means that our Skip-Gram neural network has\na tremendous number of weights, all of which would be updated slightly\nby every one of our billions of training samples!\n\nNegative sampling addresses this by having each training sample only\nmodify a small percentage of the weights, rather than all of them.\nHere’s how it works:\n\n\n  When training the network on the word pair (“fox”, “quick”), we will\ngenerate another $k$ words randomly from the vocabulary. These $k$\nwords are called negative samples. The paper says that selecting\n5-20 words works well for smaller datasets, and you can get away\nwith only 2-5 words for large datasets.\n\n\n\n    \n\n\nAs we can see, here we have generated $4$ words randomly from the\nvocabulary and they are (“king”, “orange”, “box”, and “the”). As we can\nsee, the word “the” exists in the context of the word “quick”, but it’s\nok.\n\n\n  After that, we will multiply the Embedding Matrix $E$ by the one-hot\nvector of the five words producing embedding vector:\n\n\n\\[O_{\\text{fox}} \\odot E = e_{\\text{fox}},O_{\\text{quick}} \\odot E = e_{\\text{quick}},O_{\\text{king}} \\odot E = e_{\\text{king}}\\]\n\n\\[O_{\\text{orange}} \\odot E = e_{\\text{orange}},O_{\\text{box}} \\odot E = e_{\\text{box}},O_{the} \\odot E = e_{the}\\]\n\n\n  Finally, change the problem into a classification problem where the\ninput is the word-pairs; and the output is either 1 or zero.\n\n\nRecall that the output layer of our model has a weight matrix of\n$300 \\times 10,000$. So, we will just be updating the weights for our\npositive word (“quick”), plus the weights for $4$ other words that we\nwant to output 0. That’s a total of $5$ output neurons, and $1,500$\nweight values total. That’s only $0.06\\text{\\%}$ of the $3$ Million\nweights in the output layer!\n\nChoosing the “negative samples” randomly isn’t that efficient because\nthe probability of a common word like “and” to be chosen is higher than\nany other word. Instead, we use a “unigram distribution” where the\nprobability for selecting a word as a negative sample is related to its\nfrequency, with more frequent words being more likely to be selected as\nnegative samples as we can see in the following formula:\n\n\\[P\\left( w_{i} \\right) = \\frac{f\\left( w_{i} \\right)^{\\frac{3}{4}}}{\\sum_{j = 0}^{n}\\left( f\\left( w_{i} \\right)^{\\frac{3}{4}} \\right)}\\]\n\nWhere $P\\left( w_{i} \\right)$ represents the probability of a word\n$w_{i}$ to be selected as a negative sample. As we can see, each word is\ngiven a weight equal to its frequency (word count)\n$f\\left( w_{i} \\right)$ raised to the $\\frac{3}{4}$ power. The decision\nto raise the frequency to the $\\frac{3}{4}$ power appears to be\nexperiential\n\n\\[J = \\frac{1}{S}\\sum_{i = 1}^{S}\\left( \\sum_{- m \\leq j \\leq m,j \\neq 0}^{}{\\log\\left( p\\left( w_{i + j} \\middle| w_{i} \\right) \\right)} \\right)\\]\n\nCBOW\n\nWe have explained the Skip-Gram model, now let’s explore the other\nWord2Vec model which is the Continuous Bag-of-Words (CBOW) model. As we\nhave said before, the only difference between Skip-Gram and CBOW is the\nshape of the input and output layer of the network. So, if we understand\nthe Skip-Gram model then the CBOW model should be quite straight-forward\nbecause in many ways they are mirror images of each other.\n\nTo understand that model, let’s consider having the following sentence\n“the quick brown fox jumps over the lazy dog.”. Here, we are going to\nuse a small window size of $2$ just for the example. A more reasonable\nsize would be $5$ or more. The word highlighted in blue is the output\nword (target).\n\n\n    \n\n\nUnlike the Skip-Gram, the continuous bag of words model context is\nrepresented by multiple words for a given target word. For example, we\ncould use “The quick fox jumps” as context words for “brown” as the\ntarget word. This calls for a modification to the neural network\narchitecture. The modification, shown below, consists of replicating the\ninput to hidden layer connections $C$ times which represents the window\nsize of the context (four in our case):\n\n\n    \n\n\nLike the Skip-Gram, we will use a one-hot vector to encode our\nwords. This vector will have $10,000$ components (one for every word in\nour vocabulary). The output of the network is a single vector (also with\n10,000 components) containing, for every word in our vocabulary, the\nprobability that a randomly selected nearby word is that vocabulary\nword. Here’s the architecture of our neural network, noting that there\nis no activation function on the hidden layer neurons, but the output\nneurons use Softmax. Everything else will be exactly as the\nSkip-Gram model.\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "Sentence Embedding",
        "url"       : "/word-embedding/sentence_embedding",
        "date"      : "22/05/2014",
        "content": "Here, we are going to talk about an important issue that tried to use\nthe Word Embedding to produce a sentence embedding. By sentence\nembedding, we mean to provide a vector of $d$ length that has the\nmeaning of the sentence in a numerical form; the same form as we did\nwith word embedding.\n\n\n    \n\n\nFollowing this approach, we could be able to found similarity between\ntwo sentences that has similar meaning. And this could give us some\nsuperiority over Cosine Similarity which determines the similarity bases\non the common words. Using two sentences like “Mexico wishes to\nguarantee citizen’s safety” and “Mexico intends to avoid more violence”\nwill be not that similar using Cosine Similarity even though they are\npretty similar in the meaning. And two sentences like “Iranians Vote in\nPresidential Election” and “Keita Wins Mali Presidential Election” will\nbe very similar using cosine similarity although they are not similar in\nthe meaning.\n\nLe and Mikolov from Google, in their paper: “Distributed\nRepresentations of Sentences and\nDocuments” published in 2014,\nproposed different techniques for Sentence Embedding as we are going to\ndiscuss below:\n\nBag-of-Words (BoW)\n\nThis is the simplest method of which we can covert the word embedding\nvectors into sentence embedding vector. In this method we get the\naverage of the words vectors that form the sentence. So, the\nsentence embedding vector of “Natural Language Processing” is:\n\n\\[v\\left( \\text{Natural Language Processing} \\right) = \\frac{v\\left( \\text{Natural} \\right) + v\\left( \\text{Language} \\right) + v\\left( \\text{Processing} \\right)}{3}\\]\n\nBut this method neglects a lot of information like the sequence of the\nwords and that might give false results. So, for example the sentence\n“You are going there to teach not to play.” will have the same sentence\nembedding as “You are going there to play not to each.” even though they\nare exactly the opposite.\n\nDistributed Bag-of-Words (DBoW)\n\nLe and Mikolov from Google, in their paper “Distributed Representations\nof Sentences and Documents”\npublished in 2014, proposed an distributed bag-of-words (DBOW) which\nused only the paragraph context vector to predict the words in the\nparagraph. This simple model is analogous to the skip-gram version of\nword2vec, except the paragraph vector is used to predict all the words\nparagraph instead of using the target word to predict the context words.\nAs in the skip-gram model, DBOW is very computationally and memory\nefficient. Empirical results have shown that both DM and DBOW outperform\nbag-of-words and bag-of-n-gram models for text representations.\nFurthermore, averaging the DM and DBOW vector representations often\nyields the best performance overall.\n\n\n    \n\n\ndoc2vec\n\nThe model generates fixed-length feature representations from variable\nlength pieces of text, making it useful for application to sentences,\nparagraphs, sections, or entire documents. The key to the approach is to\nassociate every paragraph with a unique paragraph vector $u^{i}$, which\nis averaged with the word vectors $w_{j}^{i}$ of the $J$ words in the\nparagraph to yield a representation of the paragraph $p^{i}$:\n\n\\[p^{i} = u^{i} + \\sum_{j = 1}^{J}w_{j}^{i}\\]\n\nThe paragraph vector ui can be thought of acting as a memory that\nremembers word order context. During training, a sliding window of\ncontext words $C$ and the paragraph vector $p^{i}$ are used to predict\nthe next word in the paragraph context. Both paragraph vectors and word\nvectors are trained via backpropagation. While the paragraph vector is\nunique to each paragraph and shared across all contexts generated from\nthe same paragraph, the word vectors are shared across the entire\ncorpus. It is notable that the\n\n\n    \n\n\nLater Check:\n\nThere is a famous paper published by Sanjeev Arora, Yingyn Liang, and\nTengyu Ma, who are a group of researchers at Princeton, \nand they call it “A simple but Tough-to-beat Baseline for Sentence\nEmbedding”.\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "GloVe",
        "url"       : "/word-embedding/GloVe",
        "date"      : "25/10/2014",
        "content": "GloVe (Global Vectors for Word Representation) is a model released in\n2014 by Stanford NLP Group researchers Jeffrey Pennington, Richard\nSocher, and Chris Manning for learning word embedding and\npublished in the paper: GloVe: Global Vectors for Word\nRepresentation. The GloVe\nauthors present some results which suggest that their model is\ncompetitive with Google’s popular word2vec package.\n\nThe key idea behind GloVe model is to combine two concepts into one\nmodel; It learns word vectors by using the same idea of Word2Vec\nmodel beside examining [word co-occurrences within a text\ncorpus.\n\nCo-occurrence Matrix\n\nBefore we train the actual model, we need to construct a co-occurrence\nmatrix $X$, where a cell $X_{\\text{ij}}$ represents how often the word\n$i$ appears in the context of the word $j$ using a certain window size.\nWe run through our corpus just once to build the window-based\nco-occurrence matrix $X$ whose dimension is\n$\\left| V \\right| \\times \\left| V \\right|$. We will construct our model\nbased only on the values collected in $X$. So, assuming that our corpus\nhas only three sentences which are “I like deep learning.”, “I like\nNLP.”, and “I enjoy flying.” and assuming also that we have a window of\njust one word; then our co-occurrence matrix would be:\n\n\n    \n\n\nThere are a lot of things we should notice looking at this matrix:\n\n\n  \n    The co-occurrence matrix is symmetric, which means that the\nco-occurrence count of “I” and “like” is the same as the\nco-occurrence count of “like” and “I”.\n  \n  \n    The shape of the matrix is $V \\times V$ where $V$ is the size of the\nvocabulary which is $8$ in our case here. So, imagine using a\nmillion-word vocabulary.\n  \n  \n    There are a lot of zeros in this matrix. So, most of the used\nstorage will be just zeros.\n  \n\n\nTo be able to use this matrix in our GloVe model, there are some hacks\nthat we need to do to this matrix:\n\n\n  \n    Words like “the”, “he”, “she”, “and” …etc. are too frequent, so we\ncan limit their count to a certain number, say $100$, or just\nignore them all.\n  \n  \n    Use Pearson correlations instead of counts, then set negative values\nto $0$.\n  \n  \n    Ramped windows that count closer words more.\n  \n\n\nModel\n\nAs I said before, GloVe models combine Word2Vec model with the\nco-occurrence matrix. So, we can imagine that the GloVe model is a\nmodification of the Word2Vec model where we use the word-by-word\nco-occurrence matrix with word-vectors that we want to train just like\nthat:\n\n\\[w_{i}^{T}.w_{j} + b_{i} + b_{j} = log\\left( X_{\\text{ij}} \\right)\\]\n\nWhere\n\n\n  \n    $i$ and $j$ are the word-pairs.\n  \n  \n    $X_{\\text{ij}}$ refers to the co-occurrence count of word $i$ with word $j$.\n  \n  \n    $w_{i}$ and $w_{j}$ are the two word-vectors that we are trying to learn.\n  \n  \n    $b_{i}$ and $b_{j}$ are the biases that we are trying to learn as well.\n  \n\n\nThis formula works well, but we can weigh words differently to derive\nmore flexibility and robustness like so:\n\n\\[\\sum_{i,j = 1}^{\\left| V \\right|}{f\\left( X_{\\text{ij}} \\right)\\left( w_{i}^{T}.w_{j} + b_{i} + b_{j} - log\\left( X_{\\text{ij}} \\right) \\right)^{2}}\\]\n\nSuch as that:\n\n\n  \n    $\\left| V \\right|$ is the number of word-pairs in the vocabulary.\n  \n  \n    $f\\left( X_{\\text{ij}} \\right)$ represents a simple function that\nrestrict the values of the co-occurrence count. We can use the\nfollowing function:\n  \n\n\n\\[f\\left( X_{\\text{ij}} \\right) = \\left\\{ \\begin{matrix}\n\\left( \\frac{X_{\\text{ij}}}{x_{\\max}} \\right)^{\\alpha}\\text{if}\\ \\ X_{\\text{ij}} &amp;lt; x_{\\max} \\\\\n1\\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{otherwise} \\\\\n\\end{matrix} \\right.\\]\n\nWhere $x_{\\max}$ is called “the cutoff” usually equals to $100$. When we\nencounter extremely common word pairs (where\n$X_{\\text{ij}} &amp;gt; x_{\\max}$), this function will simply return $1$. For\nall other word pairs, it will return some weight in the range\n$\\left( 0,1 \\right)$, where the distribution of weights in this range is\ndecided by $\\alpha$ which is usually equals to $0.75$.\n\nOR we could use a simpler function like the\n$\\max\\left( X_{\\text{ij}},c \\right)$ where $c$ is a constant number\n(usually $100$).\n\n\n    \n\n\nEvaluation\n\nBefore talking about how to evaluate our model, let’s first take about\nthe hyper-parameters that can be tuned to get better and better results.\nThe following graph is from the published GloVe paper:\n\n\n  \n    First, Same hyper-parameters as Word2Vec models which are:\n\n    \n      \n        Initialization method.\n      \n      \n        Learning rate $\\alpha$.\n      \n      \n        Word vector dimension $d$. Best value for $d$ is about $300$ and\nit slightly drops-off afterwards. But, it might be different\nfor downstream tasks.\n      \n      \n        Window size $m$. As we can see in the following graph, the best\nvalue is around $8$.\n      \n      \n        Using symmetric or Asymmetric windows. Symmetric\nis using the window size for both directions (left and right).\nAsymmetric is using the window size for just one direction.\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Using either co-occurrence count or the Pearson\ncorrelations instead of counts and set negative values to\n$0$.\n  \n  \n    Training Time (no. of iterations): Usually the more … the better\n  \n\n\n\n    \n\n\nNow, how to evaluate our model? Let’s first talk about how to evaluate\nNLP tasks in general. Actually, there are two methods (Intrinsic and\nExtrinsic). Intrinsic methods try to find a mathematical formula or a\nsub-task to evaluate our model. While Extrinsic methods try to get our\nmodel into a real-life task. Both have some pros and cons:\n\n\n\n    \n        \n            \n            Intrinsic\n            Extrinsic\n        \n    \n    \n        Pros\n        \n            \n                Fast to compute.\n                Very useful when trying to get intuition about the model.\n            \n        \n        \n            \n                Very sufficient in case of good results.\n            \n        \n    \n    \n        Cons\n        \n            \n                Not enough to decide if the model is helpful or not.\n            \n        \n        \n            \n                Takes a long time to compute.\n                In the case of bad results, it is unclear if the subsystem is the problem or the intrinsic task wasn&#39;t properly performed.\n               \n        \n    \n\n\n\nSo, let’s explain this in more details… Assume that we have a\nword-vectors for a $1,000,000$ word vocabulary and we want to evaluate\nif these word-vectors are worth publishing or not?? When thinking about\nintrinsic evaluation, we might think to get just a $10,000$ words and\ntest on a word analogy task]. This is fast to compute, and we can\ntune some of the parameters to get better and better results. But,\n$10,000$ isn’t enough, we need to get the whole matrix into\nconsideration. So, we decided to use the whole $1,000,000$ words into\nthe same task (word analogy). Putting in mind that such a task might\ntake hours and hours to train and evaluate. And the results might get\nworse as the hyper-parameters that we have tuned are for the $10,000$\ntask not the whole corpus.\n\nSo, the solution is to to balance between these two methods. At first,\nwe have to start with the intrinsic method and change only one\nparameter. And don’t confirm this change unless it’s evaluated by the\nExtrinsic method.\n\n\n  Note:\nWe can use a pre-trained GloVe word embedding freely from the Stanford\nGloVe official website. We can know a lot about these pre-trained models\nfrom the name, for example the pre-trained model “glove.6B.50d”, it’s\ncalled 6B because it’s formed using 6 billion words in context, and 50d\nbecause the extracted features (dimension) are 50.\n\n\nRe-training Word Vectors\n\nWe can always use pre-trained word vectors whether it’s Word2Vec\nembeddings or GloVe embeddings, but what about use these pre-trained\nword vectors as the initial weights of our model?? Is it a good idea??\n\nActually, that’s a great question and the answer is not that simple. But\nluckily there is a rule-of-thumb that we can use… if the training\ndataset is quite small, then it’s so much better if we don’t train our\nmodel using these pre-trained vectors. And if the training dataset is\nvery large, it may work better to train the model using the pre-trained\nvectors. And that’s because if the dataset is quite small, some of the\nword vectors will get better values but not all of them, so the model\nloses generalization.\n\nLet’s see how… assume that we have a small data corpus where the word\n“TV” and “telly” are mentioned and the word “television” is not; then\nwhen training the model using these small dataset, the model changes the\n“TV” and “telly” word vectors according to their context. But what about\nthe word “television&quot;?? It won’t change, which makes its vector less\nsimilar to the vectors of “TV” and “telly” putting in mind that these\nthree words are the same. So, their pre-trained vectors are pretty\nsimilar. That’s why when training a model with small dataset some of the\nwords are left out, and similar word-vectors becomes less similar.\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "sense2vec",
        "url"       : "/word-embedding/sense2vec",
        "date"      : "19/11/2015",
        "content": "One of the limits of word2vec is polysemy, which means that one word\ncould have multiple meanings or senses. For example, the word “bank”\ncould be a verb meaning “do financial work” or a noun meaning “financial\ninstitution”. And this problem is known in NLP by the name of\n“word-sense disambiguation”. In 2015, Andrew Trask proposed a model in\nhis paper: Sense2Vec - A Fast And Accurate Method For Word Sense\nDisambiguation In Neural Word\nEmbeddings. called “sense2vec”.\n\nSense2vec is a simple method to achieve word-sense disambiguation that\nleverages supervised labeling such as part-of-speech. The sense2vec\nmodel can learn different word senses of this word by combining a\nsingle-sense embedding model with POS labels as shown in the following\nfigure:\n\n\n    \n\n\nGiven a corpus, sense2vec will create a new corpus for each word for\neach sense by concatenating a word with its POS label. The new corpus is\nthen trained using word2vec’s CBOW or skip-gram to create word\nembeddings that incorporate word sense (as it relates to their POS\nusage) as shown in the following figure:\n\n\n    \n\n\nTO BE CONTINUED…\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "De-biasing Word Vectors",
        "url"       : "/word-embedding/de-biasing",
        "date"      : "21/07/2016",
        "content": "In this paper: Man is to Computer Programmer as Woman is to Homemaker?\nDebiasing Word Embeddings published in\n2016, the researchers examined the gender biases that can be reflected in a\nword embedding and explore some algorithms for reducing the bias.\n\nFirst, what does biasing of word vectors really mean? OK, let’s put it\nthat way. There are some words that relate only the females like\n“actress, waitress, mother, aunt, …” and there are some words that\nrelate only the males like “actor, waiter, father, uncle, …”. And\nthere some other words that can relate to the two genders like\n“programmer, developer, assistant, doctor, …”.\n\nSurprisingly, most of the word embedding that are out there are biasing\ntowards a certain gender due to the context they were mentioned at.\nLet’s see the “glove.6B.50d.txt” for example, there are some words that\nrelate to the female more that the male like “lipstick, arts,\nliterature, doctor, receptionist, fashion, …”. And let’s be honest,\nsome of them make perfect sense like “lipstick” for example, but there\nare also some of them that don’t make any sense like “doctor” and\n“receptionist” which must be gender unspecific.\n\nWe&#39;ll see how to reduce the bias of these vectors, using an algorithm\ndue to Boliukbasi 2016. Note that some word pairs such as\n&quot;actor&quot;/&quot;actress&quot; or &quot;grandmother&quot;/&quot;grandfather&quot; should remain\ngender specific, while other words such as &quot;receptionist&quot; or\n&quot;technology&quot; should be neutralized, i.e. not be gender-related. You\nwill have to treat these two types of words differently when de-biasing.\n\nImplicit Association Test\n\nImplicit Association Test is the test used to check if two sets of words\nare biased towards a certain topic or not. And that’s how it works;\nassume that you have two sets of words that you want to check the bias\nbetween them (they are called attributes):\n\n\n  \n    X: {male, man, boy, brother, he, him, his, son}.\n  \n  \n    Y: {female, woman, girl, sister, she, her, hers, daughter}.\n  \n\n\nNow, let’s assume that we have another two sets of words that represent\ntwo opposing topics (they are called target words):\n\n\n  \n    A: {math, algebra, geometry, calculus, equations, numbers}.\n  \n  \n    B: {poetry, art, dance, literature, novel, symphony, drama}.\n  \n\n\nAnd this can be done via the following formula:\n\n\\[s\\left( X,Y,A,B \\right) = \\sum_{x \\in X}^{}s\\left( x,A,B \\right) - \\sum_{y \\in Y}^{}s\\left( y,A,B \\right)\\]\n\n\\[s\\left( w,A,B \\right) = \\frac{1}{\\text{card}\\left( A \\right)}\\sum_{a \\in A}^{}\\cos\\left( w,a \\right) - \\frac{1}{\\text{card}\\left( B \\right)}\\sum_{b \\in B}^{}\\cos\\left( w,b \\right)\\]\n\nWhere:\n\n\n  \n    $s\\left( X,Y,A,B \\right)$: is the association test between all words\nof attributes and target words. And it could have three possible\nvalues:\n\n    \n      \n        If the score is positive, it means the association between X\nand A is big than Y and B. and the higher the score is, the\nmore association there is.\n      \n      \n        If the score is negative, it means the association between Y\nand B is big than X and A. and the lower the score is, the\nmore association there is.\n      \n      \n        If the score is zero, it means there are no association\nbetween X and A and Y and B.\n      \n    \n  \n  \n    $s\\left( w,A,B \\right)$: is the association strength between word w\nand set A and away from set B.\n  \n  \n    $\\text{card}\\left( A \\right)$: It’s the cardinality a set of words A\nwhich is a measure of a set&#39;s size, meaning the number of unique\nelements in that set. For instance, the set $A = { 1,2,4}$ has a\ncardinality of $3$ for the three elements that are in it.\n  \n\n\nNeutralization\n\nBy neutralization, we mean to neutralize the bias for non-gender\nspecific words. So, the words like “receptionist, doctor, literature,\nart, technology, …” will be gender unspecific. We are going to do that\nusing some linear algebra concepts like so:\n\n\n    \n\n\nThe figure above should help visualizing what neutralization does. If\nyou&#39;re using a 50-dimensional word embedding, the 50-dimensional space\ncan be split into two parts: The gender-direction $\\overrightarrow{g}$,\nand the remaining 49 dimensions, which we&#39;ll call\n${\\overrightarrow{g}}_{\\bot}$. Even though\n${\\overrightarrow{g}}_{\\bot}$ is 49 dimensional, given the\nlimitations of what we can draw on a screen, we illustrate it using a\n1-dimensional axis below.\n\nIn linear algebra, we say that the 49-dimensional vector\n${\\overrightarrow{g}}_{\\bot}$ is perpendicular (or “orthogonal”) to\n$\\overrightarrow{g}$, meaning it is at can NOT be affected by\n$\\overrightarrow{g}$ . The neutralization step takes a vector such as\n$e_{\\text{receptionist}}$ and zeros out the component in the direction\nof $\\overrightarrow{g}$ , giving us\n$e_{\\text{receptionist}}^{\\text{debiased}}$.\n\n\\[e_{\\text{debiased}} = e_{w} - e_{\\text{proj}},e_{\\text{proj}} = \\frac{e_{w}\\text{.g}}{\\left\\| g \\right\\|_{2}} \\ast g\\]\n\nWhere $e_{w}$ is the word embedding of a certain word $w$, $g$ is the\ngender direction, $e_{\\text{proj}}$ is the projection of $e_{w}$ onto\nthe direction $g$ and finally $e_{\\text{debiased}}$ is the de-biased\nform of $e_{w}$.\n\nLet’s implement a function which can remove the bias of a given word:\n\ndef project(A, B):\n    return (np.dot(A, B) / np.sum(B**2)) * B\n\ndef neutralize(word, g, word_embedding):\n    e_w = word_embedding[word]\n    e_proj = project(e, g)\n    e_debiased = e_w - e_proj\n    return e_debiased\n\nNow, we can try:\n\n&amp;gt;&amp;gt;&amp;gt; # we can get the gender vector by simply doing so:\n&amp;gt;&amp;gt;&amp;gt; g = embedding[&#39;women&#39;] - embedding[&#39;man&#39;]\n&amp;gt;&amp;gt;&amp;gt; # cosine similarity before neutralizing:\n\n&amp;gt;&amp;gt;&amp;gt; cosine_similarity(embedding[&#39;receptionist&#39;], g))\n0.330779417506\n\n&amp;gt;&amp;gt;&amp;gt; e_debiased = neutralize(&quot;receptionist&quot;, g, embedding)\n&amp;gt;&amp;gt;&amp;gt; # cosine similarity after neutralizing:\n&amp;gt;&amp;gt;&amp;gt; cosine_similarity(e_debiased, g))\n-3.26732746085e-17\n\n\nEqualization\n\nBy equalization, we mean to equalize the values of gender-specific words\nlike “(actress, actor), (father, mother), …”. Equalization is applied\nto pairs of words that you might want to have differ only through the\ngender property. As a concrete example, suppose that &quot;actress&quot; is\ncloser to &quot;babysit&quot; than &quot;actor.&quot; By applying neutralizing to\n&quot;babysit&quot;, we can reduce the gender-stereotype associated with\nbabysitting. But this still does not guarantee that &quot;actor&quot; and\n&quot;actress&quot; are equidistant from &quot;babysit.&quot; The equalization algorithm\ntakes care of this.\n\nThe key idea behind equalization is to make sure that a particular pair\nof words are equi-distant from the 49-dimensional vector\n${\\overrightarrow{g}}_{\\bot}$. In pictures, this is how equalization\nworks:\n\n\n    \n\n\nThe derivation of the linear algebra to do this is a bit more complex,\nbut the key equations are:\n\n\\[\\mu = \\frac{e_{w1} + e_{w2}}{2},\\mu_{B} = \\text{proj}\\left( \\mu,{\\text{bia}s}_{\\text{axis}} \\right),\\mu_{\\bot} = \\mu - \\mu_{B}\\]\n\n\\[e_{w1B} = \\frac{\\sqrt{\\left| 1 - \\left\\| \\mu_{\\bot} \\right\\|_{2}^{2} \\right|} \\ast \\text{proj}\\left( e_{w1},\\text{bias}_{\\text{axis}} \\right) - \\mu_{B}}{\\left| \\text{proj}\\left( e_{w1},\\text{bias}_{\\text{axis}} \\middle| - \\mu_{B} \\right) \\right|}\\]\n\n\\[e_{w2B} = \\frac{\\sqrt{\\left| 1 - \\left\\| \\mu_{\\bot} \\right\\|_{2}^{2} \\right|} \\ast \\text{proj}\\left( e_{w2},\\text{bias}_{\\text{axis}} \\right) - \\mu_{B}}{\\left| \\text{proj}\\left( e_{w2},\\text{bias}_{\\text{axis}} \\middle| - \\mu_{B} \\right) \\right|}\\]\n\n\\[e_{1} = e_{w1B} + \\mu_{\\bot},e_{2} = e_{w2B} + \\mu_{\\bot}\\]\n\nLet’s get to the implantation:\n\ndef equalize(pair, bias_axis, word_to_vec_map):\n    w1, w2 = pair\n    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n    mu = (e_w1 + e_w2) / 2.\n    mu_B = project(mu, bias_axis)\n    mu_orth = mu - mu_B\n   \n    e1_orth = mu_orth\n    e2_orth = mu_orth\n    \n    e_w1B = np.sqrt(np.abs(1 - normalize(mu_orth)**2))\\\n                * (project(e_w1, bias_axis) - mu_B) \\\n                / np.linalg.norm(np.abs(project(e_w1, bias_axis) - mu_B))\n\n    e_w2B = np.sqrt(np.abs(1 - normalize(mu_orth)**2)) \\\n                * (project(e_w2, bias_axis) - mu_B) \\\n                / np.linalg.norm(np.abs(project(e_w2, bias_axis) - mu_B))\n    \n    e1 = e_w1B + e1_orth\n    e2 = e_w2B + e2_orth\n    return e1, e2\n\n\nNow, let’s see it in action\n\n&amp;gt;&amp;gt;&amp;gt; # cosine similarities before equalizing:\n&amp;gt;&amp;gt;&amp;gt; cosine_similarity(embedding[&#39;man&#39;], g))\n-0.117110957653\n\n&amp;gt;&amp;gt;&amp;gt; cosine_similarity(embedding[&#39;man&#39;], g))\n0.356666188463\n\n&amp;gt;&amp;gt;&amp;gt; e1, e2 = equalize((&#39;man&#39;, &#39;woman&#39;), g, embedding)\n&amp;gt;&amp;gt;&amp;gt; # cosine similarities after equalizing:\n&amp;gt;&amp;gt;&amp;gt; cosine_similarity(e1, g))\n-0.700436428931\n\n&amp;gt;&amp;gt;&amp;gt; cosine_similarity(e2, g))\n-0.700436428931\n\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "Subword Embedding",
        "url"       : "/word-embedding/subword_embedding",
        "date"      : "19/06/2017",
        "content": "Methods such as word2vec or GloVe ignore the internal structure of words\nand associate each word (or word sense) to a separate vector\nrepresentation. For morphologically rich languages, there may be a\nsignificant number of rare word forms such that either a very large\nvocabulary must be maintained or a significant number of words are\ntreated as out-of-vocabulary (OOV).\n\nAs previously stated, out-of-vocabulary words can significantly impact\nperformance due to the loss of context from rare words. An approach that\ncan help deal with this limitation is the use of subword embeddings\nproposed by this paper: “Enriching Word Vectors with Subword\nInformation”, where vector\nrepresentations $z_{g}$ are associated with character n-grams g and\nwords $w_{i}$ are represented by the sum of the n-gram vectors:\n\n\\[w_{i} = \\sum_{g \\in \\mathbb{G}_{w}}^{}z_{g}\\]\n\nFor instance, the vector for the word “take” consists of the sum of the\nvectors for the n-grams {t, a, k, e, &amp;lt;t, ta, ke, , e&amp;gt;, &amp;lt;ta, tak, ake,\nke&amp;gt;} when $n \\in \\lbrack 1,2,3\\rbrack$ as show in the following figure:\n\n\n    \n\n\nAs n-grams are shared across words, this allows for representation of\neven unseen words since an OOV word will still consist of n-grams that\nwill have representations. Subword embeddings can significantly boost\nNLP tasks such as language modeling and text classification.\n\nTO BE CONTINUED…\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "Contextualized Word Embedding: ELMO &amp; BERT",
        "url"       : "/word-embedding/contextualized_word_embedding",
        "date"      : "22/03/2018",
        "content": "In the past few years, a number of new methods leveraging contextualized\nembeddings have been proposed. These are based on the notion that\nembeddings for words should be based on contexts in which they are used.\nThis context can be the position and presence of surrounding words in\nthe sentence, paragraph, or document.\n\nELMO\n\nELMO stands for “Embeddings from Language Models” and it is based on a\nvery important concept which is contextualized word embeddings.\nContextualized word embeddings means that: instead of using a fixed\nembedding for each word, let’s look at the entire sentence before\nassigning each word in it an embedding.”. So, each word will have a\ndifferent embedding based on the context of the sentence.\n\nSo, for example the word “Paris”, it can be used to describe the city or\nit can be a female name like “Paris Hilton” for example. In all previous\nword embedding techniques, the word “Paris” wil lhave the same word\nembedding for both the city and the name. In ELMO, “Paris” will still\nhave just one embedding vector where it sums all the multiple embeddings\nresulting from the different context the word appeared at.\n\n\n    \n\n\nELMO was published in this paper: “Deep contextualized word\nrepresentations” by “Allen\nInstitute for AI” in 2018. And ELMO creates these contextualized word\nembeddings by using a bi-directional LSTM trained on a massive dataset\nto predict the next word in a sequence of words - a task called Language\nModeling. This is convenient because we have vast amounts of text data\nthat such a model can learn from without needing labels.\n\n\n    \n\n\nELMO comes up with the contextualized embedding from bi-directional LSTM\nthrough the following three steps:\n\n\n  \n    Given a word “w”, we will concatenate the forward and backward word\nembeddings of each word at every layer. So, in the previous\narchitecture, where ELMO has only two layers, we will have three\nsets of vectors:\n\n    \n      \n        Concatenate the forward and backward of the “stick” word embeddings.\n      \n      \n        Concatenate the forward and backward of the first LSTM layer.\n      \n      \n        Concatenate the forward and backward of the second LSTM layer.\n      \n    \n  \n  \n    Multiply each word vector by learnable parameters that represent the\nimportance of that word embedding.\n  \n  \n    Eventually, sum all three weighted vectors together to get just one\nvector.\n  \n\n\nAnd all three steps can be summarized in the following image:\n\n\n    \n\n\nBERT\n\nThis part relies heavily on BERT and how it works. So, if you need a\nrefresher, check the BERT part in the language model document. BERT\nstands for “Bidirectional Encoder Representations from Transformers” and\nit’s a language model architecture that can be fine-tuned for various\ntasks. BERT, also, can be used to create contextualized word embeddings\nlike ELMO as shown in the following figure:\n\n\n    \n\n\nAccording to the paper: “BERT: Pre-training of Deep Bidirectional\nTransf”, the output of each\nencoder layer along each token’s path can be used as a word embedding\nfor that token. And according to the paper, there are some word vectors\nthat work best as contextualized word embedding knowing that it might\ndepend on the task. So, the task the paper used was NER and the\nfollowing summarized the results:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "LASER",
        "url"       : "/word-embedding/LASER",
        "date"      : "25/09/2019",
        "content": "LASER stands for “Language-Agnostic Sentence\nRepresentation”. LASER is an encoder-decoder architecture proposed\nby FAIR in 2019 and published in their paper: Massively Multilingual\nSentence Embeddings for Zero-Shot Cross-Lingual Transfer and\nBeyond. The official code for\nthis paper can be found in the Fairseq official GitHub repository:\nfairseq/laser.\n\nLASER was trained on parallel corpora to learn joint multilingual\nsentence representations for 93 languages. The encoder is\nlanguage-agnostic which maps the source sequence into a fixed-length\nvector representation, which is used by the decoder alongside the\nlanguage ID embedding $L_{\\text{id}}$ to create the target sequence.\nThis decoder is then discarded, and the encoder is kept to embed\nsentences in any of the training languages.\n\n\n    \n\n\nIn this paper, they studied a stacked BiLSTM with 1 to 5 layers, each\n512-dimensional resulting 1024-dimensional sentence representations\n(after concatenating both directions). The decoder has always one layer\nof dimension 2048. The input embedding size is set to 320, while the\nlanguage ID embedding has 32 dimensions.\n\nTraining minimizes the cross-entropy loss on the training corpus,\nalternating over all combinations of the languages involved. They used\nAdam with a constant learning rate of 0.001 and dropout set to 0.1, and\ntrain for a fixed number of epochs.\n\nTO BE CONTINUED\n"
      }
    
  
]
